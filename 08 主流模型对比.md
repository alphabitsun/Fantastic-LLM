# 08. 主流模型对比

Transformer

Bert

GPT

GPT3

GPT-3沿用了GPT-2的结构，但是在网络容量上做了很大的提升，并且使用了一个[Sparse Transformer](https://zhida.zhihu.com/search?content_id=239245991&content_type=Article&match_order=1&q=Sparse+Transformer&zhida_source=entity)的架构，具体如下：

1.GPT-3采用了96层的多头transformer，头的个数为 96；

2.词向量的长度是12,888；

3.上下文划窗的窗口大小提升至2,048个token；

4.使用了alternating dense和locally banded sparse attention

**Sparse Transformer：**

Sparse Transformer是一种旨在处理高维、稀疏和长序列数据的Transformer拓展版，相比于传统的Transformer架构，Sparse Transformer**通过在自注意力机制中引入稀疏性，减少了网络中计算的数量**，从而可以**处理更长的序列数据**。具体的：在处理高维、稀疏数据时，Sparse Transformer可以避免对所有输入的位置进行计算，**只计算与当前位置相关的位置**，从而**提高了计算效率**。

Sparse Transformer 在处理高维、稀疏和长序列数据时引入**稀疏注意力机制（Sparse Attention）**，核心在于：**并非对所有位置都计算注意力，而是只选择一小部分“相关”的位置进行注意力计算**。那么你提到的关键问题是：

> 如何知道哪些位置是“相关”的，哪些是不相关的？
> 

这个“相关”并不是动态学习得到的（如常规的注意力），而是**通过预定义的稀疏模式**来近似实现的。

---

## **🔍 稀疏性如何定义？（Sparse Attention Patterns）**

Sparse Transformer 采用 **预定义的稀疏连接模式**（hard-coded sparse patterns）来确定哪些位置是“相关”的。以下是常见的几种方式：

---

### **1.**

### **固定窗口（Sliding Window Attention）**

每个位置只关注其前后固定窗口内的若干个位置（例如前后各16个 token），用于捕捉局部依赖。

- ✅ 优点：保留局部信息。
- ❌ 缺点：无法捕捉远程依赖。

---

### **2.**

### **跳跃连接（Strided Attention）**

每个位置只关注按一定步长跳跃的位置，例如每隔 k 个 token 关注一次。

- ✅ 优点：可捕捉部分远程信息。
- ❌ 缺点：覆盖不连续，可能遗漏重要依赖。

---

### **3.**

### **全局位置（Global Attention）**

选择一些重要位置（比如句首、句尾、特定 token）与所有位置互相注意。

- ✅ 优点：为全局信息提供桥梁。
- ❌ 缺点：需要知道哪些位置重要（可预设或学习）。

---

### **4.**

### **局部 + 全局混合（如 BigBird, Longformer）**

结合固定窗口、全局连接、稀疏跳跃，形成一种**块状稀疏结构**。

> 举例：Longformer 中一个位置可能会：
> 
- 注意其前后几个 token（局部）
- 注意几个全局 token（如 [CLS]）
- 注意一些跳跃位置

---

### **5.**

### **Learned Sparse Attention（可学习稀疏性）**

某些变体（如 Routing Transformer、Reformer）使用数据驱动的方法来**动态学习哪些 token 是相关的**，比如：

- 使用 **局部敏感哈希（LSH）** 将相似的 token 哈希到同一组里；
- 使用 **聚类** 或 **KNN** 找到相关 token；
- 使用 **门控机制** 动态决定稀疏连接。

---

## **✅ 总结**

> Sparse Transformer 中，“相关位置”并不是通过标准注意力机制动态得出的，而是通过
> 
> 
> **预定义的稀疏模式**
> 
> **提前决定**
> 

| **方法** | **是否动态** | **描述** |
| --- | --- | --- |
| 固定窗口 | ❌ | 每个 token 只关注相邻的一小段范围 |
| 跳跃连接 | ❌ | 每隔一定步长连接 |
| 全局 token | ❌ | 一些 token 全连接，用于信息桥梁 |
| LSH / Routing | ✅ | 利用局部敏感哈希等方法分组 token，实现稀疏连接 |
| 可学习注意力模式 | ✅ | 模型自己决定连接结构，如 Routing Transformer 中的动态稀疏 |

如果你想深入分析每种稀疏结构的数学原理或源码实现，我可以继续讲解（例如 BigBird 的 block sparse matrix 形式，或者 Reformer 的哈希桶机制）。需要我展开吗？

**ChatGPT前身：InstructGPT**

## **2.DataSet**

InstructGPT的训练分为3个步骤，每个步骤对应一个专属的训练数据集：

# Llama

decode-only

预归一化

使用RoPE(旋转位置编码)

**与LLaMA 2 相比：**

1）LLaMA 3具有 128K词汇量大小的Tokenizer，可以更有效的对文本进行编码，从而显着提高模型性能。

2）8B 和 70B 的LLaMA3都采用了分组查询注意力 (GQA)机制，以提高Inference速度。（LLaMA 2 70B也采用了GQA）

3）在8,192 个Token的较长序列上训练模型，使用掩码机制确保自注意力不会跨越文档边界。需要注意的是LLaMA 3采用了8K Token进行训练，并不代表只能生成8K Token以内文本。（LLaMA 2 为4096）

### **长文本支持RoPE**

LLaMA 3 中依然采用了原始的RoPE（旋转嵌入）作为位置嵌入编码方案，没有使用诸如Position Interpolation，Dynamic NTK之类的技术。当然这也给国内魔改中文LLaMA 3提供了更大的空间。RoPE的主要优势包括：

- 可以扩展到任意序列长度。
- 随着相对距离的增加，Token间的依赖性减弱。
- 为线性自注意力配备相对位置编码的能力。

旋转位置嵌入（RoPE）使用旋转矩阵对绝对位置进行编码，同时在自注意力公式中结合了明确的相对位置依赖性。旋转位置嵌入保持了序列长度的灵活性、随相对距离的增加而衰减的token间依赖性，以及为线性自注意力配备的相对位置编码的能力。

### **3.4 微调方法**

Meta在预训练后使用了有监督微调（SFT）、拒绝采样、近端策略优化（PPO）和直接策略优化（DPO）的组合微调算法。 SFT 中使用的提示（Prompt）质量、PPO 和 DPO 中使用的偏好排名对对齐（Align）模型的性能有巨大影响。

在训练后，模型知道如何生成不同的答案，但模型一般不知道如何选择出正确的答案。对偏好排名的训练使模型能够学习如何选出正确答案，提升在推理和编码任务上的性能。

![](https://pic4.zhimg.com/v2-3e226502e33938fa8530fd14599c7de5_1440w.jpg)

使用有监督微调（SFT）、近端策略优化（PPO）和直接策略优化（DPO）的组合微调

Qwen3

decode only

遵循基本的transformer解码器架构，由四部分组成

1. [**自注意力机制](https://zhida.zhihu.com/search?content_id=258062888&content_type=Article&match_order=1&q=%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6&zhida_source=entity)（Self-Attention）**：允许模型关注输入序列中的不同位置，捕捉长距离依赖关系。
2. [**前馈网络](https://zhida.zhihu.com/search?content_id=258062888&content_type=Article&match_order=1&q=%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C&zhida_source=entity)（Feed-Forward Network）**：由两个线性变换和一个非线性激活函数组成，增强模型的表示能力。
3. [**层归一化](https://zhida.zhihu.com/search?content_id=258062888&content_type=Article&match_order=1&q=%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96&zhida_source=entity)（Layer Normalization）**：稳定训练过程，加速收敛。
4. [**残差连接](https://zhida.zhihu.com/search?content_id=258062888&content_type=Article&match_order=1&q=%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5&zhida_source=entity)（Residual Connection）**：缓解梯度消失问题，便于训练深层网络。

```
graph TD
    A[输入Embedding] --> B[位置编码]
    B --> C[Transformer层 1]
    C --> D[Transformer层 2]
    D --> E[...]
    E --> F[Transformer层 N]
    F --> G[输出层]

    subgraph "Transformer层结构"
    H[输入] --> I[层前归一化]
    I --> J[自注意力机制]
    J --> K[残差连接]
    K --> L[层前归一化]
    L --> M[前馈网络/MoE]
    M --> N[残差连接]
    N --> O[输出]
    end
```

上图展示了Qwen3的整体架构和单个Transformer层的内部结构。与传统Transformer不同，Qwen3采用了层前归一化（Pre-Layer Normalization）设计，即在每个子层（自注意力和前馈网络）之前应用层归一化，而不是之后。这种设计有助于稳定训练过程，特别是对于深层模型。

1. [**分组查询注意力](https://zhida.zhihu.com/search?content_id=258062888&content_type=Article&match_order=1&q=%E5%88%86%E7%BB%84%E6%9F%A5%E8%AF%A2%E6%B3%A8%E6%84%8F%E5%8A%9B&zhida_source=entity)（Grouped Query Attention, GQA）**：

传统的多头注意力机制（Multi-Head Attention, MHA）为每个注意力头分配独立的查询（Q）、键（K）和值（V）投影。而GQA则让多个查询头共享同一组键值头，显著减少了参数量和计算量，同时保持了性能。

在Qwen3中，不同规模的模型采用了不同的GQA配置：

| **模型** | **查询头数量** | **键值头数量** | **头数比例** |
| --- | --- | --- | --- |
| Qwen3-0.6B | 8 | 4 | 2:1 |
| Qwen3-1.7B | 16 | 4 | 4:1 |
| Qwen3-4B | 32 | 8 | 4:1 |
| Qwen3-8B | 32 | 8 | 4:1 |
| Qwen3-14B | 40 | 10 | 4:1 |
| Qwen3-32B | 64 | 8 | 8:1 |
| Qwen3 MoE系列 | 128 | 16 | 8:1 |

这种设计大大减少了模型的参数量和计算量，同时保持了性能。

1. [**QK-Norm**](https://zhida.zhihu.com/search?content_id=258062888&content_type=Article&match_order=1&q=QK-Norm&zhida_source=entity)：

Qwen3移除了传统Transformer中的QKV-bias（查询、键、值投影的偏置项），并引入了QK-Norm技术。QK-Norm对查询和键向量进行归一化，使得注意力分数的分布更加稳定，有助于提高模型的稳定性和性能。

1. [**旋转位置编码](https://zhida.zhihu.com/search?content_id=258062888&content_type=Article&match_order=1&q=%E6%97%8B%E8%BD%AC%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81&zhida_source=entity)（Rotary Positional Embedding, RoPE）**：

Qwen3采用了RoPE作为位置编码方法，它通过对查询和键向量应用旋转变换，将位置信息直接编码到自注意力计算中。与传统的位置编码相比，RoPE具有更好的相对位置感知能力和外推性。

### **3.1.3 前馈网络与激活函数**

Qwen3的前馈网络（Feed-Forward Network, FFN）采用了标准的两层结构，但在激活函数和设计细节上有所创新：

1. **SwiGLU激活函数**：

Qwen3采用了SwiGLU（Swish-Gated Linear Unit）作为前馈网络的激活函数，它是对传统GLU（Gated Linear Unit）的改进，使用Swish函数替代了Sigmoid函数，提供了更好的梯度流和性能。

SwiGLU激活函数使得Qwen3在训练过程中收敛更快，并在各种任务上取得更好的性能。

1. **隐藏层维度**：

Qwen3的前馈网络隐藏层维度通常是模型维度的4倍，这种设计提供了足够的模型容量，使模型能够学习复杂的模式和关系。

| **模型** | **模型维度** | **FFN隐藏维度** | **比例** |
| --- | --- | --- | --- |
| Qwen3-0.6B | 1024 | 4096 | 4:1 |
| Qwen3-1.7B | 1536 | 6144 | 4:1 |
| Qwen3-4B | 2560 | 10240 | 4:1 |
| Qwen3-8B | 3584 | 14336 | 4:1 |
| Qwen3-14B | 4608 | 18432 | 4:1 |
| Qwen3-32B | 6144 | 24576 | 4:1 |
| Qwen3 MoE系列 | 8192 | 32768 | 4:1 |

这种设计在保持模型表达能力的同时，也考虑了计算效率和训练稳定性。

### **3.2 密集模型与MoE模型的架构差异**

Qwen3系列包含两种类型的模型：密集模型（Dense Models）和混合专家模型（Mixture-of-Experts Models, MoE）。这两种模型在架构上有显著差异，下面我们将详细比较它们的设计特点。

### **3.2.1 密集模型架构**

密集模型是传统的Transformer架构，其中所有参数在每次前向传播中都会被激活。Qwen3的密集模型从0.6B到32B，覆盖了从轻量级到大型的多个规模。

密集模型的主要特点包括：

1. **参数高效利用**：所有参数都参与每次计算，充分利用了模型容量。
2. **实现简单**：架构相对简单，易于实现和部署。
3. **训练稳定**：训练过程相对稳定，超参数调整相对简单。
4. **推理速度快**：在相同参数量下，推理速度通常快于MoE模型。

以下是Qwen3密集模型的主要配置参数：

| **模型** | **层数** | **模型维度** | **注意力头数** | **参数量** |
| --- | --- | --- | --- | --- |
| Qwen3-0.6B | 24 | 1024 | 8 | 0.6B |
| Qwen3-1.7B | 24 | 1536 | 16 | 1.7B |
| Qwen3-4B | 32 | 2560 | 32 | 4.0B |
| Qwen3-8B | 32 | 3584 | 32 | 8.0B |
| Qwen3-14B | 40 | 4608 | 40 | 14.0B |
| Qwen3-32B | 60 | 6144 | 64 | 32.0B |

### **3.2.2 MoE模型架构**

MoE模型是一种稀疏激活的神经网络架构，其核心思想是"专家分工"。在Qwen3的MoE模型中，每个Transformer层的前馈网络被替换为多个"专家"（Expert）网络，但在处理每个输入时，只激活其中的一部分专家。

`graph TD
    A[输入] --> B[自注意力层]
    B --> C[MoE层]
    C --> D[输出]

    C --> E[路由器]
    E --> F[专家1]
    E --> G[专家2]
    E --> H[专家3]
    E --> I[...]
    E --> J[专家N]

    F --> K[加权合并]
    G --> K
    H --> K
    I --> K
    J --> K

    K --> D`

上图展示了MoE层的基本结构，其中路由器（Router）负责决定每个输入应该由哪些专家处理，然后将多个专家的输出加权合并得到最终结果。

Qwen3的MoE模型具有以下特点：

1. **细粒度专家分割**：每个MoE层包含多个专家，每个专家都是一个完整的前馈网络。
2. **Top-k路由**：对于每个输入token，路由器选择k个最相关的专家进行处理（Qwen3中k=2）。
3. **无共享专家**：与一些包含共享专家的MoE模型不同，Qwen3的每个专家都有独特的参数，增强了专业化程度。
4. **全局批次负载平衡**：为了确保各个专家的工作负载均衡，Qwen3引入了全局批次负载平衡损失。

Qwen3的MoE模型配置如下：

| **模型** | **层数** | **模型维度** | **注意力头数** | **专家数量** | **激活专家数** | **总参数量** | **激活参数量** |
| --- | --- | --- | --- | --- | --- | --- | --- |
| Qwen3-30B-A3B | 60 | 8192 | 128 | 10 | 1 | 30B | 3B |
| Qwen3-72B-A7B | 80 | 8192 | 128 | 10 | 1 | 72B | 7B |
| Qwen3-235B-A22B | 120 | 8192 | 128 | 10 | 2 | 235B | 22B |

MoE架构的主要优势在于：

1. **参数效率**：虽然总参数量很大，但每次推理只激活一小部分，大大提高了参数效率。
2. **性能提升**：在相同激活参数量下，MoE模型通常比密集模型性能更好。
3. **专业化能力**：不同专家可以专注于不同类型的输入，提高模型处理多样化任务的能力。

例如，Qwen3-235B-A22B虽然总参数量达到235B，但每次推理只激活22B参数，这使得它能够在保持高性能的同时，控制计算资源需求。

### **3.3 分词器实现与多语言支持**

分词器（Tokenizer）是大型语言模型的重要组成部分，它负责将原始文本转换为模型可以处理的token序列。Qwen3的分词器设计考虑了多语言支持和处理效率，下面我们将详细介绍其实现。

### **3.3.1 字节级字节对编码（Byte-level BPE）**

Qwen3采用了字节级字节对编码（Byte-level Byte-Pair Encoding, BBPE）作为分词方法。BBPE的核心思想是将文本视为字节序列，然后应用BPE算法学习常见的字节组合，形成词汇表。

BBPE的主要优势包括：

1. **通用性**：能够处理任何语言和字符，包括表情符号、特殊符号等。
2. **无未知词**：由于基于字节，任何输入都可以被编码，不会出现未知词（UNK）。
3. **效率**：编码和解码过程高效，适合实时应用。

### **3.3.2 多语言支持扩展**

Qwen3将支持的语言从Qwen2.5的29种扩展到了119种语言和方言，这一扩展主要通过以下方式实现：

1. **扩大词汇表**：Qwen3的词汇表包含15.2万个token，比Qwen2.5的15.1万略有增加，但通过更高效的编码方式支持了更多语言。
2. **多语言训练数据**：在预训练阶段使用了覆盖119种语言的大规模文本语料，使模型能够学习各种语言的模式和规则。
3. **语言识别与切换**：Qwen3能够自动识别输入文本的语言，并在不同语言之间无缝切换，这对于多语言对话和翻译任务尤为重要。

### **3.3.3 特殊token与控制码**

Qwen3的分词器包含多种特殊token和控制码，用于标记序列的开始和结束、控制生成行为等：

1. **基本特殊token**：
2. `<bos>`: 序列开始标记
3. `<eos>`: 序列结束标记
4. `<pad>`: 填充标记
5. `<unk>`: 未知词标记
6. **控制码**：
7. 思考模式控制：用于切换思考模式和非思考模式
8. 语言控制：用于指定生成特定语言的文本
9. 格式控制：用于控制生成文本的格式（如代码、表格等）

这些特殊token和控制码增强了Qwen3的灵活性和可控性，使用户能够更精确地控制模型的行为。

### **3.4 长上下文处理技术**

Qwen3能够处理长达128K tokens的超长上下文，这一能力主要通过以下技术实现：

### **3.4.1 RoPE基频调整**

Qwen3将RoPE的基频从传统的10,000增加到了1,000,000，这一调整使得模型能够更好地区分远距离位置，提高了长序列处理能力。

### **3.5 架构设计的性能影响**

Qwen3的架构设计对其性能有显著影响，下面我们将分析不同架构组件对模型性能的贡献。

### **3.5.1 GQA对推理效率的影响**

GQA通过减少键值头的数量，显著降低了内存使用和计算量，同时保持了性能。实验表明，与传统MHA相比，GQA可以：

- 减少约30-50%的内存使用
- 提高约20-40%的推理速度
- 在保持或略微降低性能的情况下，大大提高了模型的实用性

### **3.5.2 MoE架构的参数效率**

MoE架构通过稀疏激活，实现了更高的参数效率。实验表明：

- Qwen3-30B-A3B（激活3B参数）的性能接近Qwen3-14B（14B参数）
- Qwen3-72B-A7B（激活7B参数）的性能超过Qwen3-32B（32B参数）
- Qwen3-235B-A22B（激活22B参数）的性能远超其他开源模型

这表明MoE架构能够以更少的计算资源实现更高的性能，特别适合资源受限的场景。

### **3.5.3 长上下文技术的实际效果**

Qwen3的长上下文处理技术使其能够有效处理超长文本，实验表明：

- 在32K tokens长度的测试中，Qwen3保持了接近100%的准确率
- 在64K tokens长度的测试中，Qwen3保持了约95%的准确率
- 在128K tokens长度的测试中，Qwen3仍然保持了约85%的准确率

这种长上下文处理能力使Qwen3能够理解和分析长文档、书籍章节，甚至整本书的内容，大大扩展了其应用场景。

总的来说，Qwen3的架构设计在保持高性能的同时，实现了更高的计算效率和更强的扩展性，使其成为一个既强大又实用的开源大语言模型。通过深入理解这些架构组件，开发者和研究者可以更好地利用Qwen3的能力，并在此基础上进行进一步的创新和优化。

Deepseek
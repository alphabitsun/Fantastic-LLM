






## 4.6 Moonshot 研究

### 4.6.1 Mooncake

### 4.6.2 Kimi k1.5

### 4.6.3 Kimi K2

# 5. 训练优化

## 5.1 模型压缩

### 5.1.1 量化

* 线性量化 vs. 非线性量化

* 训练后量化 vs. 量化感知训练

* 经典算法：QAT, PTQ, DoReFa-Net

### 5.1.2 剪枝

* 结构化剪枝 vs. 非结构化剪枝

* 基于幅值的剪枝 vs. 基于重要性的剪枝

* 经典算法：Magnitude Pruning, Lottery Ticket Hypothesis

### 5.1.3 蒸馏

* 离线蒸馏 vs. 在线蒸馏

* 基于logits的蒸馏 vs. 基于特征的蒸馏

* 经典算法：KD, FitNets, Attention Transfer

## 5.2 高效模型架构

### 5.2.1 高效注意力机制

> 论文：[**Reformer**](https://arxiv.org/pdf/2001.04451)  **[Longformer](https://arxiv.org/pdf/2004.05150)  [Linformer](https://arxiv.org/pdf/2006.04768)  [Big Bird](https://arxiv.org/pdf/2007.14062)  [Performer](https://arxiv.org/pdf/2009.14794)**

* **Longformer**



* **Big Bird**



* **Reformer**



* **Linformer**



* **Performer**



### 5.2.2 混合专家模型 MoE

> 论文：[**Switch Transformer**](https://arxiv.org/pdf/2101.03961)  **[SoftMoE](https://arxiv.org/pdf/2308.00951)  [LoRAMoE](https://arxiv.org/pdf/2312.09979)  [DeepSeekMoE](https://arxiv.org/pdf/2401.06066)**

* **Switch Transformer**





* **SoftMoE**





* **LoRAMoE**





* **DeepSeekMoE**



## 5.3 优化算法改进

### 5.3.1 高级优化技术

### 5.3.2 学习率调度

## 5.4 并行训练

> 论文：**[ZeRO](https://arxiv.org/pdf/1910.02054)  [Megatron-LM](https://arxiv.org/pdf/1909.08053)  [GPipe](https://arxiv.org/pdf/1811.06965)**

这里主要讲 3 种并行策略：数据并行 **DP**（**D**ata **P**arallelism）、张量并行 **TP**（**T**ensor **P**arallelism）和流水线并行 **PP**（**P**ipeline **P**arallelism）。各种并行策略示意图如下所示：

![          数据并行]()

![           张量并行]()

![           流水线并行]()

### 5.4.1 **数据并行**

在近年来的深度学习模型训练中，使用更多的训练数据和更大的模型趋势未改。更大的模型和数据量意味着更多的计算量和存储需求，也意味着更久的训练时间。那么如何将计算和存储需求分布到多个训练设备来提升训练速度，是关键问题。
数据并行 DP 是解决上述问题的的一种并行策略，其主要逻辑遵&#x5FAA;**`Single Program Multiple Data`**&#x7684;原则，即在数据并行的模型训练中，训练任务被切分到多个设备上，每个进程维护相同的模型参数和相同的计算任务，但是处理不同的数据。通过这种方式，同一全局数据下的数据和计算被切分到了不同的进程，从而减轻了单个设备上的计算和存储压力。在深度学习模型训练中，数据并行可作为通过增加并行训练设备来提高训练吞吐量的方法。

> **注**：**Single Program Multiple Data&#x20;**&#x5E76;行模式中的要求：
>
> 1. **`Single Program`**：在深度学习训练中每个进程上模型的组网和参数相同
>
> 2. **`Multiple Data`**：在深度学习训练中为每个进程上模型处理不同的数据

**例**：以常见&#x7684;**`ResNet50`**&#x6A21;型使&#x7528;**`32GB V100`**&#x5361;训练为例。假设训练时单卡最大能支持的 local batch size 为256，训练一个 step 的耗时&#x4E3A;**`1`**&#x79D2;，则单卡训练时的吞吐&#x4E3A;**`256 images/s`**。如果我们使&#x7528;**`32`**&#x5F20;**`V100`**&#x505A;数据并行训练，假设没有损耗，那么理论上的训练吞吐可达&#x5230;**`32 x 256 = 8192 images/s`**。实际上由于数据并行时多机多卡的通信消耗等，实际加速效率会有折扣，但在加速效率&#x4E3A;**`0.8`**&#x65F6;，训练吞吐也可达&#x5230;**`32 x 256 x 0.8 = 6554 images/s`**。如果使用更多的 GPU，并行训练的速度将会更高，大大减少训练需要的时间。
深度学习训练中数据并行的实现方式可以有多种，这里介绍目前主流深度学习训练框架中数据并行的实现方式：基&#x4E8E;**`Distributed Synchronous SGD`**&#x7684;梯度同步数据并行。

数据并行分为了两种模式：**DP**（**D**ata **P**arallel）和 **DDP**（**D**istributed **D**ata **P**arallel） 。

**Data Parallel**

DP是一种单进程多线程的并行策略，只能在单机上进行训练，从卡做 Forward 和 Backward 并行，主卡做梯度聚合和优化器更新，具体步骤如下：

> 1. 单进程控制多GPU，即本质上是单进程多线程
>
> 2. 首先将模型加载到主 GPU 上，再复制到各个指定从 GPU；
>
> 3. 将输入数据按照 Batch 维度进行拆分，各个 GPU 独立进行 forward 计算；
>
> 4. 将结果同步给主 GPU 完成梯度计算和参数更新，将更新后的权重参数复制到各个 GPU

**存在的问题：** 由于其是单进程控制多个GPU，故会存在GPU之间负载不均衡的问题，主GPU负载较大。

**Distributed Data Parallel**

**DDP** 采&#x7528;**`AllReduce`**&#x67B6;构，多进程的方式，突破锁的束缚。在单机和多机上都可以使用。负载分散在每个 GPU 节点上，通信成本（时间）是恒定的，与 GPU 数量无关，等于参数量$$V$$除以带宽$$B$$。**DDP** 不需要通过主 GPU 分发全模型的参数到每个 GPU 上。使&#x7528;**`ring-all-reduce`**&#x7684;方式进行通讯，随着 GPU 数量$$N$$增加，总传输量恒定。也就是理论上，随着 GPU 数量的增加，ring all-reduce有线性加速能力。

![]()

**学习率设置**

数据并行模式下学习率的设置基本原则是学习率正比于全局 **batch size。** 与单卡训练相比，数据并行训练通常有两种配置：

* 一种是保持保持所有计算设备的 batch size 的总和与单卡训练的 batch size 保持一致。这种情形下，由于数据并行训练和单卡训练的 batch size 是一致的，通常保持数据并行模式下各个计算设备上的学习率与单卡训练一致。

* 另一种情形是保持每个计算设备的 batch size 和单卡训练的 batch size 一致。这种情形下，数据并行模式的全局 batch size 是单卡训练的$$N$$倍，$$N$$是数据并行计算的设备数。因此，通常需要将数据并行模式下每个计算设备的学习率相应的设置为单卡训练的$$N$$倍。这样，数据并行模式下的初始学习率通常较大，不利于模型的收敛。因此，通常需要使用 warm-up 机制。即，在初始训练时使用较小的学习率，并逐步缓慢增加学习率，经过一定迭代次数后，学习率增长到期望的学习率。

**数据集切分**

输入数据切分实现上比较简单，一般有两种常用的实现方式：

> 1. 在每个训练 epoch 开始前，将整个训练数据集根据并行进程数划分，每个进程只读取自身切分的数据
>
> 2. 数据的读取仅由具体某个进程负责，一般为rank0，该进程在数据读取后同样根据并行进程数将数据切分成多块，再将不同数据块发送到对应进程上

数据并行中，通常将数据集切分为$$N$$份，每个训练卡负责训练其中的一份数据。这里$$N$$是数据并行的并行度。由于每一个迭代中，各个训练卡均需要做一次梯度同步。因此需要确保对于每个 epoch，各个训练卡经历相同的迭代数，否则，运行迭代数多的训练卡会一直等待通信完成。实践中，通常通过**数据补齐**或者**丢弃**的方式保证各个训练卡经历相同的迭代数。

> **注**：数据补齐的方式指的是，为某些迭代数少训练数据补充部分数据，从而保证切分后的各份数据集的迭代次数相同；丢弃的方式则是丢弃部分迭代次数较多的数据，从而保证各份数据集的迭代次数相同。

通常，在每个 epoch 需要对数据做 shuffle 处理。因此，根据 shuffle 时机的不同，有两种数据切分的方法。

**在数据切分前做 shuffle**

首先对完整的数据做 shuffle 处理，做相应的数据补充或丢弃，然后做数据的切分。

**在数据切分后做 shuffle**

首先做数据的补充或丢弃和数据切分，然后对切分后的每一份数据分别做 shuffle 处理。

**参数同步**

数据并行实现的关键问题在于如何保证训练过程中每个进程上模型的参数相同。因为训练过程的每一个 step 都会更新模型参数，每个进程处理不同的数据会得到不同的 Loss。由 Loss 计算反向梯度并更新模型参数后，如何保证进程间模型参数正确同步，是数据并行需要解决的最主要问题。根据梯度更新公式，只要保证两点就能解决这个问题：

> 1. 每个进程上模型初始化参数$$W_0$$相同，保证每个进程模型参数初始相同有两种常用的实现方法：
>
>    1. 所有进程在参数初始时使用相同的随机种子并以相同的顺序初始化所有参数
>
>    2. 通过某个具体进程初始化全部模型参数，之后由该进程向其他所有进程广播模型参数
>
> 2. 每个进程每次更新时的梯度$$\Delta W$$相同

基于上述方法使每个进程得到一份相同的模型初始化参数后，梯度同步的数据并行训练就可以进一步拆解为如下三个部分：

**前向计算**：每个进程根据自身得到的输入数据独立前向计算，因为输入数据不同每个进程会得到不同的Loss。

**反向计算**：每个进程根据自身的前向计算独立进行反向计算，因为每个进程上的 Loss 不同，每个进程上在反向中会计算出不同的梯度。这时一个关键的操作是要在后续的更新步骤之前，对所有进程上的梯度进行同步，保证后续更新步骤中每个进程使用相同的全局梯度更新模型参数。

![]()

这一个梯度同步过程是用一&#x4E2A;**`AllReduce Sum`**&#x540C;步通信操作实现的，对梯度使用 **AllReduce Sum** 操作后每个进程上得到的梯度是相同的，这时候的梯度值等于所有进程上梯度对应位置相加的和，然后每个进程&#x7528;**`AllReduce`**&#x540E;的梯度和除以数据并行中的进程数，这样得到的梯度是同步之前所有进程上梯度的平均值。如下图所示。

**参数更新**：每个进程经过上述步骤后得到相同全局梯度，然后各自独立地完成参数更新。因为更新前模型各进程间的参数是相同的，更新中所使用的梯度也是相同的，所以更新后各进程上的参数也是相同的。
上述是主流框架中数据并行的实现过程。和单卡训练相比，最主要的区别在于反向计算中的梯度需要在所有进程中进行同步，保证每个进程上最终得到的是所有进程上梯度的平均值。

### 5.4.2 **张量并行**

随着技术的发展，业界内训练的模型越来越大，模型朝着更深和更宽的方向发展。以 NLP 领域为例，模型从 BERT 发展到 GPT，模型规模从数亿参数量增加到数百亿甚至是数千亿。当参数规模为千亿时，存储模型参数就需要数百 GB 的显存空间，超出单个 GPU 卡的显存容量。显然，仅靠数据并行无法满足超大规模模型训练对于显存的需求。为了解决这个问题，可以采用模型并行技术。与数据并行在不同设备都有完整的计算图不同，模型并行是不同设备负责单个计算图不同部分的计算。

模型并行从计算图的切分角度，可以分为以下几种：

1. 按模型的层切分到不同设备，即**层间并行**，称为**流水线并行**。

2. 将层内的参数切分到不同设备，即**层内并行**，称为**张量并行**，这里讲张量并行。

![层间并行]()

![层内并行]()

张量并行总体来说就是将张量操作划分到多个设备上，以加速计算或增加模型大小；对模型每一层的层内参数进行切分，即对参数矩阵切片，并将不同切片放到不同 GPU 上；比如将原本在单卡中的矩阵乘法，切分到不同卡中进行矩阵乘法。训练过程中，正向和反向传播计算出的数据通过使&#x7528;**`All gather`**&#x6216;&#x8005;**`All reduce`**&#x7684;方法完成整合。

![]()

如上图，在 Tansformer 中，张量并行会把 **MHA** 和 **FFN** 都进行切分以并行化。利用 Transformer 网络的结构，通过添加一些同步原语来创建一个简单的模型并行实现。张量并行适用于模型单层网络参数较大的情况。同时缺点也是十分明显：

> 1. 当环境是多机多卡，张量并行所需的 **all-reduce** 通信需要跨服务器进行连接，这比单机多 GPU 服务器内的高带宽通信要慢，因为机间通信比卡间通信成本高
>
> 2. 高度的模型并行会产生很多小矩阵乘法，这可能会降低 GPU 的利用率

**张量并行**解决两个问题：

> 1. **切分方式**：参数如何切分到不同设备
>
> 2. **数学等价**：切分后，如何保证数学一致性

Transformer 结构主要由嵌入式表示 **Embedding** 层、矩阵乘 **MatMul** 层和交叉熵 loss 计算 **CrossEntropyLoss** 层构成。以上三种类型的组网层有较大的特性差异，需要设计对应的张量模型并行策略，但总体上看核心思想都是利用分块矩阵的计算原理，实现其参数切分到不同的设备。下面详细介绍这三种层的切分方式：

**Embedding 切分**

对于 **Embedding** 算子，如果总的词表非常大，会导致单卡显存无法容纳 **Embedding** 层参数。

**例**：当词表数量是$$50304$$，词表表示维度为$$5120$$，类型&#x4E3A;**`FP32`**，那么整层参数需要显存大约为$$\frac{50304\times 5120\times 4}{1024\times 1024}=982\text{MB}$$，反向梯度同样需要$$982\text{MB}$$，仅仅存储就需要将近$$2\text{GB}$$。

对于 **Embeding** 层的参数，可以按照词的维度切分，即每张卡只存储部分词向量表，然后通&#x8FC7;**`AllReduce`**&#x6C47;总各个设备上的部分词向量结果，从而得到完整的词向量结果。

![Embedding 切分]()

上图描述了单卡 **Embedding** 和 **Embedding** 两卡张量模型并行的示意图。在单卡上，执行 Embedding 操作，bz是 batch size 大小，**Embedding** 的参数大小&#x4E3A;**`[word_size, hidden_size]`**，计算得&#x5230;**`[bz, hidden_size]`**&#x5F20;量。下半部分为 **Embedding** 张量模型并行示例，其将 **Embedding** 参数沿 **word\_size** 维度，切分为两块，每块大小&#x4E3A;**`[word_size/2, hidden_size]`**，分别存储在两个设备上，即每个设备只保留一半的词表。当每张卡查询各自的词表时，如果无法查到，则该词的表示&#x4E3A;**`0`**，各自设备查询后得&#x5230;**`[bz, hidden_size]`**&#x7ED3;果张量，最后通&#x8FC7;**`AllReduceSum`**&#x901A;信，跨设备求和得到完整的全量结果，可以看出这里的输出结果和单卡执行的结果一致。

**MatMul 切分**

矩阵乘的张量模型并行充分利用矩阵分块乘法的原理。假设要实现如下矩阵乘法$$Y=XA$$，其中$$X$$是维度为$$M\times N$$的输入矩阵，$$A$$是维度为$$N\times K$$的参数矩阵，$$Y$$是结果矩阵，维度为$$M\times K$$。如果参数矩阵$$A$$非常大，甚至超出单张卡的显存容量，那么可以把参数矩阵$$A$$切分到多张卡上，并通过集合通信汇集结果，保证最终结果在数学计算上等价于单卡计算结果。这里，参数矩阵$$A$$存在两种切分方式：

**列切分**

![]()

参数矩阵$$A$$按列切块。如上图所示，将矩阵$$A$$按列切成$$A=[A_1| A_2]$$，分别将$$A_1, A_2$$放置在两张卡上。两张卡分别计算$$Y_1=XA_1$$和$$Y_2=XA_2$$。计算完成后，通&#x8FC7;**`AllGather`**&#x64CD;作获取其它卡上的计算结果，拼接在一起得到最终的结果矩阵$$Y$$。



**行切分**

![]()

参数矩阵$$A$$按行切块。如上图所示，将矩阵$$A$$按行切成$$A=\begin{bmatrix}A_1 \\
A_2\end{bmatrix}$$，为了满足矩阵乘法规则，输入矩阵$$X$$需要按列切分$$X=[X_1|X_2]$$。同时将矩阵分块，分别放置在两张卡上，每张卡分别计算$$Y_1=X_1A_1$$，$$Y_2=X_2A_2$$。计算完成后，通&#x8FC7;**`AllReduce`**&#x5F52;约其他卡上的计算结果，可以得到最终的结果矩阵$$Y$$。

Transformer 中的 **FFN** 结构均包含两层全连接层，即存在两个矩阵乘，这两个矩阵乘分别采用上述两种切分方式，如右图所示。对第一个全连接层的参数矩阵按列切块，对第二个全连接层参数矩阵按行切块。这样第一个全连接层的输出恰好满足第二个全连接层数据输入要求即按列切分，因此可以省去第一个全连接层后&#x7684;**`AllGather`**&#x901A;信操作。

![]()

**CrossEntropyLoss 计算**

分类网络最后一层一般会选用 softmax 和 cross\_entropy 算子来计算交叉熵损失。如果类别数量非常大，会导致单卡显存无法存储和计算 logit 矩阵。针对这一类算子，可以按照类别数维度切分，同时通过中间结果通信，得到最终的全局的交叉熵损失。

首先计算的是 softmax 值，如下公式：

$$x_\text{max} = \max_N(\max_k(x_k))$$

$$\text{softmax}(x_i)=\frac{e^{x_i}}{\sum_je^{x_j}}=\frac{e^{x_i-x_\text{max}}}{\sum_je^{x_j-x_\text{max}}}=\frac{e^{x_i}-x_\text{max}}{\sum_N\sum_ke^{x_j-x_\text{max}}}$$

其中$$N$$表示张量模型并行的设备号



![]()

得到 softmax 之后，同时对标签 target 按类别切分，每个设备得到部分 loss，最后在进行一次通信，得到全量的 loss。整个过程，只需要进行三次小量的通信，就可以完成 **CrossEntropyLoss** 的计算，流程图如图所示。

### 5.4.3 **流水线并行**

模型切分一般有两种方式：参数（Tensor）切分和图切分。在上一节张量并行中介绍过，张量并行可以把 shape 较大的参数切分到多个卡，从而有效减小在每个卡上的参数量。但是，采用这种方式在单机 8 卡上，&#x5982;**`32GB V100`**，最多训练 **10B** 左右的 **Dense** 模型，而且由于其通信和计算不能重叠的特点一般不适合多机之间使用。更大的模型需要从层级别进行进一步的图切分以减少单卡存储的容量需求，同时隐藏卡之间的通信时间，更加充分利用 GPU 卡的算力。为此业界提出了**流水线并行**的方式解决上述切分和调度的问题。流水线原理是将不同的层分配给指定 GPU 进行计算，流水线并行只需其之间点对点地通讯传递部分信息。具体步骤包括：

> 1. 在流水线并行之中，一个模型的各层会在多个GPU上做切分
>
> 2. 一个 batch 被分割成较小的 micro-batch，并在这些微批上进行流水线式执行
>
> 3. 通过流水线并行，一个模型的层被分散到多个设备上
>
> 4. 当用于具有相同 Transformer 块重复的模型时，每个设备可以被分配相同数量的 Transformer 层
>
> 5. 在流水线模型并行中，训练会在一个设备上执行一组操作，然后将输出传递到流水线中下一个设备，下一个设备将执行另一组不同操作

广义上讲，流水线并行从图切分的角度，将模型进一步按层切分到不同的设备上，相邻设备间在计算时只需要传递邻接层的中间变量和梯度。如下图左所示，这里对 FFN 采用**张量并行**，把全连接层参数切分到设备内的不同卡上，在同一设备内进&#x884C;**`AllReduce`**&#x6C42;和的全量通信参数为$$2MK$$，$$M$$是矩阵行维度，$$K$$是矩阵列维度。而采用流水线并行将 FFN 层作为切分点切分模型层，在设备间只需要发送或接收$$MK$$的参数量，因此相比张量模型并行，流水线并行通信参数量更少，如下图右所示。这里展示的是 Gpipe 中的朴素流水线并行示例，同一时刻只有一个设备进行计算，其余设备处于空闲状态，计算设备利用率通常较低。

![张量并行]()

![流水线并行]()

与之相对应，将朴素流水线并行的 batch 再进行切分，减小设备间空闲状态的时间，可以显著提升流水线并行设备利用率。如下&#x56FE;**`F-then-B`**&#x8C03;度方式所示，将原本的数据并行切分后的 **mini-batch** 划分成多个 **micro-batch**，每个流水线并行的计算单元先整体进行前向计算，再进行反向计算。通过在同一时刻分别计算模型的不同部分，**F-then-B** 可以显著提升设备资源利用率。但也不难看出这种 **F-then-B** 模式由于缓存了多个 **micro-batch** 的中间变量和梯度，显存的实际利用率并不高。

![]()

由此产生&#x51FA;**`1F1B`**&#x6D41;水线并行方式解决了这个问题。在 **1F1B** 模式下，前向计算和反向计算交叉进行，可以及时释放不必要的中间变量，如下图所示。**1F1B** 方式相比 **F-then-B** 方式峰值显存可以节&#x7701;**`37.5%`**，对比朴素流水线并行峰值显存明显下降，设备资源利用率显著提升。

![]()

**例**：以上图 **1F1B** 中 stage4 &#x7684;**`F42`**&#x4E3A;例，**`F42`**&#x5728;计算前，**`F41`**&#x7684;反&#x5411;**`B41`**&#x5DF2;经计算结束，即可释&#x653E;**`F41`**&#x7684;中间变量，从&#x800C;**`F42`**&#x53EF;以复&#x7528;**`F41`**&#x4E2D;间变量的显存。

**F-then-B** 和 **1F1B** 调度方式都采用增加 batch 数量，即将 **mini-batch** 切分为 **micro-batch** 增加设备资源利用率。这里从 **micro-batch** 角度从理论上推导下计算空闲，即 **bubble&#x20;**&#x548C; **micro-batch** 实际计算时间之间的关系。bubble 时间的计算公式：

$$t_\text{pb}=(p-1)(t_f+t_b)$$

其中$$t_\text{pb}$$是一个 mini-batch 中 bubble 的时间，$$p$$是流水线计算单元的数量，$$t_f$$是每个 micro-batch 的前向时间，$$t_b$$是每个 micro-batch 的反向时间。

$$t_\text{id}=m(t_f + t_b)$$

其中$$t_\text{id}$$是一个 mini-batch 的计算时间，$$m$$是一个 mini−batch 中 micro-batch 的数量。由上述两个公式可以得到：

$$S_\text{Bubble}=\frac{t_\text{pb}}{t_\text{id}}=\frac{p-1}{m}$$

由此不难看出，在流水线计算单元数$$p$$一定的情况下，影响 bubble 占比的是一个 mini-batch 中 micro-batch 的数量$$m$$。在保证$$m\gg p$$的情况下，可以有效降低 bubble 占比。

### 5.4.4 **序列并行**

目前，有两篇关于序列并行的文章：

> 1. **Colossal-AI** 发表的论文：[**Sequence Parallelism**](https://arxiv.org/pdf/2105.13120)
>
> 2. **Megatron-LM** 发表的论文：[**Sequence Parallelism**](https://arxiv.org/pdf/2205.05198)

这两者解决的问题并不相同，前者主要是解决模型的输入长度限制，而后者是主要是减少模型显存的。下面分别讲解。

* **Colossal-AI 序列并行**

**Colossal-AI** 序列并行诞生的背景是 Self Attention 的内存需求是输入长度的平方。其复杂度为$$O(n^2)$$，其中，$$n$$是序列长度。也就是说长序列数据将增加中间内存使用量，从而限制设备的训练能力。而现有的工作侧重于从算法的角度降低时间和空间复杂度。因此，作者提出了序列并行，这是一种内存高效的并行方法，可以打破输入序列长度限制，并在 GPU 上有效地训练更长的序列；同时，该方法与大多数现有的并行技术兼容，如**数据并行**、**流水线并行**和**张量并行**。并且序列并行不再需要单个设备来保存整个序列。 即在稀疏注意力的情况下，序列并行能够训练具有无限长序列的 Transformer。这里展示了序列并行与流水线并行和张量并行的区别

![]()

具体来说就是将输入序列分割成多个块，并将每个块输入到其相应的 GPU 设备中。为了计算注意力输出，将环状通信与自注意力计算相结合，并提出了环自注意力 **RSA**（**R**ing **S**elf-**A**ttention），如下图所示。

![在设备之间传输 key 以计算注意力得分]()

![在设备之间传输 value 以计算注意力层的输出]()

实验表明，当按批量大小和序列长度进行缩放时，序列并行表现良好。当扩展&#x5230;**`64`**&#x4E2A; NVIDIA P100 GPU 时，与张量并相比，该算法分别实现&#x4E86;**`13.7`**&#x500D;&#x548C;**`3.0`**&#x500D;的最大批量大小和序列长度。

通过稀疏注意力，序列可以处理具有超&#x8FC7;**`114K`**&#x4E2A; Token 的序列，这比现有的在单个设备上保存整个序列的稀疏注意力运行长度超&#x8FC7;**`27`**&#x500D;。

除此之外，与张量并行和流水线并行不同，序列并行不受超参数，如**注意力头数**、**层数**等的限制。 因此，只要序列长度能被序列并行大小整除，序列并行就可以使用。

* **Megatron-LM 序列并行**

**Megatron-LM** 的初衷是考虑通过其他方式分摊张量并行中无法分摊的显存，因此提出了序列并行的方法。**Megatron-LM** 借用了 **Colossal-AI** 把 Sequence 这个维度进行平均划分的思想。在张量的基础上，将 Transformer 层中的 **LayerNorm** 以及 **Dropout** 的输入按输入长度维度进行了切分，使得各个设备上面只需要做一部分的 **Dropout** 和 **LayerNorm** 即可。

这样做的好处有：

> 1. **LayerNorm&#x20;**&#x548C; **Dropout&#x20;**&#x7684;计算被平摊到了各个设备上，减少了计算资源的浪费
>
> 2. **LayerNorm&#x20;**&#x548C; **Dropout&#x20;**&#x6240;产生的激活值被平摊到各个设备上，进一步降低了显存开销

Megatron-LM 首先分析了 Transformer 模型运行时的显存占用情况。假设输入长度为$$s$$，batch size为$$b$$，hidden dim为$$h$$，attention head数量为$$a$$，则每一层 Transformer 显存占用为$$sbh(34+5\frac{as}{h})$$

![Transformer 模型结构]()

当开启了张量并行之后，Transformer 层中的部分模块的显存可以被分摊到不同的设备之间。如右图所示，不能被分摊的部分主要是两个 **LayerNorm** 块的输入和输出$$4bsh$$；两个 **dropout mask** 块$$2bsh$$；一共是$$10bsh$$。

![带有张量并行的 Transformer 层]()

假设张量并行大小为$$t$$，因此，每个设备每一层 Transformer 的显存占用为$$sbh(10+\frac{24}{t}+5\frac{as}{ht})$$

下面开启张量并行以及序列并行，Transformer 层中的 **LayerNorm** 和 **Dropout** 块也会被切分，对 Tensor 在 Sequence 维度进行切分，切分数量等于张量并行大小。

![带有张量并行和序列并行的 Transformer 层]()

每个设备每一层 Transformer 的显存占用为$$sbh(\frac{10}{t}+\frac{24}{t}+5\frac{as}{ht})=sbh(\frac{34}{t}+5\frac{as}{ht})$$

当然，做了额外的切分就会带来通信方式的改变。Transformer 层的张量并行通信是由正向传播两&#x4E2A;**`All-Reduce`**&#x4EE5;及反向传播两&#x4E2A;**`All-Reduce`**&#x7EC4;成。

而序列并行由于对 Sequence 维度进行了划分，**`All-Reduce`**&#x5728;这里已经不合适了。为了收集在各个设备上进行序列并行所产生的结果，需要插&#x5165;**`All-Gather`**&#x7B97;子；而为了使得张量并行所产生的结果可以传入序列并行层，需要插&#x5165;**`Reduce-Scatter`**&#x7B97;子。

![带有张量并行和序列并行的 MLP 层]()

在右图中，$$g$$所代表的就是前向传播的 **All-Gather**，反向传播的 **Reduce-Scatter**，$$\overline{g}$$则是相反的操作。

因此在 Megatron-LM 同时开启序列并行和模型并行时，每一个 Transformer 层完成一次前向传播和反向传播一共&#x6709;**`4`**&#x4E2A; **All-Gather** &#x548C;**`4`**&#x4E2A; **Reduce-Scatter** 算子。乍一看，通信的操作比 Megatron-LM 仅开启张量并行多，但其实不然，因为一个 **All-Reduce** 就相当于一个 **Reduce-Scatter** 和一个 **All-Gather**，所以他们的总通信量是一样的。通过添加序列并行并没有增加额外的通信开销，反而在后向传播代码的实现上，还把 **Reduce-Scatter** 和权重梯度的计算做了重叠，进一步减少了通信所占用的时间，使得提高设备的 FLOPs Utilization 成为了可能。

作者发现在 Transformer 层里有一些操作是产生的激活值大，但计算量小。因此需要去掉这一部分的激活值，通过选择性激活重新计算（Selective Activation Recomputation）来进一步降低显存。与此同时，其他的激活值就通通保存，以节省重计算量。通过对激活值的占比分析，序列并行降低&#x4E86;**`40%`**&#x5DE6;右的激活值开销。选择性激活重新计算也降低&#x4E86;**`40%`**&#x5DE6;右的激活值开销。当两个特性都打开的时候，总共可以降&#x4F4E;**`80%`**&#x5DE6;右的激活值开销，尽管比全部激活值重计算的结果要稍高，但是在吞吐率上的提升还是非常的明显的。

### 5.4.5 **MoE 并行**







## 5.5 系统优化

> **论文**：**[MPT](https://arxiv.org/pdf/1710.03740)  [FlashAttention-1](https://arxiv.org/pdf/2205.14135)  [FlashAttention-2](https://arxiv.org/pdf/2307.08691)  [FlashAttention-3](https://arxiv.org/pdf/2407.08608)**
>
> 代码：[**FlashAttention-1**](https://github.com/HazyResearch/flash-attention)

### 5.5.1 混合精度训练

通常训练神经网络模型的时候默认使用的数据类型为单精&#x5EA6;**`FP32`**。近年来，为了加快训练时间、减少网络训练时候所占用的内存，并且保存训练出来的模型精度持平的条件下，业界提出越来越多的混合精度训练的方法。这里的混合精度训练是指在训练的过程中，同时使用单精&#x5EA6;**`FP32`**&#x548C;半精&#x5EA6;**`FP16`**。

* **浮点数据类型**

浮点数据类型主要分为双精&#x5EA6;**`FP64`**、单精&#x5EA6;**`FP32`**、半精&#x5EA6;**`FP16`**。在神经网络模型的训练过程中，一般默认采用单精度 **FP32** 浮点数据类型，来表示网络模型权重和其他参数。在了解混合精度训练之前，这里简单了解浮点数据类型。

根据 **IEEE&#x20;**&#x4E8C;进制浮点数算术标&#x51C6;**`IEEE 754`**&#x7684;定义，浮点数据类型分为双精度 **FP64**、单精度 **FP32**、半精度 **FP16** 三种，其中每一种都有三个不同的位来表示。

![]()

**FP64** 表示的是采&#x7528;**`8`**&#x4E2A;字节&#x5171;**`64`**&#x4F4D;来进行编码存储的一种数据类型；同理，**FP32&#x20;**&#x8868;示采&#x7528;**`4`**&#x4E2A;字节&#x5171;**`32`**&#x4F4D;来表示；**FP16&#x20;**&#x5219;是采&#x7528;**`2`**&#x5B57;节&#x5171;**`16`**&#x4F4D;来表示，如上图所示。从图中可以看出，与 **FP32** 相比，**FP16** 的存储空间是 **FP32** 的一半，**FP32** 则是 **FP64** 的一半。每个浮点数都分为三个部分：

> 1. 最高位表示**符号位`sign`** bit
>
> 2. 中间表示**指数位`exponent`** bit
>
> 3. 低位表示**分数位`fraction`** bit

**例**：假设使用 **FP16**，第一位符号&#x4F4D;**`sign`**&#x8868;示正负符&#x53F7;**，用$$S$$表示，**&#x63A5;着 **5** 位表示指&#x6570;**`exponent`，用**$$E$$**表示**，最后 **10** 位表示分&#x6570;**`fraction`，用**$$M$$**表示**。公式为：$$x=(−1)^S×2^{E−15}×(1+\frac{M}{1024})$$

同理，一个规则化的 **FP32** 的真值为：$$x=(−1)^S×2^{E−127}×(1+\frac{M}{2^{23}})$$；一个规格化的 **FP64** 的真值为：$$x=(−1)^S×2^{E−1023}×(1+\frac{M}{2^{52}})$$。

**FP16** 的数值表示范围取决于其格式和特殊值的定义。根据 IEEE 754 标准，**FP16** 的数值范围可以分为以下几部分：**正规数范围** 、**非规格化数范围** 和 **特殊值**:

**正规数范围**

正规数是指指数位$$E\ne0$$且$$E\ne31$$的数。指数的实际值为$$E−15$$；尾数部分隐含一个前&#x5BFC;**`1`**，因此尾数的范围是 **`[1,2)`**。

**最大正规数**：指数位$$E = 30$$，尾数部分全为 1，即$$M = 1023$$，因此最大正规数为：

$$x_\text{max}=(−1)^0×2^{15}×(1+\frac{1023}=65504$$

**最小正规数**：指数位$$E = 1$$，尾数部分全为 0，因此最小正规数为：

$$x_\text{min, normal}=(−1)^0×2^{−14}×(1+0)=2^{−14}=0.00006103515625$$

**非规格化数范围**

非规格化数是指指数位$$E=0$$的数。这时指数固定为 $$−14$$；尾数部分没有隐含的前导`1`，为 $$\frac{M}{1024}$$。

**最小非零数**：指数位$$E = 0$$，实际指数为$$-14$$；尾数部分最低有效位$$M = 1$$，即尾数为$$\frac{1}{1024}$$。因此最小非零数为：

$$x_\text{min, subnormal}=(−1)^0×2^{−14}×\frac{1}{1024}=2^{−24}=5.9604644775390625×10^{−8}$$



**特殊值**

**零**：当$$E=0$$且$$M=0$$时表示零。符号位$$S$$决定是正零$$+0$$还是负零$$−0$$。

**无穷大**：当$$E=31$$且$$M=0$$时表示无穷大。符号位$$S$$决定是正无穷$$+∞$$还是负无穷$$−∞$$。

**NaN**：当$$E=31$$且$$M\ne0$$时，表示 **NaN**（**N**ot **a** **N**umber）。

总的来说，FP16 的数值表示范围如下：

> 1. **正规数范围**：正数$$[2^{-14}, 65504]$$，负数$$[-65504, -2^{-14}]$$
>
> 2. **非规格化数范围**：正数$$[2^{-24}, 2^{-14})$$，负数$$(-2^{-14}, -2^{-24}]$$
>
> 3. **特殊值** ：零$$+0$$和$$−0$$，无穷大$$+\infty$$和$$-\infty$$，NaN
>
> 4. **完整数值范围**：$$[−65504,−5.96×10^{−8}]∪\{−0,+0\}∪[5.96×10^{−8},65504]$$

使用 **FP16&#x20;**&#x8BAD;练神经网络，相对比使用 **FP32&#x20;**&#x6709;一些优点和缺点：

**优点**

1. **减少内存占用**：**FP16&#x20;**&#x7684;位宽是 **FP32** 的一半，因此权重等参数所占用的内存也是原来的一半，节省下来的内存可以放更大的网络模型或者使用更多的数据进行训练。

2. **加快通讯效率**：针对分布式训练，特别是在大模型训练的过程中，通讯的开销制约了网络模型训练的整体性能，通讯的位宽少了意味着可以提升通讯性能，减少等待时间，加快数据的流通。

3. **计算效率更高**：在特殊的 AI 加速芯片如华为Ascend 910 和 310 系列或 NVIDIA VOTAL 架构的 Titan V and Tesla V100 的 GPU 上，使用 **FP16** 的执行运算性能比 **FP32** 更加快。

**缺点**

1. **数据溢出**：**FP16** 相比 **FP32** 的有效范围要窄很多，使用 **FP16** 替换 **FP32** 会出现上&#x6EA2;**`Overflow`**&#x548C;下&#x6EA2;**`Underflow`**&#x7684;情况。而在深度学习中，需要计算网络模型中权重的梯度，因此梯度会比权重值更加小，往往容易出现下溢情况。

2. **舍入误差**：**`Rounding Error`**&#x6307;示是当网络模型的反向梯度很小，一般 **FP32** 能够表示，但是转换到 **FP16** 会小于当前区间内的最小间隔，会导致数据溢出。&#x5982;**`0.00006666666`**&#x5728; **FP32** 中能正常表示，转换到 **FP16** 后会表示成&#x4E3A;**`0.000067`**，不满足 **FP16** 最小间隔的数会强制舍入。

为了想让深度学习训练可以使用 **FP16** 的好处，又要避免精度溢出和舍入误差。于是可以通过 **FP16** 和 **FP32** 的**混合精度训练**，混合精度训练过程中可以引入**权重备份 Weight Backup**、**损失放大 Loss Scaling**、**精度累加 Precision Accumulated** 三种相关的技术。

* **权重备份 Weight Backup**

这种方法主要是用于解决舍入误差的问题。其主要思路，可以概括为：weights, activations, gradients 等数据在训练中都利用 FP16 来存储，同时拷贝一份 **FP32** 的 weights，用于更新。由于在更新权重公式为$$w=w+η∗g$$，在深度模型中，$$η∗g$$的参数值可能会非常小，利用 **FP16** 来进行相加的话，则很可能会出现舍入误差问题，导致更新无效。

![]()

因此通过将权重 weights 拷贝成 **FP32** 格式，并且确保整个更新过程是在 **FP32** 格式下进行的。即$$w_{32} = w_{32} + \eta * g_{16}$$

权重用 **FP32** 格式备份一次，使得内存占用反而更高，额外拷贝一份 weights 增加了训练时候内存的占用。 但是实际上，在训练过程中内存中分为动态内存和静态内容，其中动态内存是静态内存&#x7684;**`3-4`**&#x500D;，主要是中间变量值和激活 activations 的值，而这里备份的权重增加的主要是静态内存。只要动态内存的值基本都是使用 **FP16** 来进行存储，则最终模型与整网使用 **FP32** 进行训练相比起来，内存占用也基本能够减半。

* **损失缩放 Loss Scaling**

如下图左所示，如果仅仅使用 **FP32** 训练，模型收敛得比较好，但是如果用了混合精度训练，会存在网络模型无法收敛的情况。原因是梯度的值太小，使用 **FP16** 表示会造成了数据下溢出 Underflow 的问题，导致模型不收敛，如图中灰色的部分。于是需要引入损失缩放 Loss Scaling 技术。

![]()

![]()

上图右是在网络模型训练阶段， 某一层的激活函数梯度分布式中，其中&#x6709;**`68%`**&#x7684;网络模型激活参数位 **0**，另外&#x6709;**`4%`**&#x7684;精度在$$[2^{-32}, 2^{-20}]$$这个区间内，直接使用 FP16 对这里面的数据进行表示，会截断下溢的数据，所有的梯度值都会变为 **0**。

右图&#x662F;**`SSD`**&#x68C0;测器网络在 **FP32** 训练过程中收集的各层激活梯度值直方图。可以看到，**FP16** 的大部分可表示范围都未使用，而许多值都低于最小可表示范围，变成了零。因此如果直接使用 **FP16** 进行训练，网络是发散的。但如果我们把梯度缩放 8 倍，即指数增加 3 倍，那么小于$$2^{−27}$$的梯度值才会变为 0，$$[2^{−27},2^{−24})$$的值得以保留，此时网络就可以正常训练了。

![]()

![]()

为了解决梯度过小的问题，论文中对计算出来的 loss 值进行缩放，由于链式法则的存在，loss 上的缩放会作用也会作用在梯度上。这样比起对每个梯度进行缩放更加划算。缩放过后的梯度，就会平移到 **FP16** 有效的展示范围内。这样，缩放过的梯度就可以一直使用 **FP16** 进行存储了。只有在进行更新的时候，才会将缩放过的梯度转化为 **FP32**，同时将缩放抹去。

可以选择一个恒定的缩放因子，论文使用&#x4E86;**`8`**&#x5230;**`32K`**&#x8BAD;练了各种网络；可以根据经验选择。如果有梯度统计数据，可以选择一个较大的因子，同时保证其与最大绝对梯度值的乘积小&#x4E8E;**`65504`**，即 **FP16** 的最大值。只要不导致溢出，缩放因子较大也没有坏处。通过检查计算出的权重梯度，例如权重梯度值未缩放时，可以有效地检测到溢出。一种方法是在检测到溢出时跳过权重更新，直接进入下一次迭代。

损失放大是需要结合混合精度实现的，其主要的主要思路是：

> 1. **Scale Up** 阶段：网络前向计算后在反响传播前，将损失变化值增大$$2^K$$倍
>
> 2. **Scale Down** 阶段：反向传播后将权重梯度缩$$2^K$$倍，恢复 **FP32&#x20;**&#x503C;进行存储

**动态损失缩放 Dynamic Loss Scaling**

上面提到的损失缩放都是使用一个默认值对损失值进行缩放，为了充分利用 **FP16** 的动态范围，可以更好地缓解舍入误差，尽量使用比较大的放大倍数。总结动态损失缩放算法，就是每当梯度溢出时候减少损失缩放规模，并且间歇性地尝试增加损失规模，从而实现在不引起溢出的情况下使用最高损失缩放因子，更好地恢复精度。

动态损失缩放的算法如下：

> 1. 动态损失缩放的算法会从比较高的缩放因子开始，如$$2^{24}$$，然后开始进行训练迭代中检查数是否会溢出：
>
>    1. 没有梯度溢出：不进行缩放，继续进行迭代
>
>    2. 梯度溢出：缩放因子减半，重新确认梯度更新情况，直到数不产生溢出的范围内
>
> 2. 在训练的后期，loss 已经趋近收敛稳定，梯度更新的幅度往往小了，这个时候可以允许更高的损失缩放因子来再次防止数据下溢
>
> 3. 因此，动态损失缩放算法会尝试在每$$N=2000$$次迭代将损失缩放增加$$F$$倍数，然后检查是否溢出

* **精度累加 Precision Accumulated**

在混合精度的模型训练过程中，使用 **FP16** 进行矩阵乘法运算，利用 **FP32** 来进行矩阵乘法中间的累加，然后再将 **FP32** 的值转化为 **FP16** 进行存储。简单而言，就是利用 **FP16** 进行矩阵相乘，利用 **FP32** 来进行加法计算弥补丢失的精度。 这样可以有效减少计算过程中的舍入误差，尽量减缓精度损失的问题。

在 Nvidia Volta 结构中带&#x6709;**`Tensor Core`**，可以利用 **FP16** 混合精度来进行加速，还能保持精度。Tensor Core 主要用于实现 **FP16** 的矩阵相乘，在利用 **FP16** 或者 **FP32** 进行累加和存储。在累加阶段使用 **FP32** 能够大幅减少混合精度训练的精度损失。

![]()

* **混合精度训练策略** **AMP**（**A**utomatic **M**ixed **P**recision）

混合精度训练不仅仅是在深度学习，另外在HPC的迭代计算场景下，从迭代的开始、迭代中期和迭代后期，都可以使用不同的混合精度策略来提升训练性能的同时保证计算的精度。以动态的混合精度达到计算和内存的最高效率比是一个较为前沿的研究方向。

**例**：NVIDIA &#x7684;**`APEX`**&#x6DF7;合精度库，里面提供了4种策略，分别是

![使用 FP32 进行训练的O0           只优化前向计算部分的O1      除梯度更新外都使用混合精度的O2     使用 FP16 进行训练的O3    ]()

O1 策略中，会根据实际 Tensor 和操作之间的关系建立黑白名单来使用 FP16。例如 GEMM 和 CNN 卷积操作对于 FP16 操作特别友好的计算，会把输入的数据和权重转换成 FP16 进行运算，而 Softmax、BN 等标量和向量在 FP32 操作好的计算，则是继续使用 FP32 进行运算，另外还提供了动态损失缩放。

O2 策略中，模型权重参数会转化为 FP16，输入的网络模型参数也转换为 FP16，BN 使用 FP32，另外模型权重文件复制一份 FP32 用于跟优化器更新梯度保持一致都是 FP32，另外还提供动态损失缩放。使用了权重备份来减少舍入误差和使用损失缩放来避免数据溢出。

**总结**

混合精度训练的计算流程：

![]()

> 1. 参数以 **FP32** 存储
>
> 2. 正向计算，用 **FP16** 算子把参数从 **FP32** 转换成 **FP16** 进行计算
>
> 3. 将 Loss 层设置为 **FP32** 进行计算
>
> 4. 反向计算，乘以损失缩放因子，避免反向梯度过小而产生下溢
>
> 5. **FP16** 参数参与梯度计算，结果转换回**FP32**
>
> 6. 除以损失缩放因子，还原放大的梯度
>
> 7. 判断梯度是否存在溢出，溢出则跳过更新，否则优化器以 **FP32** 对参数进行更新

### 5.5.2 内存优化

* 梯度检查点：通过重计算减少显存占用

* 梯度累计

* 激活重计算：在前向传播中不保存中间激活值，反向传播时重新计算



### 5.5.3 FlashAttention-1

* **基础知识**

GPU 内存层次结构包含多种不同大小和速度的内存形式，**内存容量越小，读写速度越快**。&#x4EE5;**`A100 GPU`**&#x4E3A;例，主要有两种类型：

> 1. **高带宽内存`HBM`**：也就是 GPU 显存，**A100** 具有 **`40-80GB`** HBM，带宽为 **`1.5-2.0TB/s`**
>
> 2. **SRAM**：位于 GPU 片上，&#x6BCF;**`108`**&#x4E2A;流式多处理器都&#x6709;**`192KB`**&#x7684; SRAM，带宽约&#x4E3A;**`19TB/s`**

二者的位置分布不同，HBM 在 VRAM 结构中，而 SRAM 在 GPU 内部。

![]()

可以看到，片上 **SRAM** &#x6BD4;**&#x20;HBM** 快一个数量级，但内存容量小很多数量级。随着计算相对于内存速度变得更快，内存 **HBM** 访问越来越成为操作瓶颈。因此，利用快速 **SRAM** 变得更加重要。

GPU 有大量线程来执行操作，称为内核 **Kernel**。每个 **Kernel** 将输入从 **HBM** 加载到寄存器和 **SRAM**，进行计算，然后将输出写入 **HBM**。根据计算和内存访问的平衡，操作可以分为**计算限制**或**内存限制**。这通常通过算术强度来衡量，即内存访问的每个字节的算术运算数量：

> 1. **计算限制**：操作所花费的时间由算术运算的数量决定，而访问 **HBM** 的时间要少得多。典型的例子是大内部维度的矩阵乘法，以及大量通道的卷积
>
> 2. **内存限制**：操作所花费的时间由内存访问次数决定，而计算所花费的时间要少得多。如逐元素操&#x4F5C;**`Activation`**、**`Dropout`**&#x548C;归约操&#x4F5C;**`Sum`**、**`Softmax`**、**`BN`**、**`LN`**

![]()

如右图所示，**masking**，**softmax&#x20;**&#x548C; **dropout&#x20;**&#x662F;占用大量时间的操作，而不是矩阵乘法，即使大部分运算量是在 matmul 中。

* **常规 Attention**

传统的 Attention 计算伪代码如右表所示，下面展示了这一过程的详细步骤：

![]()

> 1. 计算$$Q,K,V$$矩阵，放在 **HBM** 中
>
> 2. 为了计算$$QK$$注意力得分，将$$Q,K$$从 **HBM** 中取出来，写入 **SRAM**，然后计算$$S=QK^\top$$，再把$$S$$从 **SRAM** 写入 **HBM**
>
> 3. 从 **HBM** 加载$$S$$到 **SRAM**，计算$$P=\rm{Softmax}(\it{S})$$，然后再把$$P$$从 **SRAM** 写入 **HBM**
>
> 4. 从 **HBM&#x20;**&#x52A0;载$$P,V$$到 **SRAM**，计算最终的输出$$O=PV$$，然后把$$O$$写入 **HBM**

可以看到，这个过程中存在多次 **HBM** 和 **SRAM** 之间的读写操作。同时由于 Attention 的计算方式，导致中间的临时变量$$S,P$$的参数量和输入序列长度的平方成正比。因此在训练时，长序列输入在计算 Attention 时会产生更大参数量的临时变量，会占用更大显存空间，导致更多的访问消耗。也就是说，Attention 操作主要是**内存限制**问题，通信时间是制约计算效率的主要因素。

* **FlashAttention 算法**

FlashAttention 主要的思想：减少通信时间，也就是减少 IO 操作，使得计算尽可能多的访问片上的 SRAM，尽可能少的访问片外的 HBM。论文主要包含两个重要贡献：

> 1. 通过分块计算，融合多个操作，减少中间结果缓存
>
> 2. 反向传播时，重新计算中间结果，类似于梯度检查点的原理

**除去 Softmax 操作的分块计算**

在计算 Attention 主要有两个临时变量$$S,P$$，FlashAttention 的分块计算，使得不需要存储这两个临时变量，而是直接在 SRAM 计算得到部分最终结果$$O$$，从而减少了内存访问开销。这里先忽略 Softmax 操作，因为在分块计算时比较麻烦。

**例**：假设$$Q,K,V$$矩阵的大小为$$(4, 3)$$，那么 FlashAttention 的分块计算过程如右图所示，由于矩阵乘法的性质，每次分块计算得到的结果，都是最终结果矩阵中的一部分值。



![]()

**Softmax 分块计算**

下面来解决 Softmax 分块计算，Softmax 的计算公式如下：

$$\text{softmax}({x_j})=\frac{e^{x_j}}{\sum_{i=1}^ke^{x_i}}$$

但是，如果数据类型&#x4E3A;**`FP16`**，那么最大可以表示&#x4E3A;**`65504`**，因此当$$x_i=12$$时，$$e^{12} = 162754$$，超过了 **FP1**所能表示的最大值。因此需要使&#x7528;**`Safe Softmax`**&#x65B9;法来避免这个问题：

令$$m(X) = \max(x_1, x_2, \dots, x_N)$$表示全局最大值，对于输入向量$$X = [x_1, x_2, \dots, x_N]$$，Softmax 计算公式为：

$$\text{softmax}(x_i) = \frac{e^{x_i - m(X)}}{\sum_{j=1}^N e^{x_j - m(X)}}$$

也就是在计算 Softmax 之前，先对输入数据进行归一化处理。此时计算 Softmax 的流程为：

> 1. $$X = [x_1, x_2, \dots, x_N]$$
>
> 2. $$m(X)=\max(x_1, x_2, \dots, x_N)$$
>
> 3. $$p(X)=\left[e^{x_1-m(X)},\cdots,e^{x_N-m(X)}\right]$$
>
> 4. $$l(X)=\sum_{i}p(X)_i$$
>
> 5. $$\text{softmax}(X)=\frac{p(X)}{l(X)}$$

接下来看分块处理时，假设这里分为两块处理，首先需要在每个块内找到最大值$$m(X^*)$$，类似的得到$$p(X^*),l(X^*)$$：

$$X=[x_1,\cdots,x_N,x_{N+1},\cdots,x_{2N}], X^1 = [x_1, x_2, \dots, x_N], X^2 = [x_{N+1},\dots, x_{2N}]$$

$$m(X^1)=\max(x_1, x_2, \dots, x_N), m(X^2)=\max(x_{N+1},\dots, x_{2N})$$

$$p(X^1)=\left[e^{x_1-m(X^1)},\cdots,e^{x_N-m(X^1)}\right], p(X^2)=\left[e^{x_{N+1}-m(X^2)},\cdots,e^{x_{2N}-m(X^2)}\right]$$

$$l(X^1)=\sum_{i}p(X^1)_i, l(X^2)=\sum_{i}p(X^2)_i$$

然后再计算数据的全局最大值，并且更新$$p(x),l(x)$$，最后计算得到输入数据的 Softmax 值。由于全局最大值，一定是各个块内最大值中的一个，因此在更新$$p(x),l(x)$$，只需要乘以每个块最大值相对于全局最大值的差值的指数，就可以了：

$$m(X)=m(\left[X^1, X^2\right])=\max(m(X^1),m(X^2))$$

$$p(X)=\left[e^{m(X^1)-m(X)}p(X^1), e^{m(X^2)-m(X)}p(X^2)\right]$$

$$l(X)=l(\left[X^1,X^2\right])=e^{m(X^1)-m(X)}l(X^1)+e^{m(X^2)-m(X)}l(X^2)$$

$$\text{softmax}(X)=\frac{p(X)}{l(X)}$$

**Attention 分块计算**

**FlashAttention** 的完整计算流程如下：

![]()

![在红色外层循环中，遍历 K 和 V 矩阵的分块，并加载到快速的片上 SRAM 中。在每个分块中，会遍历 Q 矩阵的分块，并加载到 SRAM 中，并将结果写回到 HBM 中。]()

这里详细展开讲解：

0\.  初始化：**HBM** 的容量以 GB 为单位衡量，因此可以直接分配$$Q, K$$和$$V$$。

1. 计算行/列块大小：这里取$$\lceil
   \frac{M}{4d}\rceil$$是因为$$q,k$$和$$v$$向量是$$d$$维的，而且还需要组合输出的$$d$$维向量。所以这个大小基本上允许用$$q,k,v$$和$$o$$向量最大化 SRAM 的容量。假设$$M = 1000, d = 5$$，那么块大小为$$\frac{1000}{4\times5}= 50$$。所以一次加&#x8F7D;**`50`**&#x4E2A;$$q, k, v, o$$向量的块，这样可以减少 HBM/SRAM 之间的读/写次数。

2. 用&#x5168;**`0`**&#x521D;始化输出矩阵$$O$$，它将作为一个累加器，$$l$$也类似，它的目的是保存 **softmax** 的累积分母，即 exp 分数的总和。$$M$$初始化为$$-\infty$$，保存逐行最大分数。因为是对其进行$$\max$$运算符，因此无论第一个块的$$\max$$是什么，它肯定大于$$-\infty$$。

3. 把$$Q∈R^{N×d}$$切分成$$T_r=⌈\frac{N}{B_r}⌉$$个大小为$$B_r×d$$的块，把$$K$$和$$V$$切分成$$T_c=⌈\frac{N}{B_c}⌉$$个大小为$$B_c×d$$的块。因此每次计算$$QK^\top V$$是$$B_r×d$$的$$Q_i$$和$$d×B_c$$的$$K^\top_j$$和$$B_c×d$$的$$V_j$$，这样得到的最终大小是$$(B_r×d)×(d×B_c)×(B_c×d)=(B_r×d)$$。

4. 根据前面的计算，结果矩阵$$O$$需要切分成$$B_r×d$$的块来存放中间结果。长度为$$N$$的$$l$$和$$m$$也要切分成$$B_r$$个元素的块，用于存放这些行当前的指数累加值和当前最大值。

5. 外循环，即跨列循环，跨$$K, V$$向量

6. 将$$K_j$$和$$V_j$$块从 HBM 加载到 SRAM。在这个时间点&#x6709;**`50%`**&#x7684; SRAM 未被占用，专用于$$Q$$和$$O$$。

7. 开始跨行内部循环，即跨$$Q$$向量。

8. 把$$Q_i(B_r×d)$$和$$O_i(B_r×d)$$加载进 SRAM，同时把$$l_i(B_r)$$和$$m_i(Br)$$也加载进去。$$Q_i$$和$$O_i$$会占据另一半的显存。而$$l_i$$和$$m_i$$比较小，可以放到寄存器里。

9) 计算分块矩阵$$Q_i(B_r×d)$$和$$K_j^\top(d×B_c)$$的乘积，得到score $$S_{ij}(B_r×B_c)$$。注意这里不需要计算$$N×N$$的得分$$S$$矩阵，而只需要很小的$$S_{ij}$$。假设外层循环下标$$j=3$$，内层循环下标$$i=2$$，$$N=25$$，块大小是$$5$$，那么计算如右图所示。这里计算的 attention 得分是$$Q$$为&#x7B2C;**`6-10`**&#x4E2A; token，$$K$$是&#x7B2C;**`11-15`**&#x4E2A; token。

10) 计算$$\tilde{m}_{ij}, \tilde{l}_{ij}$$和$$\tilde{P}_{ij}$$，使用前面的公式就可以简单的得出。$$\tilde{m}_{ij}$$是逐行计算的，找到每一行的最大值。$$\tilde{P}_{ij}$$是逐点运算，把$$S_{ij}$$减去第i行的最大值$$\tilde{m}_{ij}$$，然后在计算指数。$$\tilde{l}_{ij}$$也是逐行计算，把每一行的$$\tilde{P}_{ij}$$加起来。

11) 计算$$m^\text{new}_i$$和$$l^\text{new}_i$$。如右图，$$m_i$$包含了在当前块$$j=3$$之前所有行块的最大值，即保存了$$j=1$$和$$j=2$$的绿色块&#x7B2C;**`6~10`**&#x884C;的最大值。而$$\tilde{m}_{ij}$$是上一步得到的当前黄色块的最大值。因此取两者的最大值就得到前 3 个块，绿色加黄色块共 15 列，的最大值。$$l^\text{new}_i$$的计算也是类似的，只不过求和前需要用当前的$$e^{−m^\text{new}_i}$$修正。

![]()

![]()

12. 第12步公式的加号前半部分是更新当前块$$j=3$$之前的块$$j<3$$的 softmax 值，那么当更新当前块时，这些变量有可能都会变，如最大值$$m$$等。所以第一步需要重新计算。因为之前的$$PV$$没有保存，所以我们可以用$$l$$乘以$$O$$恢复出$$PV$$。论文中是矩阵的形式，也就是$$\text{diag}(l_i)×O_i$$。恢复出来的$$R$$再乘以$$e^{m_i−m^\text{new}_i}$$就是修正后的$$PV$$，也就是$$e^{x_i−\max(x)}v_j$$。而公式中加号后半部分是当前块$$j=3$$，$$e^{\tilde{m}_i−m^\text{new}_i}$$是当前块的最大值减去$$j\le3$$所有块的最大值，这是对当前指数$$\tilde{P}_{ij}$$的修正。由于第10步中$$\tilde{P}_{ij}=e^{S_{ij}-\tilde{m}_{ij}}$$，那么这里其实就是$$e^{S_{ij}−m^\text{new}_i}$$。最后把新的$$PV$$除以新的$$l$$存到$$O$$里，只不过这里的除非也是用矩阵乘法来表示，也就是最前面的$$(\text{diag}(l^\text{new}_i))^{−1}$$。因为对角矩阵的逆就是它对角线元素的逆，也就是变成了除法。

13. 把最新的累计量$$l_r,m_r$$写回 HBM，它们的大小都是$$B_r$$。

14. 终止内层循环

15. 终止外层循环

16. 返回结果$$O$$

* **多头注意力**

要扩展到 **batch\_size > 1** 和 **num\_heads > 1** 实际上并不难。算法基本上是由单个线程块处理的。这个线程块在单个流多处理&#x5668;**`SM`**&#x4E0A;执行，例如，A100 上有 108 个这样的处理器。为了并行化计算，只需要在不同的 **SM** 上并行运行 **batch\_size \* num\_heads** 线程块。该数字与系统上可用的 **SM** 数量越接近，利用率就越高。

* **反向传播**

在反向传播中，一旦输入的$$Q、K、V$$已经加载到 **SRAM**，通过重新计算注意力矩阵$$S$$和$$P$$的值，就可以避免存储较大的中间值。通过不保存大小为$$𝑁×𝑁$$的大矩阵$$S$$和$$P$$，**FlashAttention** 产&#x751F;**`10-20`**&#x500D;内存节省，取决于序列长度，这里序列长度$$𝑁$$为线性内存，而不是二次内存。由于减少内存读写，反向传播也实现&#x4E86;**`2-4`**&#x500D;的加速。

反向传播也应用了分块。虽然反向传播在概念上比前向传播更简单，因为没有 softmax 重缩放，但实现明显更复杂。这是因为在 SRAM 中有更多的值需要保留，以便在反向传播中执行 5 个矩阵乘法，而在前向传播中只有 2 个矩阵乘法。

### 5.5.4 FlashAttention-2



### 5.5.5 FlashAttention-3

# 6. 检索增强生成 RAG

## 6.1 概述

**RAG** 近年来发展迅速，如下图所示。在大模型时代，**RAG** 的发展轨迹呈现出几个明显的阶段特征：

> 1. 最初，**RAG** 的诞生与 **Transformer** 架构的兴起相吻合，其重点是通过预训练模型引入额外的知识来增强语言模型。这一早期阶段以改进预训练技术的基础性工作为特征。
>
> 2. 随后，**ChatGPT** 的出现成为了一个关键转折点，大型语言模型展示了强大的上下文学习能力。**RAG** 的研究方向逐渐转向在推理阶段为LLM提供更优质的信息，以应对更加复杂且知识密集型的任务，这推动了 **RAG** 研究的快速发展。
>
> 3. 随着研究的深入，**RAG** 的优化不再局限于推理阶段，而是开始更多地结合 **LLM** 的微调技术。

![]()

**RAG** 的一般流程如下图所示：用户向 LLM 提出一个问题。由于 LLM 依赖于预训练数据，它最初无法提供有关最新动态的信息更新。**RAG** 通过从外部数据库中获取并整合相关知识，弥补了这一信息鸿沟。这里 **RAG** 收集了与用户查询相关的新闻文章。这些文章与原始问题结合，形成一个全面的 prompt，使 LLM 能够生成一个基于充分信息的答案。

![]()

RAG 的研究在不断发展，从时间线上来说，可以分为三个阶段：

> 1. **初级 RAG**（Naive RAG）
>
> 2. **高级 RAG**（Advanced RAG）
>
> 3. **模块化 RAG**（Modular RAG）

* **初级 RAG**

这一范式是最早的 **RAG** 实现方法，在 **ChatGPT** 出来后就被广泛采用。初级 **RAG** 遵循一个传统的流程，包括索引、检索和生成。

1. **索引 Indexing**

首先清洗和提取多种格式的原始数据，&#x5982;**`PDF`**、**`HTML`**、**`Word`**&#x548C; **`Markdown`**。这些数据随后被转换为统一的纯文本格式。为了适应语言模型的上下文限制，文本被分割成更小、更易处理的片段。接着，这些片段通过 **Enbedding** 模型编码为向量表示，并存储在向量数据库中。这一步骤对于在后续检索阶段实现高效的相似性搜索至关重要。

* **检索 Retrieval**

当接收到用户的 Query 时，**RAG** 系统会使用与索引阶段相同的 **Embedding** 模型，将查询转化为向量表示。然后，系统计算查询向量与索引语料库中各片段向量之间的相似度得分。系统根据相似度得分优先选择并检索出与查询最相关的前 **K** 个片段。这些片段随后被用作 prompt 中的扩展上下文。

* **生成 Generation**

![]()

用户的 Query 和选中的文档被整合为一个连贯的 prompt，LLM 负责基于此提示生成回答。模型的回答方式可能因任务特定的标准而有所不同，它既可以依赖其固有的参数化知识，也可以仅限于所提供文档中的信息进行回答。在持续对话的情况下，任何现有的对话历史都可以被整合到提示中，从而使模型能够有效地参与多轮对话交互。

当然，初级的 **RAG** 存在很多局限性：

> 1. **检索挑战**：检索阶段常常面临精确性和召回率的问题，导致选择的片段可能与查询不匹配或无关，同时遗漏关键信息。面对复杂问题时，仅基于原始查询的单次检索可能不足以获取足够的上下文信息。
>
> 2. **生成困难**：在生成回答时，模型可能会出现幻觉问题，即生成的内容并未得到检索上下文的支持。此外，生成结果可能还存在无关性、毒性或偏见等问题，从而影响响应的质量和可靠性。而且生成模型可能过度依赖增强信息，导致输出只是简单地复述检索到的内容，而未能提供有洞察力或综合性的信息。
>
> 3. **增强障碍**：将检索到的信息与不同任务结合可能会带来挑战，有时会导致输出内容不连贯或支离破碎。当从多个来源检索到相似信息时，过程可能会遇到冗余问题，从而导致重复的回答。确定不同段落的重要性与相关性，以及确保风格和语气的一致性，进一步增加了复杂性。

* **高级 RAG**

高&#x7EA7;**&#x20;RAG&#x20;**&#x5F15;入了特定的改进措施，以解决初级 **RAG** 的局限性。为了提升检索质量，采用了 pre-retrieval 和 post-retrieval 策略。为了解决索引问题，高级 **RAG** 通过滑动窗口方法、细粒度分段以及元数据的引入来优化其索引技术。此外，还结合了多种优化方法以简化检索过程。

1. **Pre-retrieval**

此阶段重点在于优化索引结构和原始 Query。 &#x20;

* **Post-Retrieval**

将相关上下文被检索出来，与 Query 有效整合就显得尤为重要。此阶段的主要方法包括重新排序片段和上下文压缩。 &#x20;

![]()

* **模块化 RAG**

模块化 **RAG** 架构相比前两种 **RAG** 范式，提供了更强的适应性和多功能性。它通过多种策略改进组件，例如增加用于相似性搜索的搜索模块以及通过微调优化检索器。除此之外还引入重构的 RAG 模块和重新排列的 RAG 流程等。模块化 RAG 支持组件间的顺序处理和集成的端到端训练。目前模块化 RAG 用的越来越多。

**新模块**

模块化 RAG 框架引入了额外的专用组件来增强检索和处理能力：

![]()

> 1. **Search 模块**适应特定场景，能够利用 LLM 生成的代码和查询语言直接跨多种数据源进行搜索，如搜索引擎、数据库和知识图谱
>
> 2. **Fusion 模块**通过多查询策略扩展用户查询为不同视角，利用并行向量搜索和智能重排序来揭示显性和转化性知识，解决了传统搜索的局限性
>
> 3. **Memory** **模块**利用 LLM 的记忆引导检索，创建一个无界记忆池，通过迭代自我增强使文本更贴近数据分布
>
> 4. **Routing 模块**在多样数据源中导航，为查询选择最佳路径，无论是摘要生成、特定数据库搜索还是合并不同信息流
>
> 5. **Predict 模块**通过 LLM 直接生成上下文，减少冗余和噪声，确保相关性和准确性
>
> 6. **Adapter 模块**将 RAG 适配到各种下游任务，自动化零样本输入的提示检索，并通过少量样本查询生成创建任务专用检索器

这种综合方法不仅简化了检索过程，还显著提高了检索信息的质量和相关性，以更高的精度和灵活性满足广泛的任务和查询需求。

**新模式**

模块化 RAG 通过允许模块替换或重新配置，相比于初级和高级 RAG 的 Retrieve 和 Read 的固定结构，展现出显著的适应性。此外，模块化 RAG 通过集成新模块或调整现有模块之间的交互流程，进一步扩展了这种灵活性，增强了其在不同任务中的适用性。

> **例**：&#x50CF;**`Rewrite-Retrieve-Read`**&#x8FD9;种模型，利用了 LLM 的能力，通过一个重写模块和语言模型反馈机制来优化检索查询，并不断更新重写模型，从而提升任务表现。类似的，**`Generate-Read`**&#x65B9;法直接用大语言模型生成的内容取代传统检索，&#x800C;**`Recite-Read`**&#x5219;更注重从模型权重中提取信息，增强了模型处理知识密集型任务的能力。此外，混合检索策略结合了关键词、语义和向量搜索，以应对各种不同的查询需求。还有一种方法是使&#x7528;**`sub-queries`**&#x548C;**`HyDE`**，通过关注生成答案和真实文档之间的嵌入相似性，来提高检索的相关性。

调整模块的排列和交互方式，可以动态利用一个模块的输出来增强另一个模块的功能，这体现了提升模块协同作用的好处。模块化 **RAG** 流程的灵活设计展现了自适应检索的优势，这种方法会根据不同的场景来判断是否需要进行检索，强于传统固定的检索流程。此外，灵活的架构还让 **RAG** 更容易和其他技术结合起来，比如微调或者强化学习，从而进一步提升性能。例如，微调检索器以获得更好的检索结果，微调生成器以实现更个性化的输出，或进行协作微调。

* **RAG 与微调的抉择**

在优化 LLM 的方法中，**RAG** 经常和**微调 Fine-tuning** 以及**提示工程 prompt engineering** 作比较。每种方法都有自己的特点，如下图。这里用坐标系来对比这三种方法在两个维度上的差异：**对外部知识的需求**和**对模型适配的需求**：

![]()

> 1. **提示工程**利用模型本身的能力，几乎不需要外部知识，也不需要对模型进行太多调整
>
> 2. **RAG** 适合用来完成精准的信息检索任务
>
> 3. **微调**需要慢慢内化知识，适合需要复制特定结构、风格或格式的场景

**RAG** 在动态环境中表现突出，因为它能实时更新知识，还能高效利用外部知识源，而且解释性很强。不过，它也有缺点，比如延迟较高，还涉及到数据检索中的伦理问题。相比之下，微调更静态一些，虽然更新时需要重新训练，但它可以让模型的行为和风格深度定制。不过，微调需要大量的计算资源来准备数据集和进行训练，虽然能减少幻觉问题，但面对不熟悉的数据时可能会遇到困难。

研究还表明，在针对不同主题的知识密集型任务的多次评估中，尽管无监督微调有一些改进，但 **RAG** 在处理训练中遇到的已有知识和全新的知识时，始终表现得更好。此外 LLM 很难通过无监督微调学习到新的事实信息。

选择 **RAG** 还是微调，取决于应用场景中对数据动态性、定制化程度和计算能力的具体需求。**RAG** 和微调并不是非此即彼的关系，它们可以互相补充，在不同层面上提升模型的能力。有时候，把两者结合起来使用，可能会达到最佳效果。不过，涉及 **RAG** 和微调的优化过程可能需要多次迭代，才能得到满意的结果。

## 6.2 RAG 核心组件

### 6.2.1 检索模块

* **检索源**

RAG 依赖外部知识库，检索源的类型和检索单元的粒度都会影响最终的生成结果。

**数据结构**

最开始文本是检索的主要来源。后来检索源逐渐扩展到包括半结构化数据，&#x5982;**`PDF`**&#x7B49;，和结构化数据，如`知识图谱`，**`KG`**&#x7B49;，以进一步增强效果。除了从原始外部来源检索外，最近的研究还越来越多地倾向于利用 LLM 自身生成的内容来进行检索和优化。

1. **非结构化数据**：比如文本，是最广泛使用的检索来源，主要从语料库中获取。在**开放域问答 ODQA** 任务中，主要的检索来源是维基百科的数据集，目前主流版本包&#x62EC;**`HotpotQA`**&#x548C;**`DPR`**。此外，常见的非结构化数据还包括跨语言文本以及领域特定数据，如`医学`和`法律`领域。

2. **半结构化数据**：通常指包含文本和表格信息混合的数据，比&#x5982;**`PDF`**。处理半结构化数据对传统的RAG系统来说是个挑战，主要有两个原因：

   > 1. 文本分割过程可能会无意间拆散表格，导致检索时数据损坏
   >
   > 2. 将表格融入数据会增加语义相似性搜索的复杂性

   在处理半结构化数据时有两种主流的方法：

   > 1. 利用 LLM 的代码能力，在数据库中的表格上执行 **Text-2-SQL** 查询，比&#x5982;**`TableGPT`**
   >
   > 2. 将表格转换为文本格式，然后用基于文本的方法进行进一步分析

   但是目前这两种方法都有一定的弊端，还需要进一步探索。

3. **结构化数据**：比如知识图谱 **KGs**，通常是经过验证的，能够提供更精确的信息。

   > **例**：**`KnowledGPT`**&#x901A;过生成知识库搜索 Query 并将知识存储在个性化数据库中，增强了 RAG 模型的知识丰富性。**`G-Retriever`**&#x9488;对 LLM 在理解和回答基于文本图问题上的局限性，将图神经网络 GNN、LLM 和 RAG 结合起来，通过 LLM 的 soft prompting 提升对图的理解和问答能力，并利用 **PCST**（**P**rize-**C**ollecting **S**teiner **T**ree）优化问题实现针对性的图检索。

然而，结构化数据的构建、验证和维护结构化数据库需要额外的努力，这与非结构化数据不同。

* **LLM 生成内容**：为了解决RAG中外部辅助信息的局限性，一些研究聚焦于挖掘大语言模型的内部知识。

  > **例**：**`SKR`**&#x5C06;问题分类为已知或未知，有选择地应用检索增强。**`GenRead`**&#x76F4;接用 LLM 生成器取代了检索器，发现由于 LLM 生成的上下文与因果语言建模的预训练目标更一致，一般会包含更准确的答案。**`Selfmem`**&#x901A;过检索增强生成器迭代创建一个无限制的记忆池，并利用记忆选择器挑选出与原始问题形成**对偶问题**的输出，从而实现生成模型的自我增强。

**检索粒度**

除了检索源的数据格式外，另一个重要因素是检索单元的粒度。

> * **粗粒度的检索单元**可以为问题提供更多相关的信息，但可能也包含冗余内容，这会在下游任务中分散检索器和语言模型的注意力
>
> * **细粒度的检索单元**增加了检索的负担，但也不能完全保证语义的完整性和满足所需的知识需求

在推理过程中选择适当的检索单元粒度，可以提升密集检索器的性能以及下游任务的效果。

1. 在**文本**中，检索单元的粒度从细到粗包括：`词元`**` Token`**、`短语`**` Phrase`**、`句子`**` Sentence`**、`命题`**` Proposition`**、`段落块`**` Chunks`**、`文档`**` Document`**

   > **例**：**`DenseX`**&#x63D0;出了以命题作为检索单元的概念。命题被定义为文本中的原子表达，每个命题封装了一个独特的事实片段，并以简洁、自包含的自然语言格式呈现。这种方法旨在提高检索的精确性和相关性。

2. 在**知识图谱**中，检索单元的粒度包括：`实体`**` Entity`**、`三元组 `**`Triplet`**、`子图 `**`sub-Graph`**

这里检索单元的粒度还可以根据下游任务进行调整。

* **索引 Indexing**

在索引阶段，文档会被处理、分割并转化为嵌入向量 Embedding，然后存储到向量数据库中。索引构建的质量决定了在检索阶段能否获取到正确的上下文。

**分块**

最常见的方法是将文档按照固定数量的 Token 分割成块，例&#x5982;**`100`**、**`256`**、**`512`**&#x4E2A;Token。

> * 较大的块能够捕获更多上下文信息，但也会引入更多噪声，同时需要更长的处理时间和更高的成本
>
> * 较小的块虽然噪声较少，但可能无法充分传达必要的上下文信息

分块可能导致句子被截断，因此出现了递归分割和滑动窗口等优化方法，这些方法通过合并多个检索过程中的全局相关信息，实现了分层检索。然而，这些方法仍然难以在语义完整性和上下文长度之间取得平衡。因此，一些方法&#x50CF;**`Small2Big`**&#x88AB;提出，以小范围的句子作为检索单元，并提供前后相邻的大范围的句子作为上下文输入给 LLM。

**附加元数据**

分块可以附加元数据信息，例如`页码`、`文件名`、`作者`、`分类`和`时间戳`等。随后可以根据这些元数据对检索进行过滤，从而缩小检索范围。在检索过程中为文档时间戳分配不同的权重，可以实现时间感知型 **RAG**，确保知识的新鲜度并避免使用过时的信息。除了从原始文档中提取元数据外，还可以人工构建。例如，添加段落的摘要，或者引入假设性问题。这种方法也被称为反向 HyDE 。具体来说，利用 LLM 生成可以由文档回答的问题，然后在检索过程中计算原始问题与假设问题之间的相似性，从而缩小问题与答案之间的语义差距。

**结构化索引**

增强信息检索的一种有效方法是为文档建立分层结构。通过构建这种结构，RAG系统可以加速相关数据的检索和处理。

1. **分层索引结构**：文件以父子关系排列，分块 chunks 与它们相关联。每个节点存储数据摘要，有助于快速遍历数据，并帮助 RAG 系统确定需要提取哪些分块。这种方法还可以缓解因分块提取问题而导致的幻觉现象。

2. **知识图谱索引**：利用知识图谱构建文档的分层结构有助于保持一致性。这可以表示不同概念和实体之间的联系，显著减少了幻觉的可能性。另一个优势是将信息检索过程转化为 LLM 能够理解的指令，从而提高知识检索的准确性，并使 LLM 能够生成上下文连贯的响应，提升 RAG 系统的整体效率。

   > **例**：为了捕捉文档内容与结构之间的逻辑关系，**`KGP`**&#x63D0;出了一种使用知识图谱在多个文档之间构建索引的方法。该知识图谱由节点（表示文档中的段落或结构，如页面和表格）和边（表示段落之间的语义/词汇相似性或文档结构内的关系）组成，有效解决了多文档环境中的知识检索和推理问题。

* **用户问题 Query**

初级 RAG 的主要挑战之一是其直接依赖用户的原始查询作为检索基础。构建一个精确且清晰的问题很困难，不恰当的查询会导致检索效果不佳。有时问题本身复杂，语言表达也不够条理清晰。另一个难点在于语言的复杂性和歧义性。语言模型在处理专业术语或多义缩写时常常遇到困难。例如，它们可能无法判&#x65AD;**`LLM`**&#x662F;指`大语言模型`还是法律背景下的`法学硕士`。

**查询扩展 Query Expansion**

将单个查询扩展为多个查询可以丰富查询内容，提供更多上下文以弥补具体细节的缺失，从而确保生成答案的最佳相关性。

**多查询** **Multi-Query**

通过提示工程利用 LLM 扩展查询，这些查询可以并行执行。查询扩展并非随机，而是经过精心设计。

**子查询** **Sub-Query**

子问题规划的过程是生成必要的子问题，结合这些子问题可以为原始问题提供上下文并完整回答。这一过程在原则上类似于查询扩展。具体而言，复杂问题可以通过**从简单到复杂**的提示方法分解为一系列更简单的子问题。

**Chain-of-Verification, CoVe**

扩展后的查询通过大语言模型进行验证，以减少幻觉现象。经过验证的扩展查询通常具有更高的可靠性。

**查询转换 Query Transformation**

核心概念是基于转换后的查询检索片段，而非用户的原始查询。 &#x20;

**查询重写 Query Rewrite**

原始查询并不总是适合 LLM 检索，尤其是在现实场景中。因此，可以提示 LLM 对查询进行重写。除了使用 LLM 进行查询重写外，还可以使用专门的小型语言模型，例如 **RRR**（**R**ewrite-**R**etrieve-**R**ead）。在淘宝中实现的查询重写方法，称为 BEQUE，显著提高了长尾查询的召回效果，从而提升了 GMV。 &#x20;

**查询优化**

另一种查询转换方法是使用提示工程让 LLM 基于原始查询生成新查询以用于后续检索。HyDE 构建假设文档，即对原始查询的假定答案，其重点是从答案到答案的嵌入相似性，而非问题或查询的嵌入相似性。使用`回退提示法`，将原始查询抽象化以生成高层次的概念问题（或回退问题）。在RAG系统中，回退问题和原始查询都被用于检索，两者的结果都被用作语言模型生成答案的基础。 &#x20;

**查询路由 Query Routing**

根据不同的查询，将其路由到不同的 RAG 管道，适用于为多样化场景设计的多功能 RAG 系统。 &#x20;

**元数据路由/过滤 Metadata Router/Filter**

第一步是从查询中提取关键词或者实体，然后根据关键词和片段中的元数据进行过滤，以缩小搜索范围。&#x20;

**语义路由 Semantic Router**

另一种路由方法是利用查询的语义信息。具体方法可以看[详细资料](https://github.com/aurelio-labs/semantic-router)。

当然，也可以采用混合路由方法，结合语义和元数据的方法以增强查询路由效果。

* **嵌入 Embedding**

在 RAG 中，检索是通过计算问题和文档片段嵌入之间的相似性来实现的，例如余弦相似性，其中嵌入模型的语义表示能力起着关键作用。这主要包括稀疏编码器（如 BM25）和密集检索器（如基于 BERT 架构的预训练语言模型）。最近的一些工作引入了一些更加优秀的嵌入模型，例&#x5982;**`AngIE`**、**`Voyage`**、**`BGE`**&#x7B49;，这些模型得益于多任务指令微调。Hugging Face &#x7684;**`MTEB`**&#x6392;行榜评估了嵌入模型&#x5728;**`8`**&#x4E2A;任务上的表现，涵&#x76D6;**`58`**&#x4E2A;数据集。此外，**`C-MTEB`**&#x4E13;注于中文能力，覆&#x76D6;**`6`**&#x4E2A;任务&#x548C;**`35`**&#x4E2A;数据集。对于**使用哪种嵌入模型**这个问题，并没有放之四海而皆准的答案。然而，某些特定模型更适合特定的使用场景。

**混合检索 Mix/Hybrid Retrieval**

稀疏和密集嵌入方法捕获不同的相关性特征，并可以通过利用互补的相关性信息相互受益。例如，稀疏检索模型可以用于提供初始搜索结果以训练密集检索模型。此外，预训练语言模型（PLMs）可以用于学习词权重，从而增强稀疏检索。具体而言，研究表明，稀疏检索模型可以增强密集检索模型的零样本检索能力，并帮助密集检索器处理包含罕见实体的查询，从而提高鲁棒性。

**微调嵌入模型 Fine-tuning Embedding Model**

当上下文与预训练语料库显著偏离时，特别是在医疗、法律实践等高度专业化的领域，这些领域充满专有术语，针对自己的领域数据集微调嵌入模型变得至关重要，以缓解这种差异。 &#x20;

除了补充领域知识外，微调的另一个目的是对齐检索器和生成器。

> **例**：
>
> * 使用 LLM 的结果作为微调的监督信号，这种方法被称为 **LSR**（**L**M-**S**upervised **R**etriever）。
>
> * **`PROMPTAGATOR`**&#x5229;用 LLM 作为少样本查询生成器，创建特定任务的检索器，解决了监督微调中的挑战，尤其是在数据稀缺的领域。
>
> * **`LLM-Embedder`**&#x5229;用 LLM 为多个下游任务生成奖励信号。检索器通过两种监督信号进行微调：数据集的硬标签和来自LLM的软奖励。这种双信号方法促进了更有效的微调过程，使嵌入模型适应多样化的下游应用。
>
> * **`REPLUG`**&#x5229;用检索器和 LLM 计算检索文档的概率分布，然后通过计算KL散度进行监督训练。这种简单而有效的训练方法通过使用语言模型作为监督信号提升了检索模型的性能，无需特定的交叉注意力机制。此外，受 RLHF 启发，利用基于语言模型的反馈通过强化学习增强检索器。

* **适配器 Adapter**

微调模型可能会带来一些挑战，例如通过 API 集成功能或解决因本地计算资源有限而产生的限制。因此，一些方法选择引入外部适配器以辅助对齐。

> **例**：
>
> * **`UPRISE`**&#x8BAD;练了一个轻量级的提示检索器，能够从预构建的提示池中自动检索适合给定零样本任务输入的提示。
>
> * **`AAR`**（**A**ugmentation-**A**dapted **R**etriver）引入了一种通用适配器，旨在适应多个下游任务。
>
> * **`PRCA`**&#x6DFB;加了一个可插拔的奖励驱动上下文适配器，以提升特定任务的性能。
>
> * **`BGM`**&#x56FA;定了检索器和 LLM，并在两者之间训练了一个桥接&#x7684;**`Seq2Seq`**&#x6A21;型。该桥接模型旨在将检索到的信息转换为 LLM 可以有效处理的格式，使其不仅能重新排序，还能动态地为每个查询选择段落，如重复可能采用更高级的策略。
>
> * **`PKG`**&#x63D0;出了一种创新方法，通过指令微调将知识整合到白盒模型中。在该方法中，检索器模块被直接替换，以根据查询生成相关文档。这种方法有助于解决微调过程中遇到的困难，并提升模型性能。

### 6.2.2 生成模块

在检索完成后，直接将所有检索到的信息输入 LLM 以回答问题是不推荐的做法。以下将从两个角度介绍调整方法：调整检索内容和调整 LLM。

* **上下文管理**

冗余信息可能干扰 LLM 的最终生成结果，而过长的上下文还会导致 LLM 出现中间信息丢失的问题。与人类似，LLM 往往只关注长文本的开头和结尾，而忽略中间部分。因此，在 RAG 系统中，通常需要对检索到的内容进行进一步处理。

**重排序 Reranking**

重排序本质上是对文档片段重新排序，以突出最相关的结果，从而有效减少整体文档池的规模。这种方法在信息检索中具有双重作用，既作为增强器又作为过滤器，为更精确的语言模型处理提供优化后的输入。重排序可以通过基于规则的方法实现，依赖预定义指标，如`多样性`、`相关性`&#x548C;**`MRR`**，也可以通过基于模型的方法实现，例如 BERT 系列的编码器-解码器模&#x578B;**`SpanBERT`**、专门的重排序模&#x578B;**`Cohere rerank`**&#x6216;**`bge-reranker-large`**，以及通用的大语言模型`GPT`。

**上下文选择/压缩 Context Selection/Compression**

在 RAG 过程中，一个常见的误解是认为检索尽可能多的相关文档并将它们拼接成一个长的检索提示是有益的。然而，过多的上下文可能会引入更多噪声，削弱 LLM 对关键信息的感知能力。因此通常会对上下文进行选择，以减少不相关的上下文：

> **例**：
>
> **`LLMLingua`**&#x5229;用小型语言模型 SLM，&#x5982;**`GPT-2 Small`**&#x6216;**`LLaMA-7B`**，检测并移除不重要的 Token，将其转换为人类难以理解但 LLM 能够很好理解的形式。这种方法提供了一种直接且实用的提示压缩方式，在无需额外训练 LLM 的情况下平衡了语言完整性和压缩率。
>
> **`RECOMP`**&#x91C7;用了一种对比学习的方法训练信息压缩器。每个训练数据点包含一个正样本和五个负样本，编码器在整个过程中通过对比损失进行训练。 &#x20;

除了压缩上下文外，减少文档数量也有助于提高模型答案的准确性：

> * 一种方法是`过滤-重排序`范式，结合了 LLM 和 SLM 的优势。其中 SLM 充当过滤器，而 LLM 则负责重新排序。一些工作表明让 LLM 对 SLM 识别出的困难样本进行重新排序，可以显著提升各种**信息抽取 IE** 任务的效果。
>
> * 另一种简单而有效的方法是让 LLM 在生成最终答案之前评估检索到的内容。这使得 LLM 可以通过自我批评过滤掉相关性较差的文档。例如，&#x5728;**`Chatlaw`**&#x4E2D;，LLM 被提示对引用的法律条款进行自我建议，以评估其相关性。

* **LLM 微调**

基于场景和数据特征对 LLM 进行针对性微调可以获得更好的效果。这也是使用本地部署 LLM 的最大优势之一。当 LLM 在特定领域缺乏数据时，可以通过微调向其提供额外的知识。微调的另一个好处是可以调整模型的输入和输出。例如，可以让 LLM 适应特定的数据格式，并按照指示生成某种风格的响应。对于涉及结构化数据的检索任务，一些框架实施三阶段训练方案，&#x5982;**`SANTA`**，以有效封装结构和语义上的细微差别。其中第一阶段聚焦于检索器，利用对比学习来优化用户问题和文档嵌入。

通过强化学习将 LLM 的输出与人类或检索器的偏好对齐也是一种方法。例如，手动标注最终生成的答案，然后通过强化学习提供反馈。除了与人类偏好对齐外，还可以与经过微调的模型和检索器的偏好对齐。当无法访问强大的专有模型或更大参数量的开源模型时，一种简单而有效的方法是对更强大的模型进行蒸馏，&#x5982;**`GPT-4`**。LLM 的微调还可以与检索器的微调协调进行，以对齐偏好。&#x5982;**`RA-DIT`**，它通过KL散度对齐检索器和生成器之间的评分函数。

### 6.2.3 RAG 范式

根据检索器如何增强生成器，可以将 RAG 范式分为四类：

> * **基于 Query 的 RAG**
>
> * **基于潜在表示的 RAG**
>
> * **基于 Logit 的 RAG**
>
> * **推测型 RAG**

* **基于 Query 的 RAG**

受启发于提示增强的思想，基于 Query 的 RAG 将用户的问题与检索到的信息无缝结合，并直接输入到生成器的初始阶段。这种方法在 RAG 应用中非常普遍。在检索完成后，获取的内容会与用户的原始查询合并，形成一个复合输入，然后由生成器处理以生成响应。基于查询的 RAG 被广泛应用于多种任务：

![]()

> * **文本生成**：
>
>   * **`REALM`**&#x4F7F;用`双`**`BERT`**`框架`来简化知识检索和整合，将预训练模型与知识提取器结合起来。
>
>   * **`Self-RAG`**&#x5F15;入一个批评模块，用于判断是否需要进行检索。
>
>   * **`REPLUG`**&#x901A;过 API 调用 LLM，将语言模型视为黑箱，并有效地将相关外部文档整合到查询中。
>
>   * **`In-Context RALM`**&#x4F7F;&#x7528;**`BM25`**&#x8FDB;行文档检索，并训练一个预测性重排序器，对排名靠前的文档重新排序并整合。 &#x20;
>
> * **代码领域**：将文本或代码中的上下文信息整合到提示中，从而提高了下游任务的效果。 &#x20;
>
> * **知识库问答**：一些研究表明，结合检索和语言模型在知识库问答中具有显著效果。例如，**`Uni-Parser`**、**`RNG-KBQA`**&#x548C;**`ECBRF`**&#x901A;过将查询和检索到的信息合并到提示中，有效提升了问答系统的性能和准确性。 &#x20;
>
> * **AI for Science**：**`Chat-Orthopedist`**&#x5E2E;助用户进行共享决策，通过将检索到的数据整合到模型提示中，提高了大语言模型的有效性和信息精度。 &#x20;
>
> * **图像生成任务**：
>
>   * **`RetrieveGAN`**&#x901A;过将检索到的数据，如`选定的图像块及其边界框`，整合到生成器的输入阶段，提升了生成图像的相关性和精度。
>
>   * **`IC-GAN`**&#x901A;过将噪声向量与实例特征连接起来，调节生成图像的具体条件和细节。 &#x20;
>
> * **3D 生成**：**`RetDream`**&#x9996;先利用 CLIP 检索相关的 3D 资产，然后在输入阶段将检索到的内容与用户输入合并。 &#x20;

基于 Query 的 RAG 通常与大语言模型生成器结合使用，提供了模块化的灵活性，能够快速集成预训练组件以实现快速部署。在此框架下，提示设计对于利用检索到的数据至关重要。

* **基于潜在表示的 RAG**

在基于潜在表示的 RAG 框架中，检索到的对象以潜在表示的形式被整合到生成模型中。这增强了模型的理解能力，并提升了生成内容的质量。基于潜在表示的 RAG 具有多模态和任务适应性，能够融合检索器和生成器的隐藏状态，但需要额外的训练来对齐潜在空间。它使得开发能够无缝整合检索信息的复杂算法成为可能。



![]()

> * **文本领域**：**`FiD`**&#x548C;**`RETRO`**&#x662F;基于潜在表示的 RAG 的两种经典结构，许多后续工作都基于它们进行了改进：
>
>   * **FiD** 使用不同的编码器分别处理每个检索到的段落及其标题和查询，然后将生成的潜在表示合并，由单个解码器解码以生成最终输出。 &#x20;
>
>   * **RETRO&#x20;**&#x4E3A;每个分割的子查询检索相关信息，并引入了一种名为分块交叉注意力 **CCA**（**C**hunked **C**ross-**A**ttention）的新模块，用于将检索到的内容与每个子查询的标记进行整合。此外，在基于潜在表示的 RAG 范畴内还有其他值得注意的新结构。一些工作在 Transformer 块中集成了 k 近邻搜索 kNN，允许输入分块，理论上解决了 Transformer 模型长期以来受到批评的上下文长度限制问题。
>
> * **代码与科学领域**：**FiD** 在这些领域得到了广泛应用，涵盖了多个**与代码相关的任务**以及 **AI for Science**。
>
> * **图像领域**：一些工作使用交叉注意力机制，通过整合潜在表示来融合检索结果。除此之外还可以搭建文本-图像仿射组合模块 **ACM**（**A**ffine **C**ombination **M**odule），直接连接隐藏特征。
>
> * **知识领域**：一些工作采用了 **FiD** 及其衍生方法用于下游任务：
>
>   * **`EaE`**&#x901A;过实体特定参数化增强生成器的理解能力。 &#x20;
>
>   * **`TOME`**&#x5219;转向对提及的细微编码，优先考虑提及的粒度而非仅关注实体表示。
>
> * **3D 生成领域**：
>
>   * **`ReMoDiffuse`**&#x5F15;入了一种语义调制注意力机制，能够根据文本描述更准确地生成相应的 3D 动作。
>
>   * **`AMD`**&#x901A;过将原始扩散过程与参考扩散过程融合，实现了从文本到 3D 动作的高效转换。
>
> * **音频领域**：
>
>   * 一些工作使用 LLM，在注意力模块中结合密集特征编码以指导音频字幕的生成。
>
>   * **`Re-AudioLDM`**&#x4F7F;用不同的编码器从文本和音频中提取深度特征，然后将其整合到其潜在扩散模型**LDM**（**L**atent **D**iffusion **M**odel）的注意力机制中。
>
> * **视频字幕生成**：
>
>   * **`R-ConvED`**&#x4F7F;用卷积编码器-解码器网络，通过注意力机制处理检索到的视频-句子对，生成隐藏状态以生成字幕。
>
>   * **`CARE`**&#x5F15;入概念检测器，生成概念概率，并将概念表示整合到混合注意力机制中。
>
>   * **`EgoInstructor`**&#x4F7F;用门控交叉注意力合并文本和视频特征，提高了第一视角视频字幕的相关性和连贯性。

* **基于 Logit 的 RAG**

在基于 logit 的 RAG 中，生成模型在解码过程中通过 logit 整合检索信息。通常，logit 会通过简单的求和或模型计算逐步生成的概率。基于 logit 的 RAG 利用历史数据推断当前状态，并在 logit 层面融合信息，非常适合序列生成任务。它专注于生成器的训练，并允许开发利用概率分布的新方法，为未来任务提供支持。



![]()

> * **文本领域**：
>
>   * **`kNN-LM`**&#x53CA;其变体在每个解码步骤中将语言模型的概率与相似前缀的检索距离概率相结合。
>
>   * **`TRIME`**&#x548C;**`NPM`**&#x662F;传统 **kNN-LM** 方法的激进演进版本，使用本地数据库中高度对齐的标记作为输出，尤其在长尾分布场景中显著提升了性能。
>
> * **其他模态**：超出文本领域，代码和图像等其他模态也利用了基于 logit 的 RAG。
>
> * **代码领域**：
>
>   * 一些工作采用了 kNN 的概念以增强最终输出的控制，从而实现了更优的性能。
>
>   * **`EDITSUM`**&#x901A;过在 logit 层面整合原型摘要，提升了代码摘要的质量。
>
> * **图像字幕生成**：**`MA`**&#x76F4;接应用 kNN-LM 框架解决图像字幕问题，取得了良好的效果。

* **推测型 RAG**

推测型 RAG 用检索代替纯生成，从而节省资源并加速响应速度。 &#x20;

> * **`REST`**&#x5C06;推测解码中的小模型替换为检索，从而实现草稿的生成。 &#x20;
>
> * **`GPTCache`**&#x901A;过构建语义缓存来存储 LLM 的响应，解决了使用 LLM API 时高延迟的问题。 &#x20;
>
> * **`COG`**&#x5C06;文本生成过程分解为一系列复制粘贴操作，从文档中检索单词或短语，而非进行生成。 &#x20;

![]()

推测型 RAG 目前主要适用于序列数据。它将生成器与检索器解耦，使得可以直接使用预训练模型作为组件。在此范式下可以探索更广泛的策略，以有效利用检索到的内容。

## 6.3 RAG 系统优化

一般来说，提升系统性能主要&#x6709;**`5`**&#x4E2A;方面：`输入`、`检索器`、`生成器`、`结果`以及`整个流程`。

### 6.3.1 输入优化

输入最初被送入检索器，对检索阶段的最终结果有显著影响。一般有两种输入增强方法：**查询转换**和**数据增强**。

* **Query** **转换**

Query 转换通过修改输入查询，查询转换可以提升检索结果的质量。 &#x20;

> **例**：
>
> 1. **`Query2doc`**&#x548C;**`HyDE`**&#x4F7F;用原始查询生成一个伪文档，随后将其用作检索查询。伪文档包含更丰富的相关信息，有助于检索到更准确的结果。 &#x20;
>
> 2. **`TOC`**&#x5229;用检索到的内容将模糊查询分解为多个清晰的子查询，这些子查询被发送到生成器并聚合以生成最终结果。 &#x20;
>
> 3. **`RQ-RAG`**&#x5BF9;于复杂或模糊的查询，将其分解为清晰的子查询，进行细粒度检索，并综合响应以提供针对原始查询的连贯答案。 &#x20;

* **数据增强**

数据增强在检索之前改进数据，包括删除无关信息、消除歧义、更新过时文档、合成新数据等技术。 &#x20;

> **例**：
>
> 1. **`Make-An-Audio`**&#x4F7F;用字幕生成和音频-文本检索为无语言音频生成字幕，以缓解数据稀疏性问题，并添加随机概念音频以改进原始音频。 &#x20;
>
> 2. **`LESS`**&#x901A;过分析梯度信息优化下游任务的数据集选择，旨在增强模型对指令提示的响应性能。 &#x20;
>
> 3. **`ReACC`**&#x4F7F;用数据增强，包括重命名和插入无效代码等，对代码检索模型进行预训练。 &#x20;
>
> 4. **`Telco-RAG`**&#x901A;过应用`3GPP 规范词汇表`提高检索准确性，并使用路由模块将这些词汇与用户查询匹配。

### 6.3.2 检索模块优化

在 RAG 系统中，检索内容的质量决定了输入生成器的信息质量。较低的内容质量会增加模型产生幻觉或其他性能下降的风险。提高检索效率的有效方法主要有以下几种：

* **递归检索**

递归检索是通过多次搜索来获取更丰富、更高质量的内容。 &#x20;

> **例**：
>
> 1. **`ReACT`**&#x4F7F;用思维链 **CoT**（**C**hain-**o**f-**T**hought）将 Query 分解为多个部分，进行递归检索，从而提供更丰富的信息。 &#x20;
>
> 2. **`RATP`**&#x4F7F;用蒙特卡洛树搜索进行模拟，选择最佳的检索内容，并将其模板化后传递给生成器以生成输出。

* **片段优化**

片段优化是指调整片段大小以改善检索结果。 &#x20;

> **例**：
>
> 1. **`LlamaIndex`**&#x5305;含了一系列片段优化方法，其中之一基于`从小到大`的原则。其核心思想是定位更细粒度的内容，但返回更丰富的信息。例如，句子窗口检索 Sentence-Window Retrieval 提取小文本片段，并返回围绕该片段的相关句子窗口。&#x20;
>
> 2. **`RAPTOR`**&#x4E3A;了解决上下文信息不足的问题，使用递归嵌入、聚类和文本片段摘要，直到进一步聚类不可行为止，从而构建一个多层级的树状结构。 &#x20;
>
> 3. **`Prompt-RAG`**&#x901A;过预先生成目录来提高检索准确性，使模型能够根据查询自主选择相关章节。

* **检索器微调**

作为 RAG 系统核心的检索器，依赖于一个高效的嵌入模型来表示相关内容并为生成器提供输入，从而提升系统性能。具有强大表达能力的嵌入模型可以通过领域特定或任务相关的数据进行微调，以提升在目标领域的表现。

> **例**： &#x20;
>
> 1. **`REPLUG`**&#x5C06; LLM 视为黑箱，并根据最终结果更新检索器模型。 &#x20;
>
> 2. **`APICoder`**&#x4F7F;用 Python 文件、API 名称、签名和描述对检索器进行微调。 &#x20;
>
> 3. **`EDITSUM`**&#x5BF9;检索器进行微调，以减少检索后摘要之间的杰卡德距离。 &#x20;
>
> 4. **`SYNCHROMESH`**&#x5728;损失函数中加入抽象语法树 AST 的树距离，并使用目标相似性调优 **TST**（**T**arget **S**imilarity **T**uning）对检索器进行微调。 &#x20;
>
> 5. **`R-ConvED`**&#x4F7F;用与生成器相同的数据对检索器进行微调。 &#x20;

* **混合检索**

混合检索指的是同时采用多种不同的检索方法，或从多个不同来源提取信息。 &#x20;

> **例**：
>
> 1. **`RAP-Gen`**、**`BlendedRAG`**&#x548C;**`ReACC`**&#x540C;时使用密集检索器和稀疏检索器以提高检索质量。 &#x20;
>
> 2. **`Rencos`**&#x4F7F;用稀疏检索器在语法层面检索相似代码片段，并使用密集检索器在语义层面检索相似代码片段。 &#x20;
>
> 3. **`BASHEXPLAINER`**&#x9996;先使用密集检索器捕获语义信息，然后使用稀疏检索器获取词汇信息。 &#x20;
>
> 4. **`RetDream`**&#x9996;先通过文本进行检索，然后通过图像嵌入进行检索。 &#x20;
>
> 5. **`CRAG`**&#x5F15;入了一个检索评估器，用于衡量文档与查询的相关性，并根据置信度提示三种检索响应：如果准确则直接使用结果进行知识优化，如果不准确则进行网络搜索，对于模糊情况则采用混合方法。   &#x20;
>
> 6. **`UniMSRAG`**&#x5F15;入了一种新型 Token，称&#x4E3A;**`Acting Token`**，用于决定从哪个来源检索信息。 &#x20;

* **重排序**

重排序技术指的是对检索到的内容重新排序，以实现更高的多样性和更好的结果。 &#x20;

> **例**：
>
> 1. **`Re2G`**&#x5728;传统检索器之后应用了一个重排序模型，以减少因将文本压缩为向量而导致的信息损失影响。 &#x20;
>
> 2. **`AceCoder`**&#x4F7F;用选择器对检索到的程序进行重排序，减少冗余程序并获得多样化的检索结果。 &#x20;
>
> 3. **`XRICL`**&#x5728;检索后使用基于蒸馏的示例重排序器。 &#x20;
>
> 4. **`Rangan`**&#x4F7F;用量化影响度量 **QIM**（**Q**uantized **I**nfluence **M**easure），评估查询与参考之间的统计偏差，以评估数据子集的相似性并对检索结果进行重排序。 &#x20;
>
> 5. **`UDAPDR`**&#x4F7F;用 LLM 高效生成合成查询，用于训练领域特定的重排序器，并通过多教师知识蒸馏开发出一个连贯的检索器。 &#x20;
>
> 6. **`LLM-R`**&#x901A;过使用静态 LLM 进行文档排名和奖励模型训练来迭代优化其检索器，并辅以知识蒸馏。每次训练周期逐步改进检索器，从而实现渐进式优化。 &#x20;

* **检索转换**

检索转换涉及对检索到的内容进行改写，以更好地激活生成器的潜力，从而改善输出效果。 &#x20;

> **例**：
>
> 1. **`FILCO`**&#x9AD8;效地从检索到的文本中清除无关内容，仅保留相关支持内容，简化生成器的任务并促进准确答案预测。 &#x20;
>
> 2. **`FiD-Light`**&#x6700;初使用编码器将检索到的内容转换为向量，然后对其进行压缩，从而显著减少了延迟时间。 &#x20;
>
> 3. **`RRR`**&#x901A;过模板将当前查询与每轮中的前 k 个文档整合，随后通过预训练的 LLM 对其进行重构。

### 6.3.3 生成模块优化

在 RAG 系统中，生成器的质量通常决定了最终输出结果的质量。因此，生成器的能力决定了整个 RAG 系统效果的上限。

* **提示工程** **Prompt Engineering**

专注于提升大语言模型输出质量的提示工程技术，如提示压缩、Stepback Prompt、Active Prompt、思维链提示 Chain of Thought Prompt 等，都适用于 RAG 系统中的大语言模型生成器。&#x20;

> **例**：&#x20;
>
> 1. **`LLMLingua`**&#x4F7F;用一个小模型压缩查询的整体长度，以加速模型推理，减轻无关信息对模型的负面影响，并缓解中间丢失现象。 &#x20;
>
> 2. **`ReMoDiffuse`**&#x4F7F;用 ChatGPT 将复杂描述分解为解剖学文本脚本。 &#x20;
>
> 3. **`ASAP`**&#x5C06;包含输入代码、函数定义、分析结果和相应注释的示例元组整合到提示中，以获得更好的结果。 &#x20;
>
> 4. **`CEDAR`**&#x4F7F;用设计好的提示模板，将代码演示、查询和自然语言指令组织成提示。 &#x20;
>
> 5. **`XRICL`**&#x5229;用思维链技术，在跨语言语义解析和推理中增加翻译对作为中间步骤。 &#x20;
>
> 6. **`ACTIVERAG`**&#x4F7F;用认知纽带机制校准大语言模型的内在认知，并在答案生成中应用思维链提示。 &#x20;
>
> 7. **`Make-An-Audio`**&#x80FD;够使用其他模态作为输入，为后续过程提供更丰富的信息。

* **解码微调 Decoding Tuning**

解码微调涉及通过微调超参数来增强生成器控制，例如增加多样性、限制输出词汇表等调整。 &#x20;

> **例**：&#x20;
>
> 1. **`InferFix`** 通过调整解码器中的温度参数来平衡结果的多样性和质量。 &#x20;
>
> 2. **`SYNCHROMESH`** 通过实现一个补全引擎限制解码器的输出词汇表，从而消除实现错误。

* **生成器微调**

生成器的微调可以增强模型拥有更精确领域知识的能力，或更好地与检索器适配。 &#x20;

> **例**：&#x20;
>
> 1. **`RETRO`** 固定检索器的参数，并在生成器中使用分块交叉注意力机制，结合查询和检索器的内容。 &#x20;
>
> 2. **`APICoder`** 使用打乱后的新文件，并结合 API 信息和代码块对生成&#x5668;**`CODEGEN-MONO 350M`**&#x8FDB;行微调。 &#x20;
>
> 3. **`CARE`&#x20;**&#x4F7F;用图像、音频和视频-文本对训练编码器，然后微调解码器，同时减少字幕和概念检测损失，同时保持编码器和检索器固定。 &#x20;
>
> 4. **`Animate-AStory`** 使用图像数据优化视频生成器，然后微调一个 LoRA 适配器以捕捉给定角色的外观细节。 &#x20;
>
> 5. **`RetDream`** 使用渲染图像对一个 LoRA 适配器进行微调。

### 6.3.4 结果优化

在许多场景中，RAG 的结果可能无法达到预期效果，而一些结果增强技术可以帮助缓解这一问题。

* **输出重写**

输出重写指的是在某些场景下对生成器生成的内容进行改写，以满足下游任务的需求。

> **例**：
>
> **`SARGAM`**&#x5728;代码相关任务中通过使用一种特殊的 Transformer，并结合删除、占位符和插入分类器来优化输出，从而更好地与实际代码上下文对齐。
>
> **`Ring`**&#x901A;过对生成器生成的每个标记的对数概率平均值对候选结果进行重排序，从而获得多样化结果。
>
> **`CBRKBQA`**&#x901A;过将生成的关系与知识图谱中查询实体局部邻域中的关系对齐来修正结果。

### 6.3.5 RAG 链路优化

一般优化 RAG 链路有以下几种方法：

* **自适应检索**

一些关于 RAG 的研究表明，检索并不总是能提升最终结果。过度检索可能导致资源浪费，并在模型的固有参数化知识足以回答相关问题时引发潜在混淆。因此，这里介绍两种确定检索必要性的方法：**基于规则**和**基于模型**的方法。

**基于规则**

基于规则的方法通过预定义的逻辑或条件来决定是否进行检索，通常依赖于统计分析、超参数调整或明确的决策标准。这类方法的优势在于其透明性和可控性，适用于需要快速判断检索必要性的场景。例如，可以通过问题频率、输入不确定性或生成概率等规则，动态决定是否调用检索模块，从而避免不必要的资源消耗。

> **例**：
>
> 1. **`FLARE`**&#x4E3B;动根据生成过程中的概率决定是否以及何时进行搜索。
>
> 2. **`Efficient-KNNLM`**&#x5C06; **KNN-LM** 和 **NPM** 的生成概率与超参数$$\lambda$$结合，以确定生成和检索的比例。

**基于模型**

基于模型的方法利用训练好的模型或大语言模型的能力来智能判断是否需要检索。这种方法更加灵活，能够根据具体查询内容和上下文动态调整决策。例如，模型可以评估自身对问题的回答置信度，或者通过外部知识库验证答案的准确性，从而决定是否进行进一步检索。这种方法在复杂任务中表现出色，但可能需要更高的计算成本。

> **例**：
>
> 1. **`Self-RAG`**&#x4F7F;用训练好的生成器，根据不同用户查询下的检索标记决定是否执行检索。
>
> 2. **`SKR`**&#x5229;用大语言模型自身的能力预先判断是否能回答问题，如果可以回答，则不进行检索。
>
> 3. **`Rowen`**&#x5C06;问题翻译成多种语言，并检查这些语言中答案的一致性，利用结果判断是否需要进行信息检索。
>
> 4. **`AdaptiveRAG`**&#x4F7F;用一个较小的语言模型通过分类器动态决定是否根据查询复杂性进行检索。

* **迭代 RAG**

迭代 RAG 通过反复循环检索和生成阶段逐步优化结果，而不是仅进行一轮操作。

> **例**： &#x20;
>
> 1. **`RepoCoder`**&#x4F7F;用迭代检索-生成方法完成代码补全，通过先前生成的代码优化查询，以更好地利用分散的信息并改善结果。 &#x20;
>
> 2. **`ITER-RETGEN`**&#x901A;过使用生成器的输出识别知识空白，检索必要信息，并为未来的生成周期提供信息，从而迭代提升内容质量。 &#x20;
>
> 3. **`SelfMemory`**&#x8FED;代使用检索增强生成器形成一个庞大的记忆池，记忆选择器从中挑选输出以指导下一个生成周期。 &#x20;
>
> 4. **`RAT`**&#x9996;先通过零样本思维链提示由大语言模型生成内容，然后通过从外部知识库检索知识逐步修正每个思维步骤。

## 6.4 RAG 的训练

根据是否需要训练，现有的 RAG 方法可以分为两类：**无训练方法**和**基于训练的方法**。无训练方法通常在推理阶段直接利用检索到的知识，通过将检索到的文本插入 prompt 中来避免额外的训练，这种方法计算效率高。然而，一个潜在的挑战是检索器和生成器组件并未针对下游任务进行专门优化，这可能导致检索到的知识无法得到充分利用。为了充分挖掘外部知识，许多方法被提出用于微调检索器和生成器，从而引导 LLM 有效地适应并整合检索到的信息。

根据训练策略，将这些基于训练的方法分为三类：

> 1. **独立训练方法**分别独立训练 RAG 流程中的每个组件
>
> 2. **顺序训练方法**先训练一个模块，然后冻结已训练好的组件以指导其他部分的调优过程
>
> 3. **联合训练方法**同时训练检索器和生成器

接下来详细展开**无训练**、**独立训练**、**顺序训练**和**联合训练**方法。这些不同训练方法的比较如下图所示：

![]()

### 6.4.1 无训练方法

由于具有庞大的参数量，LLM 展现了接近人类水平的智能，并在各种下游任务中取得了令人满意的预测性能。然而，频繁进行微调和更新存储在模型参数中的知识极具挑战性，因为这需要大量的时间和计算资源。最近，许多公式发现，可以通过检索机制增强大语言模型的能力，使其能够从外部来源动态获取新知识，而无需额外的训练过程。这些方法不再仅仅依赖于模型参数中编码的隐性知识，而是显著提升了各种知识密集型任务的性能，例如开放域问答。根据大语言模型利用检索信息的不同方式，可以将这些无训练方法分为两类：

> 1. **基于提示工程的方法**：直接将检索到的知识整合到原始提示中
>
> 2. **基于检索的 Token 生成方法**：通过检索信息来校准标记生成过程

* **基于提示工程的方法**

由于 LLM 的生成性能高度依赖输入的 Query，许多无训练的 RAG 方法通过优化原始 prompt 来利用外部知识。具体而言，检索到的文本通常被用作上下文信息，并与原始提示结合以指导大语言模型的生成过程。

> **例**：
>
> 1. **`In-Context RALM`**&#x5728;保持大语言模型参数不变的情况下，直接将检索到的文档插入原始 prompt 之前，以增强生成过程。
>
> 2. **`IRCoT`**&#x5C06;思维链生成步骤与知识检索步骤交错进行，从而为后续推理步骤检索到比仅依赖问题作为查询的标准检索方法更相关的信息。
>
> 3. **`GENREAD`**&#x4E0E;从大型语料库中检索知识不同，它首先提示大语言模型基于查询生成上下文文档，然后根据给定的上下文和问题生成答案。
>
> 4. **`SKR`**&#x5F15;导大语言模型判断是否能够基于其内部知识回答给定问题，从而通过选择性调用检索器灵活利用内部和外部知识。
>
> 5. **`TOC`**&#x9996;先为模糊问题检索相关知识，并通过将模糊问题细化为多个明确的问题递归构建树状结构，最终聚合生成长篇答案。

* **基于检索的 Token 生成方法**

除了直接将外部知识整合到原始提示中外，辅助信息还可以用于调整标记生成过程。

> **例**：
>
> 1. **`KNN-KMs`**&#x9996;先根据给定查询从数据存储中检索出 𝑘 个最相关的上下文，并基于距离计算邻居分布。输出分布通过对邻居分布和原始模型的输出分布进行插值来校准。
>
> 2. **`Rest`**&#x7528;非参数化的检索数据存储取代参数化的草稿模型，并根据当前上下文检索相关标记以进行推测性解码。

### 6.4.2 独立训练

独立训练是指将检索器和 LLM 作为两个完全独立的过程进行训练，在训练过程中检索器和大语言模型之间没有交互。与无训练方法相比，通过训练大语言模型利用检索到的知识，或训练检索器弥合信息检索与语言生成之间的差距，可以有效提升 RAG 赋能模型的性能。对于大语言模型的训练，负对数似然损失是最具代表性的训练目标，旨在引导大语言模型基于给定输入生成期望的输出。关于检索器，它可以分为两类：

> 1. **稀疏检索器**：稀疏检索器通常利用稀疏特征（如词频）来表示文档，并基于特定任务的指标（如 TF-IDF 和 BM25）计算相关性分数。
>
> 2. **密集检索器**：深度神经网络被用于将 Query 和文档编码为密集表示，然后通常使用内积计算相关性分数并检索相关的外部知识。例如，**`DPR`**&#x4F7F;用两个独立的 BERT 网络分别编码查询和段落，并通过对比学习训练这些模型。**`CoG`**&#x8BAD;练前缀编码器和短语编码器用于检索，并将文本生成重新表述为从现有源文本集合中进行多次复制粘贴操作。

### 6.4.3 顺序训练

独立训练是一种在生成过程中利用外部知识的高效方法，因为检索器和生成器可以离线训练，并且可以直接使用现成的模型，避免了额外的训练成本。为了更好地增强检索器和生成器之间的协同作用，一些方法对检索器和大语言模型进行顺序训练。在这些顺序训练方法中，过程通常从检索器或生成器的独立预训练开始，随后固定预训练模块，同时对另一个模块进行训练。需要注意的是，各种现有的模型，&#x5982;**`BERT`**、**`CLIP`**、**`T5`**&#x7B49;，可以直接用作固定的检索器和生成器，从而跳过最初的预训练过程。与独立训练相比，顺序训练涉及检索器和生成器的协同训练，其中可训练模块受益于固定模块的辅助。根据检索器和生成器的训练顺序，顺序训练可以分为**先训练检索器**还是**先训练 LLM**。

* **先训练检索器**

首先训练检索模型，然后将其固定，接着利用检索到的知识对 LLM 进行训练。

> **例**：
>
> **`RETRO`**&#x4F7F;用独立预训练的 BERT 模型作为检索器，并训练一个编码器-解码器架构以将检索片段整合到模型的预测中。
>
> **`RALMs`**&#x4F7F;用 Google 搜索和开源的 COLBERTV2 作为预训练检索器，并微调大语言模型以有效利用检索到的段落。
>
> **`ITERRTGEN`**&#x4F7F;用预训练的 S-BERT 作为检索器，并引入了一种自适应混合检索策略以检索示例。此外，它利用 T5 作为生成器，基于目标标签和结合原始提示与检索示例的输入对其进行进一步微调。
>
> **`SMALLCAP`**&#x4F7F;用 CLIP 对输入图像和外部数据存储中的文本数据进行编码，并基于余弦相似度检索最相关的内容。通过训练一个交叉注意力层并使用 GPT-2 作为解码器来生成字幕。

* **先训练 LLM**

类似地，也可以先预训练大语言模型，然后在经过良好训练的大语言模型的监督下对检索器进行微调。

> **例**：
>
> **`DKRR`**&#x53D1;现序列到序列模型的注意力分数可以指示文档的相关性。因此，他们提议利用阅读器模型的注意力分数生成合成标签以训练检索器。
>
> **`AAR`**&#x4F7F;用一个小语言模型生成用于训练检索器的监督信号。经过良好训练的检索器可以进一步用于提升黑箱大语言模型的性能。
>
> **`RA-DIT`**&#x9996;先微调大语言模型以增强其利用检索知识的能力，然后训练检索器以使其输出更好地与大语言模型对齐。
>
> **`UPRISE`**&#x63D0;出了一种轻量级方法，通过引入提示检索器来增强大语言模型在未见任务中的零样本性能。一个冻结的大语言模型被用来指导提示检索器的微调过程，随后该检索器在推理过程中为不同任务检索适用于各种大语言模型的提示。

### 6.4.4 联合训练

联合训练方法采用端到端的范式，同时优化检索器和生成器。与顺序训练每个模块不同，联合训练方法有效增强了检索器定位外部知识以用于生成的能力，以及生成器有效利用检索信息的能力。

> **例**：
>
> **`REALM`**&#x4F7F;用**最大内积搜索 MIPS** 技术来定位最相关的文档。为了应用 MIPS，所有外部文档首先被嵌入表示，并为每个嵌入生成一个搜索索引。提出了一种异步索引更新策略，每隔数百个训练步骤刷新一次索引，以避免重新索引所有文档的时间消耗。

## 6.5 RAG 的变体

> 论文：[**Self-RAG**](https://arxiv.org/pdf/2310.11511)  **[GraphRAG](https://arxiv.org/pdf/2404.16130)  [HybridRAG](https://arxiv.org/pdf/2408.04948)  [LightRAG](https://arxiv.org/pdf/2410.05779)**

### 6.5.1 Self-RAG

### 6.5.2 GraphRAG

### 6.5.3 HybridRAG

### 6.5.4 LightRAG

# 7. 大模型智能体 Agent

## 7.1 概述

智能体长期以来被认为是一种实现人工通用智能 **AGI** 的有前景的方法，**AGI** 预期能够通过自我导向的规划和行动来完成任务。在以往的研究中，这些智能体通常被假设基于简单且启发式的策略函数进行行动，并在孤立和受限的环境中学习。这种假设与人类的学习过程存在显著差异，因为人类的思维高度复杂，并且个体能够从更广泛的环境中学习。由于这些差距，以往研究中获得的智能体通常远未达到复制人类水平的决策过程，尤其是在无约束、开放领域的场景中。

近年来， LLM 取得了显著的成功，展现了实现类人智能的巨大潜力。这种能力源于利用全面的训练数据集以及大量的模型参数。基于这一能力，AI Agent 逐渐兴起，它将 LLM 用作核心控制器，以构建具有类人决策能力的自主智能体。与强化学习相比，基于 LLM 的智能体拥有更全面的内部世界知识，这使它们即使在没有特定领域数据训练的情况下也能做出明智的行动。此外，基于 LLM 的智能体可以提供自然语言接口用于人机交互，从而具备更高的灵活性和更强的可解释性。

沿着这一方向，相关研究者开发了许多有前景的模型，其核心思想是为 LLM 赋予诸如记忆和规划等人类能力，使其表现得像人类并有效完成各种任务。

![]()



## 7.2 Agent 核心组件

基于 LLM 的智能体概念框架包含三个组成部分：大脑、感知和行动。作为控制器，大脑模块负责执行记忆、思考和决策等基本任务。感知模块负责感知和处理来自外部环境的多模态信息，而行动模块则通过工具执行操作并对周围环境产生影响。下图是一个示例来说明工作流程：当一个人询问是否会下雨时，感知模块将指令转化为大语言模型可以理解的表示形式。然后，大脑模块根据当前天气状况以及互联网上的天气预报开始推理。最后，行动模块作出响应，并将雨伞递给人类。通过重复上述过程，智能体能够不断获取反馈并与环境进行交互。

![]()

如果一个个体想要在外部环境中生存，就必须高效地适应周围环境。这要求他具备认知能力，能够感知并对外部世界的变化作出反应，这与智能体定义是一致的。因此基于 LLM 的通用框架通常由三个关键部分组成：**大脑**、**感知**和**行动**。该框架可以根据不同的应用场景进行调整，即并非所有研究都会用到每一个具体组件。总体而言，智能体按照以下工作流程运行：

> 1. **感知模块**（对应于人类的感觉系统，如眼睛和耳朵）感知外部环境的变化，并将多模态信息转化为智能体可以理解的表示形式。
>
> 2. **大脑模块**作为控制中心，进行思考、决策以及与存储相关的操作（包括记忆和知识）。
>
> 3. **行动模块**（对应于人类的四肢）借助工具执行任务并对周围环境产生影响。

通过重复上述过程，智能体能够不断获取反馈并与环境进行交互。

### 7.2.1 感知模块

人类和动物都依赖感官器官，如眼睛和耳朵等，从周围环境中获取信息。这些感知输入被转化为神经信号并传递到大脑进行处理，使我们能够感知并与世界互动。类似地，对于基于 LLM 的智能体来说，从多种来源和模态中接收信息至关重要。扩展的感知空间有助于智能体更好地理解环境、做出明智决策，并在更广泛的任务中表现出色，因此这是一个重要的发展方向。智能体通过感知模块将这些信息传递给大脑模块进行处理。

* **文本输入**

文本是承载数据、信息和知识的一种方式，文本交流已经成为人类与世界互动的重要途径之一。目前基于 LLM 的智能体已经具备通过文本输入和输出与人类交流的基本能力。在用户的文本输入中，除了显式的内容外，还隐藏着信念、欲望和意图等隐含信息。理解这些隐含意义对于智能体把握用户潜在和深层次的意图至关重要，从而提升其与用户沟通的效率和质量。

然而，当前基于 LLM 的智能体在理解文本输入中的隐含意义方面仍然面临挑战。例如，一些研究采用强化学习来感知隐含意义，并通过模型反馈推导奖励值。这有助于推断说话者的偏好，从而使智能体能够提供更加个性化和准确的响应。此外，由于智能体被设计用于复杂的现实场景中，它不可避免地会遇到许多全新的任务。对未知任务的文本指令的理解对智能体的文本感知能力提出了更高的要求。但是经过指令微调的 LLM 能够展现出卓越的 zero-shot 指令理解和泛化能力，从而无需针对特定任务进行微调。

* **视觉输入**

尽管 LLM 在语言理解和多轮对话方面表现优异，但它们天生缺乏视觉感知能力，只能理解离散的文本内容。视觉输入通常包含有关世界的丰富信息，包括物体属性、空间关系、场景布局以及智能体周围环境中的更多细节。因此，将视觉信息与其他模态的数据相结合，可以为智能体提供更广泛的上下文和更精确的理解，从而加深智能体对环境的感知。

为了帮助智能体理解图像中包含的信息，一种直接的方法是为图像输入生成相应的文本描述，这种方法被称为图像字幕生成。生成的字幕可以直接与标准文本指令关联并输入智能体。这种方法具有高度的可解释性，并且无需额外训练即可生成字幕，从而节省大量计算资源。然而，字幕生成是一种低带宽方法，在转换过程中可能会丢失大量潜在信息。此外，智能体对图像的关注可能引入偏见。

受 **Transformer** 在自然语言处理中的出色表现启发，一些科研人员将其应用扩展到了计算机视觉领域。代表性的工作&#x5982;**`ViT`**/**`VQVAE`**&#x6210;功使用 Transformer 对视觉信息进行编码。如右图所示，它首先将图像划分为固定大小的块，然后对这些块进行线性投影，作为 Transformer 的输入 Token。最终，通过计算 Token 之间的自注意力机制，他们能够整合整个图像的信息，从而实现一种高效的视觉内容感知方法。

![]()

因此，一些工作尝试直接结合图像编码器和 LLM，以端到端的方式训练整个模型。虽然智能体可以实现卓越的视觉感知能力，但这需要消耗大量的计算资源。

预训练的视觉编码器和 LLM 可以显著增强智能体的视觉感知和语言表达能力。在训练过程中冻结其中一个或两者是一种广泛采用的范式，能够在训练资源和模型性能之间取得平衡。然而，LLM 无法直接理解视觉编码器的输出，因此需要将图像编码转换为 LLM 能够理解的嵌入表示。也就是说需要将视觉编码器与 LLM 对齐。通常需要在两者之间添加一个额外的可学习接口层。

例如，**`BLIP-2`**&#x548C;**`InstructBLIP`**&#x4F7F;&#x7528;**`Q-Former`**&#x6A21;块作为视觉编码器和 LLM 之间的中间层。如上图，Q-Former 是一种 Transformer，使用可学习的 Query 向量，能够提取富含语言信息的视觉表示。

![]()

它可以为 LLM 提供最有价值的信息，减轻智能体学习视觉-语言对齐的负担，从而缓解灾难性遗忘的问题。

除此之外，一些工作，&#x5982;**`LLaVA`**&#x91C7;用了一种计算效率较高的方法，通过使用**单个投影层**实现视觉-文本对齐，减少了对额外参数训练的需求。此外，投影层可以有效地与可学习接口集成，调整其输出维度以使其与 LLM 兼容。

![]()

视频输入由一系列连续的图像帧组成。因此，智能体用于感知图像的方法也可能适用于视频领域，从而使智能体能够良好地感知视频输入。与图像信息相比，视频信息增加了时间维度。因此，智能体对不同帧之间时间关系的理解对于感知视频信息至关重要。一些工作&#x5982;**`Flamingo`**&#x5728;理解视频时通过掩码机制确保时间顺序。掩码机制限制智能体仅能访问早于当前帧的时间点的视觉信息，从而在感知特定帧时保持时间一致性。

* **听觉输入**

听觉信息也是世界信息的重要组成部分。当智能体具备听觉能力时，它可以增强对交互内容、周围环境甚至潜在危险的感知能力。目前已有许多成熟的模型和方法用于处理音频作为独立的模态。然而，这些模型通常在特定任务上表现出色。鉴于 LLM 出色的工具使用能力，一个非常直观的想法就是，智能体可以将 LLM 用作控制中心，以级联方式调用现有的工具集或模型库来感知音频信息。例如，**`AudioGPT`**&#x5145;分利用&#x4E86;**`FastSpeech`**、**`GenerSpeech`**、**`Whisper`**&#x4EE5;及其他模型的能力，这些模型在文本转语音、风格迁移和语音识别等任务中取得了优异成果。

音频频谱图提供了音频信号频率随时间变化的直观表示。对于一段时间内的音频数据段，可以将其抽象为有限长度的音频频谱图。音频频谱图具有二维表示形式，可以可视化为平面图像。因此，一些研究尝试将视觉领域的感知方法迁移到音频领域。**AST**（**A**udio **S**pectrogram **T**ransformer）采用与 **ViT** 类似的 **Transformer** 架构来处理音频频谱图。通过将音频频谱图分割为块，它实现了对音频信息的有效编码。除此之外，还有一些工作从冻结编码器的思想中获得启发，以减少训练时间和计算成本。他们通过添加相同的可学习接口层，将音频编码与其他模态的数据编码对齐。

* **其他输入**

目前许多研究已经探讨了针对文本、视觉和音频的感知单元。然而，基于 LLM 的智能体可能会配备更丰富的感知模块。未来，它们可能像人类一样感知和理解现实世界中的多样化模态。例如，智能体可能拥有独特的触觉和嗅觉器官，使其在与物体交互时能够收集更详细的信息。同时，智能体还可以清晰感知周围环境的温度、湿度和亮度，从而采取基于环境意识的行动。

此外，通过高效整合视觉、文本和光敏性等基本感知能力，智能体可以开发出各种面向人类的用户友好感知模块。**`InternGPT`**&#x5F15;入了指向指令。用户可以通过手势或移动光标选择、拖动或绘制，与图像中难以描述的具体部分进行交互。指向指令的加入有助于为单个文本指令提供更精确的说明。在此基础上，智能体有潜力感知更复杂的用户输入。例如，增强现实 **AR** / 虚拟现实 **VR** 设备中的眼动追踪技术、身体动作捕捉，甚至是脑机交互中的脑电波信号。

一个类人的基于 LLM 的智能体应具备对更广泛整体环境的感知能力。目前许多成熟的硬件设备可以帮助智能体实现这一目标：

> **激光雷达**可以创建 3D 点云地图，帮助智能体检测和识别周围的物体
>
> **全球定位系统 GPS** 可以提供精确的地理位置坐标，并与地图数据集成
>
> **惯性测量单元 IMU** 可以测量和记录物体的三维运动，提供关于物体速度和方向的详细信息

然而，这些传感器数据复杂，无法被基于 LLM 的智能体直接理解。探索智能体如何感知更全面的输入是研究 Agent 的一个重要方向。

### 7.2.2 中枢模块

人类大脑是一个复杂的结构，由大量相互连接的神经元组成，能够处理各种信息、产生多样化的思想、控制不同的行为，甚至创造艺术和文化。与人类类似，大脑是人工智能智能体的核心部分，主要由一个大型语言模型构成。

为了确保有效的沟通，进行**自然语言交互的能力**至关重要。在接收到感知模块处理过的信息后，大脑模块首先会转向存储功能，从**知识库中检索信息**，并从**记忆**中回忆相关信息。这些结果有助于智能体**制定计划、进行推理并作出明智的决策**。此外，大脑模块还可以以摘要、向量或其他数据结构的形式记录智能体过去的观察、思考和行动。同时，它还可以更新常识和领域知识，以备将来使用。基于大语言模型的智能体还可以通过其固有的**泛化能力和迁移能力**适应陌生场景。

* **自然语言交互**

语言作为交流的媒介，包含着丰富的信息。除了直观表达的内容外，背后可能还隐藏着说话者的信念、欲望和意图。得益于 LLM 本身强大的自然语言理解和生成能力，智能体不仅能够熟练地进行多语言的基本互动对话，还能表现出深度的理解能力，使人类可以轻松理解和与智能体交互。此外，以自然语言进行交流的基于 LLM 的智能体能够赢得更多的信任，并更有效地与人类合作。

1. **多轮互动对话**

多轮对话能力是实现高效且一致沟通的基础。作为大脑模块的核心，像 GPT 系列、LLaMA 系列和 T5 这样的大语言模型能够理解自然语言并生成连贯且上下文相关的回应，这有助于智能体更好地理解和处理各种问题。然而，即使是人类也很难在一次对话中毫无混淆地交流，因此需要多轮对话。与传统的仅限文本的阅读理解任务相比，&#x5982;**`SQuAD`**，多轮对话具有以下特点：

> * 互动性强，涉及多个发言者，缺乏连续性
>
> * 可能涉及多个主题，对话信息也可能冗余，使得文本结构更加复杂

总体而言，多轮对话主要分为三个步骤：

> 1. **理解自然语言对话的历史记录**
>
> 2. **决定采取什么行动**
>
> 3. **生成自然语言回应**

基于 LLM 的智能体能够利用现有信息不断优化输出，进行多轮对话并有效实现最终目标。

* **高质量的自然语言生成**

近期的大语言模型展现了卓越的自然语言生成能力，能够在多种语言中持续生成高质量的文本。&#x4ECE;**`GPT-3`**&#x5230;**`InstructGPT`**&#x518D;&#x5230;**`GPT-4`**，LLM 生成内容的连贯性和语法准确性得到了稳步提升。研究表明，这些语言模型能够适应条件文本的风格和内容。同时，**`ChatGPT`**&#x5728;语法错误检测方面的表现突出，彰显了其强大的语言能力。在对话场景中，大语言模型在对话质量的关键指标上也表现出色，包括**内容**、**相关性**和**适当性**。重要的是，它们不仅仅是复制训练数据，而是展现了一定程度的创造力，生成的文本与人类制作的基准一样新颖，甚至更加新颖。与此同时，通过可控 prompt 的使用，人类监督仍然有效，确保对这些语言模型生成的内容进行精确控制。

* **意图与隐含意义的理解**

尽管在大规模语料库上训练的模型已经足够智能以理解指令，但大多数模型仍然无法模拟人类对话或充分利用语言传递的信息。理解隐含意义对于与其他智能体进行有效的沟通和协作至关重要，并使人们能够解读他人的反馈。大语言模型的出现突显了基础模型理解人类意图的潜力，但在面对模糊指令或其他隐含意义时，这对智能体来说仍然是一个重大挑战。对于人类来说，从对话中捕捉隐含意义是自然而然的，而对于智能体来说，则需要将隐含意义形式化为奖励函数，使其能够在未见过的上下文中选择符合说话者偏好的选项。奖励建模的主要方法之一是基于反馈推断奖励，反馈主要以比较的形式呈现和无限制的自然语言描述。另一种方法则是通过动作空间作为桥梁，从描述中恢复奖励。研究表明，人类行为可以映射为从一组隐式选项中做出的选择，这有助于用单一统一的形式化方法解释所有信息。通过利用对上下文的理解，智能体可以采取高度个性化且精准的行动，满足特定需求。

* **知识**

由于现实世界的多样性，许多自然语言处理研究人员尝试利用更大规模的数据。这类数据通常是非结构化且未标注的，但其中包含语言模型可以学习的海量知识。理论上，语言模型的参数越多，能够学习的知识就越多，甚至有可能学习和理解自然语言中的所有内容。研究表明，在大规模数据集上训练的语言模型可以将广泛的知识编码到其参数中，并对各种类型的查询作出正确回应。此外，这些知识可以帮助基于大语言模型的智能体做出明智的决策。所有这些知识大致可以分为以下几类：

> * **语言学知识**：语言学知识表现为一种约束系统或语法规则，定义了一种语言中所有可能的句子。它包括词法、句法、语义和语用学。只有掌握语言学知识的智能体才能理解句子并参与多轮对话。此外，这些智能体可以通过在包含多种语言的数据集上进行训练来获得多语言知识，从而无需额外的翻译模型。
>
> * **常识知识**：常识知识是指通常在早期教育中传授给大多数人的普遍世界事实。例如，人们普遍知道药物用于治病，雨伞用于防雨。这类信息通常不会在上下文中明确提及。因此，缺乏相应常识知识的模型可能无法理解或误解预期含义。同样，缺乏常识知识的智能体可能会做出错误的决策，例如在大雨时不带伞。
>
> * **专业领域知识**：专业领域知识是指与特定领域相关的知识，如编程、数学、医学等。对于模型来说，掌握这些知识是有效解决特定领域问题的关键。例如，设计用于执行编程任务的模型需要具备编程知识，如代码格式；而用于诊断目的的模型则需要具备医学知识，如特定疾病名称和处方药。

尽管大语言模型在获取、存储和利用知识方面表现出色，但仍存在潜在问题和未解决的难题。例如，模型在训练过程中学到的知识可能会过时，甚至从一开始就可能是错误的。一个简单的解决方案是重新训练，但这需要先进的数据、大量时间和计算资源。更糟糕的是，这可能导致灾难性遗忘。因此，一些研究人员尝试编辑大语言模型，以定位和修改模型中存储的特定知识。这种方法涉及卸载错误知识的同时获取新知识。实验表明，这种方法可以部分编辑事实知识，但其底层机制仍需进一步研究。此外，大语言模型可能会生成与源信息或事实信息相冲突的内容，这种现象通常被称为幻觉。这是大语言模型无法广泛应用于事实严谨任务的关键原因之一。为了解决这一问题，一些研究人员提出了一种衡量幻觉程度的指标，为开发者提供了一个评估大语言模型输出可信度的有效参考。此外，还有一些研究人员使大语言模型能够利用外部工具来避免错误知识。这两种方法都可以缓解幻觉的影响，但仍需进一步探索更有效的解决方案。

* **记忆**

**记忆**存储了智能体过去的观察、思考和行动序列，这与Nuxoll等人提出的定义相似。正如人类大脑依赖记忆系统回顾性地利用先前的经验来制定策略和做出决策，智能体也需要特定的记忆机制以确保它们能够熟练处理一系列连续任务。当面对复杂问题时，记忆机制帮助智能体有效回顾并应用先前的策略。此外，这些记忆机制使个体能够通过借鉴过去的经验来适应陌生环境。随着基于 LLM 的智能体交互周期的扩展，两个主要挑战浮现出来：

> 1. 第一个挑战涉及**历史记录的长度问题**。基于 LLM 的智能体以自然语言格式处理先前的交互，并将历史记录附加到每个后续输入中。随着这些记录的增长，它们可能会超出大多数基于 LLM 的智能体所依赖的 Transformer 架构的限制。当这种情况发生时，系统可能会截断部分内容。
>
> 2. 第二个挑战是**提取相关记忆的困难**。随着智能体积累了大量历史观察和行动序列，它们面临着日益沉重的记忆负担。这使得建立相关主题之间的联系变得更加困难，可能导致智能体的回应与当前上下文不一致。

1. **增强记忆能力的方法**

在此，介绍几种提升基于 LLM 智能体记忆能力的方法：

> * **提高 Transformer 的长度限制**。第一种方法试图解决或缓解 Transformer 架构固有的序列长度限制。由于内在限制，Transformer 在处理长序列时表现不佳。随着序列长度的增加，计算需求因自注意力机制中的成对 token 计算而呈指数级增长。缓解这些长度限制的策略包括文本截断、分割输入以及强调文本的关键部分。一些其他研究则通过修改注意力机制来降低复杂性，从而适应更长的序列。
>
> * **总结记忆内容**。第二种增强记忆效率的策略基于记忆总结的概念。这确保智能体能够轻松提取历史交互中的关键细节。已有多种技术被提出用于总结记忆。一些方法使用提示词简洁地整合记忆，另一些则强调反思过程以创建浓缩的记忆表示。分层方法将对话简化为每日快照和总体摘要。值得注意的是，某些策略将环境反馈转化为文本封装形式，增强智能体对未来交互的上下文理解。此外，在多智能体环境中，重要通信元素被捕获并保留。
>
> * **用向量或数据结构压缩记忆**。通过使用适当的数据结构，智能体提升了记忆检索效率，从而快速响应交互。值得注意的是，一些方法依赖嵌入向量来表示记忆片段、计划或对话历史。另一种方法将句子转化为三元组配置，还有一些方法将记忆视为独特的数据对象，促进多样化的交互。此外，**`ChatDB`**&#x548C;**`DB-GPT`**&#x901A;过将 LLM 与 SQL 数据库集成，允许通过 SQL 命令进行数据操作。

* **记忆检索方法**

当智能体与其环境或用户交互时，从记忆中检索最相关内容至关重要。这确保智能体能够获取相关且准确的信息以执行特定行动。一个重要的问题随之而来：智能体如何选择最适合的记忆？通常，智能体会以自动化方式检索记忆。一种重要的自动化检索方法考虑三个指标：**时效性**、**相关性**和**重要性**。记忆得分被确定为这三个指标的加权组合，得分最高的记忆在模型上下文中优先使用。一些研究引入了**交互式记忆对象**的概念，这些对象是对对话历史的表示，可以通过总结进行移动、编辑、删除或合并。用户可以查看和操作这些对象，从而影响智能体对对话的理解。类似地，其他研究允许根据用户提供的特定命令进行记忆操作，如删除。这些方法确保记忆内容与用户期望紧密对齐。

* **推理与规划**

**推理**

推理以证据和逻辑为基础，是人类智力活动的核心，同时也是解决问题、决策制定和批判性分析的基石。演绎、归纳和溯因推理是智力活动中公认的三种主要推理形式。对于基于 LLM 的智能体而言，与人类一样，推理能力对于解决复杂任务至关重要。

关于大语言模型的推理能力，学术界存在不同的观点。一些人认为语言模型在预训练或微调阶段就具备推理能力，而另一些人则认为这种能力是在模型达到一定规模后才显现出来的。具体来说，具有代表性的 CoT 方法已被证明能够通过引导大语言模型在输出答案之前生成推理过程，从而激发其推理能力。此外，还有其他策略被提出以增强大语言模型的表现，例如自一致性、自我优化、自我改进和选择-推理等。一些研究表明，逐步推理的有效性可以归因于训练数据的局部统计结构，变量之间的局部依赖关系比对所有变量进行训练时具有更高的数据效率。

**规划**

规划是人类面对复杂挑战时采用的关键策略。对于人类而言，规划有助于组织思想、设定目标并确定实现目标的步骤。与人类类似，规划能力对智能体也至关重要，而推理能力则是规划模块的核心。这为基于大语言模型的智能体提供了一个结构化的思考过程。通过推理，智能体将复杂任务分解为更易管理的子任务，并为每个子任务制定适当的计划。此外，随着任务的推进，智能体可以通过内省调整其计划，确保更好地适应现实情况，从而实现灵活且成功的任务执行。通常，规划分为两个阶段：计划制定和计划反思。

> * **计划制定**：在计划制定过程中，智能体通常会将一个总体任务分解为多个子任务，并在此阶段提出了多种方法。值得注意的是，一些研究提倡基于大语言模型的智能体一次性全面分解问题，制定完整计划后按顺序执行。相比之下，像思维链系列研究则采用了一种自适应策略，逐一规划并处理子任务，从而在整体上更灵活地应对复杂任务。此外，一些方法强调分层规划，而另一些则主张从树状结构的推理步骤中推导出最终计划。后者认为智能体应在最终确定计划之前评估所有可能的路径。虽然基于大语言模型的智能体展现出广泛的一般知识，但在面对需要专业知识的任务时，它们偶尔会遇到挑战。通过将这些智能体与特定领域的规划器集成，已显示出更好的性能。
>
> * **计划反思**：在制定计划后，必须对其进行反思和评估优劣。基于大语言模型的智能体利用内部反馈机制，通常从现有模型中汲取见解，以优化和完善其策略与规划方法。为了更好地与人类价值观和偏好保持一致，智能体会积极与人类互动，允许他们纠正一些误解并将定制化反馈融入其规划方法中。此外，智能体还可以从实际或虚拟环境中获取反馈，例如任务完成的线索或行动后的观察结果，从而帮助它们修订和完善计划。

* **泛化**

智能不应局限于特定领域或任务，而应涵盖广泛的认知技能和能力。人类大脑的卓越特性在很大程度上归因于其高度的可塑性和适应性。它能够根据外部刺激和内部需求不断调整其结构和功能，从而适应不同的环境和任务。近年来，大量研究表明，在大规模语料库上预训练的模型可以学习通用的语言表示。利用预训练模型的能力，仅需少量数据进行微调，LLM 就能在下游任务中表现出色。无需从头训练新模型，这节省了大量计算资源。然而，通过这种针对特定任务的微调，模型缺乏通用性，并难以泛化到其他任务。基于大语言模型的智能体不仅是一个静态的知识库，还展现出动态学习能力，使它们能够快速且稳健地适应新任务。

1. **未见任务泛化**

研究表明，经过指令调优的大语言模型在无需任务特定微调的情况下表现出零样本泛化能力。随着模型规模和语料库规模的扩大，大语言模型在陌生任务中逐渐展现出显著的新兴能力。具体来说，大语言模型可以通过遵循指令并基于自身理解完成在训练阶段未遇到的新任务。其中一种实现方式是多任务学习，例&#x5982;**`FLAN`**&#x901A;过指令描述的一系列任务对语言模型进行微调，**`T0`**&#x5F15;入了一个统一框架，将所有语言问题转换为文本到文本的格式。尽管 GPT-4 纯粹是一个语言模型，但它在多种领域和任务中表现出显著能力，包括抽象、理解、视觉、编码、数学、医学、法律、对人类动机和情感的理解等。值得注意的是，prompt 的选择对于适当的预测至关重要，直接在 prompt 上进行训练可以提高模型在未见任务上的泛化鲁棒性。通过扩大模型规模以及增加训练指令的数量或多样性，这种泛化能力可以进一步增强。

* **上下文学习**

大量研究表明，大语言模型可以通过上下文学&#x4E60;**`ICL`**&#x6267;行各种复杂任务，这指的是模型从上下文中少量示例中学习的能力。少样本上下文学习通过将原始输入与几个完整示例连接作为提示来丰富上下文，从而提高语言模型的预测性能。ICL 的核心思想是通过类比学习，这类似于人类的学习过程。此外，由于 prompt 是以自然语言编写的，交互具有可解释性和可变性，使得将人类知识融入大语言模型变得更加容易。与监督学习过程不同，ICL 不涉及微调或参数更新，这可以大大降低模型适应新任务的计算成本。除了文本之外，研究人员还探索了上下文学习在不同多模态任务中的潜力，使得智能体能够应用于大规模真实世界任务。

* **持续学习**

最近的研究强调了大语言模型规划能力在促进智能体持续学习方面的潜力，这涉及技能的持续获取和更新。持续学习的一个核心挑战是灾难性遗忘：当模型学习新任务时，往往会丢失先前任务的知识。为了解决这一挑战，已经进行了大量努力，大致可分为三类：

> 1. **引入与先前模型相关的常用术语**
>
> 2. **近似先前的数据分布**
>
> 3. **设计具有任务自适应参数的架构**

基于大语言模型的智能体已成为一种新范式，利用大语言模型的规划能力结合现有技能以应对更复杂的挑战。**`Voyager`**&#x5C1D;试解决由 GPT-4 设计的自动课程提出的渐进式更难的任务。通过从简单程序中合成复杂技能，智能体不仅迅速提升了能力，还有效应对了灾难性遗忘。

### 7.2.3 行动模块

* **文本输出**

基于 Transformer 的生成式大语言模型的兴起与发展，赋予了基于 LLM 的智能体与生俱来的语言生成能力。它们生成的文本质量在多个方面表现出色，例如流畅性、相关性、多样性以及可控性。因此，基于 LLM 的智能体可以成为非常强大的语言生成器。

* **工具使用**

工具是工具使用者能力的延伸。面对复杂任务时，人类会使用工具来简化问题解决过程并提高效率，从而节省时间和资源。同样，如果智能体也学会使用工具，它们将能够更高效、更高质量地完成复杂任务。

基于 LLM 的智能体在某些方面存在局限性，而工具的使用可以增强这些智能体的能力。首先，尽管基于 LLM 的智能体拥有强大的知识库和专业能力，但它们无法记住每一条训练数据。此外，由于上下文提示的影响，它们可能无法调用正确的知识，甚至生成虚假内容。再加上特定领域和场景中的语料库、训练数据及调优不足，智能体在专业化领域的表现也会受到限制。通过专门设计的工具，LLM 可以以插件形式提升其专业知识水平，适应领域需求，并更好地满足特定领域的需要。此外，基于 LLM 的智能体在决策过程中缺乏透明性，这使得它们在医疗、金融等高风险领域的可信度较低。同时，LLM 容易受到对抗性攻击，对输入微小变化的鲁棒性不足。相比之下，借助工具完成任务的智能体表现出更强的可解释性和鲁棒性。工具的执行过程可以反映智能体处理复杂需求的方式，并增强其决策的可信度。而且，由于工具是为其特定使用场景设计的，利用这些工具的智能体更能应对输入微小变化，并具备更强的抗攻击能力。

基于 LLM 的智能体不仅需要使用工具，也非常适合进行工具集成。通过预训练过程中积累的丰富世界知识以及 CoT 提示，LLM 在复杂交互环境中展现了卓越的推理和决策能力，这有助于智能体以适当的方式分解并解决用户指定的任务。此外，LLM 在意图理解等方面表现出显著潜力。当智能体与工具结合时，工具使用的门槛得以降低，从而充分释放人类用户的创造性潜力。

1. **对工具的理解** &#x20;

智能体有效使用工具的前提是对工具的应用场景和调用方法有全面的理解。如果缺乏这种理解，智能体使用工具的过程将变得不可靠，无法真正提升其能力。借助 LLM 强大的 Zero-Shot 和 Few-Shot 学习能力，智能体可以通过描述工具功能和参数的 Zero-Shot 提示，或提供特定工具使用场景和方法的少样本提示来获取相关知识。这些学习方式类似于人类通过查阅工具手册或观察他人使用工具来学习的方法。单一工具往往不足以应对复杂任务，因此智能体需要先将复杂任务分解为子任务，而对工具的理解在任务分解中起着重要作用。

* **学习使用工具** &#x20;

智能体学习使用工具的主要方法包括从演示中学习和从反馈中学习。这包括模仿人类专家的行为，以及理解自身行为的结果，并根据环境和人类提供的反馈进行调整。环境反馈包括任务是否成功完成的结果反馈，以及捕捉行为引起环境状态变化的中间反馈；人类反馈则包括明确的评价和隐含的行为，例如点击链接。如果智能体僵化地应用工具而不具备适应性，它无法在所有场景中达到可接受的表现。智能体需要将在特定情境中学到的工具使用技能推广到更普遍的情况中，例如将针&#x5BF9;**`Yahoo`**&#x641C;索训练的模型迁移到Google搜索上。为此，智能体需要掌握工具使用策略中的通用原则或模式，这可能通过元工具学习实现。增强智能体对简单工具与复杂工具之间关系的理解，例如复杂工具如何建立在简单工具之上，有助于其推广工具使用能力，使其能够有效区分不同应用场景并将先前学到的知识迁移到新工具上。**课程学习** Curriculum Learning 方法与此需求相符，它允许智能体从简单工具开始，逐步学习复杂工具。此外，得益于对用户意图推理和规划能力的理解，智能体可以更好地设计工具使用和协作方法，从而提供更高质量的结果。

* **制造工具以实现自给自足** &#x20;

现有工具通常是为了方便人类设计的，未必最适合智能体使用。为了使智能体更好地使用工具，需要专门为智能体设计的工具。这些工具应更具模块化特性，并具有更适合智能体的输入输出格式。如果提供说明和演示，基于 LLM 的智能体还能够通过生成可执行程序或整合现有工具来创建更强大的工具。它们还可以学习进行自我调试。此外，如果作为工具制造者的智能体成功创建了一个工具，它不仅可以自己使用该工具，还可以在多智能体系统中为其他智能体生成包含工具代码和演示的包。推测未来，智能体可能会实现高度自主，并在工具使用方面表现出高度自治。

* **扩展智能体的动作空间**

借助工具，智能体可以在推理和规划阶段利用各种外部资源，例如网络应用和其他语言模型。这一过程可以为基于 LLM 的智能体提供高专业性、可靠性、多样性和质量的信息，从而促进其决策和行动。例如，基于搜索的工具可以通过外部数据库、知识图谱和网页扩大智能体可访问的知识范围和质量，而特定领域的工具可以增强智能体在相应领域的专业能力。一些研究人员已经开发出基于 LLM 的控制器，用于生成 SQL 语句查询数据库，或将用户查询转换为搜索请求并使用搜索引擎获取所需结果。此外，基于 LLM 的智能体可以使用科学工具执行化学中的有机合成任务，或与 Python 解释器交互以增强其在复杂数学计算任务中的性能。在多智能体系统中，通信工具，如电子邮件，可以在严格的安全约束下作为智能体之间互动的手段，促进协作并展现自主性和灵活性。

尽管上述工具增强了智能体的能力，但与环境的交互媒介仍以文本为基础。然而，工具旨在扩展语言模型的功能，其输出并不局限于文本。非文本输出的工具可以多样化智能体动作的模态，从而扩展基于 LLM 的智能体的应用场景。例如，智能体可以通过视觉模型完成图像处理和生成任务。在航空航天工程中，智能体被探索用于物理建模和求解复杂微分方程；在机器人领域，智能体需要规划物理操作并控制机器人执行任务。能够通过工具或多模态方式动态与环境或世界互动的智能体被称为数字化具身。

* **具身行动**

在追求通用人工智能 AGI 的过程中，具身智能体被认为是一个关键范式，因为它致力于将模型智能与物理世界相结合。具身假说从人类智能发展过程中汲取灵感，认为智能体的智能来源于与环境的持续交互和反馈，而非仅仅依赖精心编纂的教科书。同样，与传统深度学习模型通过互联网数据集学习明确能力以解决领域问题不同，人们期望基于 LLM 的智能体行为不再局限于纯文本输出或调用特定工具完成特定任务。相反，它们应能够主动感知、理解和与物理环境互动，做出决策，并基于LLM的广泛内部知识生成具体行为以改变环境。我们将这些统称为具身行动，这种能力使智能体能够以接近人类行为的方式与世界互动并理解世界。

**基于 LLM 的智能体在具身行动中的潜力**

在LLM广泛兴起之前，研究人员倾向于使用强化学习等方法探索智能体的具身行动。尽管基于强化学习的具身研究取得了广泛成功，但在某些方面仍存在局限性。简而言之，强化学习算法在数据效率、泛化能力和复杂问题推理方面面临挑战，这主要源于对动态且常模棱两可的真实环境建模的困难，或对精确奖励信号表示的高度依赖。最近的研究表明，利用LLM预训练过程中获得的丰富内部知识可以有效缓解这些问题。

1. **成本效率** &#x20;

一些策略算法在样本效率上表现不佳，因为它们需要新鲜数据进行策略更新，而收集足够的具身数据用于高性能训练既昂贵又充满噪声。这种限制也存在于一些端到端模型中。通过利用LLM的内在知识，像PaLM-E这样的智能体联合训练机器人数据与通用视觉-语言数据，从而在具身任务中实现显著的迁移能力，同时展示了几何输入表示可以提高训练数据效率。

* **具身行动泛化** &#x20;

智能体的能力应超越特定任务。面对复杂且未知的真实环境时，智能体必须展现出动态学习和泛化能力。然而，大多数强化学习算法旨在训练和评估特定任务的相关技能。相比之下，经过多样化形式和丰富任务类型的微调，LLM展现了卓越的跨任务泛化能力。例如，PaLM-E在处理新对象或现有对象的新组合时表现出令人惊讶的零样本或一样本泛化能力。此外，语言能力是基于LLM智能体的独特优势，它既是与环境互动的手段，也是将基础技能迁移到新任务的媒介。SayCan利用LLM将提示中的任务指令分解为相应的技能命令，但在部分可观测环境中，有限的先验技能往往无法达到满意的表现。为了解决这一问题，Voyager引入了技能库组件，持续收集新颖的自我验证技能，从而赋予智能体终身学习能力。

* **具身行动规划** &#x20;

规划是人类应对复杂问题以及基于LLM的智能体所采用的关键策略之一。在LLM展现出显著推理能力之前，研究人员引入了分层强化学习（HRL）方法，其中高层策略为目标设定子目标，低层策略生成适当的行为信号。类似于高层策略的作用，具有新兴推理能力的LLM可以无缝应用于复杂任务，以零样本或一样本的方式完成任务。此外，来自环境的外部反馈可以进一步提升基于LLM智能体的规划性能。根据当前环境反馈，一些工作动态生成、维护和调整高层行动计划，以减少对部分可观测环境中先验知识的依赖，从而将计划落地。反馈也可以来自模型或人类，通常被称为批评者，根据当前状态和任务提示评估任务完成情况。

**基于 LLM 的智能体的具身行动**

根据智能体在任务中的自主程度或行动的复杂性，基于LLM的具身行动主要包括观察、操作和导航。

1. **观察** &#x20;

观察是智能体获取环境信息和更新状态的主要方式，在提升后续具身行动效率方面起着关键作用。如§3.2所述，具身智能体的观察主要发生在具有多种输入的环境中，这些输入最终被整合为多模态信号。一种常见方法是使用预训练的视觉Transformer（ViT）作为文本和视觉信息的对齐模块，并标记特殊标记以表示多模态数据的位置。Soundspaces提出通过混响音频输入识别物理空间几何元素，从而增强智能体的观察视角。近期，更多研究将音频作为一种嵌入式观察模态。除了广泛使用的级联范式外，类似于ViT的音频信息编码进一步增强了音频与其他输入模态的无缝集成。智能体对环境的观察还可以来源于人类提供的实时语言指令，而人类反馈有助于智能体获取可能难以直接获得或解析的详细信息。

* **操作** &#x20;

一般来说，具身智能体的操作任务包括物体重新排列、桌面操作和移动操作。典型场景涉及智能体在厨房执行一系列任务，例如从抽屉中取出物品递给用户或清理桌面。除了精准的观察外，这还涉及利用LLM结合一系列子目标。因此，保持智能体状态与子目标之间的同步至关重要。DEPS利用基于LLM的交互规划方法维持这种一致性，并在整个多步骤、长时间推理过程中通过智能体反馈帮助纠错。与之相比，AlphaBlock专注于更具挑战性的操作任务（例如用积木拼出笑脸），这要求智能体对指令有更扎实的理解。不同于现有的开环范式，AlphaBlock构建了一个包含35个复杂高级任务的数据集，以及相应的多步规划和观察对，并通过微调多模态模型增强其对高级认知指令的理解。

* **导航** &#x20;

导航使智能体能够在环境中动态改变位置，通常涉及多角度、多目标观察以及基于当前探索的长跨度操作。在导航之前，具身智能体需要建立对外部环境的内部地图，通常以拓扑图、语义图或占用图的形式呈现。例如，LM-Nav利用VNM创建内部拓扑图，并进一步利用LLM和VLM分解输入命令和分析环境以找到最佳路径。此外，一些研究强调了空间表示的重要性，通过利用预训练的VLM模型将图像中的视觉特征与物理世界的3D重建结合，以实现空间目标的精确定位，而非传统的点或对象中心导航动作。导航通常是长跨度任务，智能体的未来状态受其过去行为影响。需要记忆缓冲区和摘要机制作为历史信息的参考，这在Smallville和Voyager中也有应用。此外，一些研究提出音频输入也具有重要意义，但将音频信息与视觉环境关联仍面临挑战。一个基本框架包括动态路径规划器，利用视觉和听觉观察以及空间记忆规划一系列导航动作。

通过整合这些能力，智能体可以完成更复杂的任务，例如具身问答。其主要目标是自主探索环境，并回答预定义的多模态问题，例如：“厨房里的西瓜比锅大吗？”“哪个更硬？”为了回答这些问题，智能体需要导航到厨房，观察两个物体的大小，然后通过比较得出答案。

在控制策略方面，如前所述，基于 LLM 并在特定具身数据集上训练的智能体通常会生成高层策略指令，以控制低层策略实现特定的子目标。低层策略可以是一个机器人Transformer，它将图像和指令作为输入，并为末端执行器和机械臂生成控制命令，以完成特定的具身任务。最近，在虚拟具身环境中，高层策略被用于控制游戏中的智能体或模拟世界中的角色。例如，Voyager通过调用Mineflayer API接口不断获取各种技能并探索世界。

**具身行动的未来前景**

基于LLM的具身行动被视为连接虚拟智能与物理世界的桥梁，使智能体能够像人类一样感知和改变环境。然而，仍存在一些限制，例如物理世界中机器人操作的成本高昂以及具身数据集的稀缺性，这促使越来越多的研究转向在 Minecraft 等模拟环境中探索智能体的具身行动。通过利用Mineflayer API，这些研究能够以低成本的方式广泛测试智能体的各种操作，包括探索、规划、自我改进甚至终身学习。尽管取得了显著进展，但由于模拟平台与物理世界之间存在显著差异，实现最佳的具身行动仍然面临挑战。为了在现实场景中有效部署具身智能体，对于更贴近真实条件的具身任务范式和评估标准的需求日益增加。另一方面，如何让智能体将语言与实际情境结合也是一个障碍。例如，“像猫一样跳下来”这样的表达主要传达了一种轻盈和安静的感觉，但这种语言隐喻需要足够的世界知识支持。有研究尝试将文本蒸馏与事后经验回放 **HER**（**H**indsight **E**xperience **R**eplay）相结合，构建一个数据集作为训练过程的监督信号。然而，随着具身行动在人类生活的各个领域中扮演越来越重要的角色，对具身数据集的进一步研究仍然是必要的。

## 7.3 Agent 系统优化

### 7.3.1 微调优化

### 7.3.2 无微调优化

## 7.4 Agent 框架

### 7.4.1 AutoGPT

### 7.4.2 BabyAGI

### 7.4.3 AutoGen

### 7.4.4 MetaGPT
\]

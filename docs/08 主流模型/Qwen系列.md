## 4.4 Qwen 系列

> **论文**：[**Qwen1**](https://arxiv.org/pdf/2309.16609)  **[Qwen2](https://arxiv.org/pdf/2407.10671)  [Qwen2.5](https://arxiv.org/pdf/2412.15115)  [YaRN](https://arxiv.org/pdf/2309.00071)  [DCA](https://arxiv.org/pdf/2402.17463)  [ABF](https://arxiv.org/pdf/2309.16039)**
>
> **代码**：**[Qwen1](https://github.com/QwenLM/Qwen)  [Qwen2.5](https://github.com/QwenLM/Qwen2)**

### 4.4.1 Qwen1

**Qwen** 是由阿里提出的聊天和预训练大型语言模型。目前开源了 **Qwen** 的各系列模型，包括基础语言模&#x578B;**`Qwen-1.8B`**、**`Qwen-7B`**、**`Qwen-14B`**&#x548C;**`Qwen-72B`**，以及相应的聊天模&#x578B;**`Qwen-1.8B-Chat`**、**`Qwen-7B-Chat`**、**`Qwen-14B-Chat`**&#x548C;**`Qwen-72B-Chat`**。各个规模模型参数如右表。

![]()

此外，阿里还发布了专门的编码和数学模型，&#x5373;**`Code-Qwen`**, **`Code-Qwen-Chat`**&#x548C;基于 **Qwen** 的数学模&#x578B;**`Math-Qwen-Chat`。**

![]()

除此之外，还有多模态大模&#x578B;**`Qwen-VL`**&#x548C;**`Qwen-VL-Chat`**。模型总览图如上图。

* **模型结构**

**Qwen** 使用经过修改的 Transformer 架构，并采用了最近流行的大型语言模型 **LLaMA** 的训练方法，具体有以下几点：

**分词**

**Qwen** 使&#x7528;**`BPE`**&#x4F5C;为分词方法，并从开源 fast BPE tokenizer 中选&#x62E9;**`tiktoken`**&#x4F5C;为起点。为了提高模型在多语言下游任务上的性能，特别是中文任务，**Qwen**增加了常用汉字、词语和其他语言中的词汇量。同时，**Qwen** 也将数字拆分为单个数字。最终的词汇表大小约&#x4E3A;**`152K`**。实验结果表明 **Qwen** 相对于其他分词器具有更高的压缩效率，在大多数语言中都能实现更高的信息传递效率。此外还证明扩展 **Qwen** 的词汇表大小不会对下游任务的表现产生负面影响。具体的分词步骤如下：

> 1. 基于开源分词器 tiktoken &#x7684;**`cl100k`**&#x57FA;础词表进行初始化
>
> 2. 针对中文场景，向词表中增添常用的中文字和词，扩充词表规模
>
> 3. 参考 GPT-3.5 和 LLaMA 的实现，将数字切分成单个数字，如：&#x5C06;**`123`**&#x5206;词&#x4E3A;**`1`**、**`2`**、**`3`**

> **注**：将数字分成各个数字的原因主要与模型的训练效率和下游任务的表现有关
>
> 1. **增强模型的泛化能力**：将数字拆分成单个数字，模型可以更好地理解和处理不同的数字组合，而不仅仅是记住特定的数字。这种方法提高了模型在处理新数字和未见过的数字组合时的泛化能力。
>
> 2. **减少词汇表的大小**：数字有无数种组合形式，若每个数字都作为一个单独的词汇存在于词汇表中，会显著增加词汇表的大小。而拆分数字可以减少词汇表的大小，从而提高模型的训练效率和推理速度。

**输入嵌入和输出投影**

**Qwen** 不共享输入 Embedding 和输出 Projection 的权重，因为输入嵌入和输出投影在功能上有不同的需求，使用独立的权重允许有更多的灵活性，以便更好地满足需求，从而提高模型的整体表现。但代价是增加了内存消耗。

**位置编码&#x20;**

**Qwen** 选择旋转位置编&#x7801;**`RoPE`**&#x4F5C;为首选选项，以将位置信息纳入 **Qwen** 模型中，并且 **Qwen** 选择使&#x7528;**`FP32`**&#x7CBE;度来计算逆频率矩阵，而不是 **BF16** 或 **FP16**，以便优先考虑模型性能并实现更高的准确性。

> **注**：使用 FP32 的原因
>
> 1. **逆频率矩阵的需要**：在RoPE中，位置编码依赖于逆频率矩阵来计算位置向量的旋转。位置向量的旋转通过旋转矩阵实现，而这个旋转矩阵又依赖于位置和频率之间的精确关系。

![Qwen 模型结构]()

> 2. **高精度计算**：如果逆频率矩阵的精度不够高，会导致位置编码不准确。这会直接影响模型对序列中相对位置关系的理解，从而影响模型在处理位置敏感任务时的性能。这种高精度有助于减少数值计算中的误差，提高模型的稳定性和准确性。

**线性偏置项&#x20;**

对于大多数层，**Qwen** 删除偏差，但对于注意力中的 **QKV** 层添加了偏差，以增强模型的外推能力。

**优点**

移除偏置项可以减少参数数量，从而降低模型的复杂度和过拟合的风险，有助于提高模型的稳定性和性能，而且在训练过程中可以更好地泛化，从而在不同的数据集上表现得更为一致。

**缺点**

在 QKV 层中添加偏差项可以增强模型的外推能力，更好地捕捉输入数据的特征，从而提升模型在处理未知或未见数据时的表现。这种增强外推能力的做法对于处理复杂任务和应对多样化的数据输入非常重要。

**归一化&#x20;**

**Qwen** 使用 **Pre-Norm&#x20;**&#x5F52;一化方法，因为在 Transformer 结构的模型中，**Pre-Norm** 是最常用的方法，并且已被证明比 **Post-Norm** 更能提高训练稳定性。此外，**Qwen** &#x7528;**`RMSNorm`**&#x53D6;代了传统层归一化技术。这一改变带来了相同的表现水平，同时也提高了效率。

**Pre-Norm&#x20;**

1. **稳定性**：在训练早期可以稳定梯度，缓解梯度消失和梯度爆炸的问题。

2. **收敛性**：收敛速度较快，因为归一化使得每个子层的输入具有较好的分布，促进了模型的学习。

   **缺点**

1) **深层网络训练**：在非常深的网络中会导致网络中的信息传递不够充分，因为归一化会削弱信息的传递。

**Post-Norm&#x20;**

1. **信息传递**：处理深层网络时表现更好，因为可以保留和传递子层信息，捕捉复杂的特征和模式。

2. **性能提升**：可以提高模型的最终性能，特别是对于需要捕捉长程依赖关系的任务。

   **缺点**

1) **训练难度**：训练初期可能会面临梯度消失或梯度爆炸的问题，导致梯度不够稳定。

2) **收敛速度**：收敛速度较慢，需要更多的训练步骤和调整超参数来稳定训练过程。

> **注**：选择 **Pre-Norm** 还是 **Post-Norm** 取决于具体的应用场景和模型需求：
>
> 1. **收敛速度和训练稳定性**：如果优先考虑训练的收敛速度和稳定性，Pre-Norm 可能是更好的选择。
>
> 2. **深层模型和信息传递**：对于需要训练非常深的模型或需要捕捉复杂依赖关系的任务，Post-Norm 可能更合适。

**激活函数**

**Qwen** 选择&#x4E86;**`SwiGLU`**&#x4F5C;为激活函数，它是 Swish 和门控线性单元的组合。**Qwen** 实验证明基于 GLU 的激活函数通常优于其他基准选项，如 GeLU。与以往研究常见的做法一样，**Qwen** 将 FFN 的维度从隐藏大小的四倍减少到三分之八倍的隐藏大小。

**LayerNorm**

1. **稳定性**：对每一层的输入进行归一化，可以缓解梯度消失和梯度爆炸的问题，提高模型的稳定性。

2. **适用性广泛**：在各种神经网络结构中表现良好，尤其适用于 RNN 和 Transformers。

3. **无批量依赖**：不依赖于批量数据的大小，在小批量甚至单样本情况下依然有效。

**缺点**

1. **计算复杂度**：需要计算均值和标准差，这在计算上相对复杂。

2. **对特征缩放敏感**：有时会对特征缩放较为敏感，需要额外调整超参数。

**RMSNorm**

1. **计算效率高**：计算比LayerNorm更简单，只需计算均方根值，不需要计算均值和标准差。&#x20;

2. **稳定性好**：能缓解梯度消失和梯度爆炸的问题，提升模型的训练稳定性。

3. **适用性**：特别适用于需要简化计算的模型，如某些大规模预训练模型。

   **缺点**

1) **信息损失**：只考虑了均方根值，可能会丢失一些与输入分布相关的信息。



> **注**：选择 **SwiGLU** 的好处
>
> 1. Swish 对于负值的响应相对较小，克服了 ReLU 某些神经元上输出始终为零的缺点。
>
> 2. GLU 的门控特性，意味着它可以根据输入的情况决定哪些信息应该通过、哪些信息应该被过滤。这种机制可以使网络更有效地学习到有用的表示，有助于提高模型的泛化能力。在大语言模型中，这对于处理长序列、长距离依赖的文本特别有用。
>
> 3. **SwiGLU** 中的参&#x6570;**`W1`**,**`W2`**,**`W3`**,**`b1`**,**`b2`**,**`b3`**&#x53EF;以通过训练学习，使得模型可以根据不同任务和数据集动态调整这些参数，增强了模型的灵活性和适应性。
>
> 4. 计算效率相比某些较复杂的激活函数如 GELU 等更高，同时仍能保持较好的性能。这对于大规模语言模型的训练和推理是很重要的考量因素。

* **预训练**

预训练阶段包括学习大量数据，以获得对世界及其各种复杂性的全面理解。这不仅包括基本的语言能力，还包括算术、编码和逻辑推理等高级技能。

**数据**

**Qwen** 使用了高达3万亿个 token 的数据进行预训练，数据主要涉及公共网络文档、百科全书、书籍、代码等，数据涉及多语言，但以中文和英文为主。为了保证预训练数据的质量，研究团队制定了一套全面的数据预处理程序：

> 1. **文本数据抓取**：对于 Web 数据，从 HTML 中提取文本内容
>
> 2. **语言识别**：采用语言识别工具确定语种
>
> 3. **去重**：为了增加数据的多样性，采用了重复数据删除技术，包括语言规范化后的精确匹配重复数据删除和使&#x7528;**`MinHash`**&#x548C;**`LSH`**&#x7B97;法的模糊重复数据删除
>
> 4. **质量过滤**：结合规则和机器学习的方法过滤低质量数据，即通过多个模型对内容进行评分，包括语言模型、文本质量评分模型
>
> 5. **安全控制**：使用模型识别并过滤涉及暴力、偏见、色情等潜在冒犯性内容
>
> 6. **上采样**：针对某些高质量源的数据进行上采样，以确保多样化的高质量内容
>
> 7. **引入指令数据**：引入高质量的指令数据
>
> 8. **防止数据泄露**：为了消除了与评估中使用的测试集中存在的任何数据重叠的问题，删除了任何与测试集中指令样本存在超过 13-gram 的文本内容

**训练**

具体的训练细节如下：

> 1. 采用标准的自回归语言模型训练目标
>
> 2. 训练时上下文长度&#x4E3A;**`2048`**
>
> 3. 注意力模块采&#x7528;**`Flash Attention`**&#x6280;术，以提高计算效率并减少内存使用
>
> 4. 采&#x7528;**`AdamW`**&#x4F18;化器，设置$$β_1=0.9, β2=0.95, \epsilon=1e-8$$
>
> 5. 使用余弦学习率，为每种模型设定一个峰值学习率，学习率会衰减到峰值的10%
>
> 6. 使&#x7528;**`BFloat16`**&#x6DF7;合精度加速训练

**外推能力扩展**

Transformer 架构的模型在注意机制的上下文长度方面有很大的限制。随着上下文长度的增加，二次复杂度计算导致计算和内存成本的急剧增加。为了解决这个问题，**Qwen&#x20;**&#x9009;择在推理阶段实现上下文长度的扩展 ，而没有在训练阶段扩展，因为在训练阶段扩展上下文会导致训练成本的急速增长。

由于 **Qwen** 使用的位置编码是 **RoPE**，虽然 **RoPE** 相较于绝对位置编码具有可外推的优点，但是在直接使用外推的过程中还是存在一些问题，因此 **Qwen** 针对这些问题，来进行了改进。

**RoPE** 使用正弦和余弦函数将位置信息嵌入到词汇向量的旋转矩阵中。然而 **RoPE** 直接外推会导致 Attention Score 显著增加：

**正弦和余弦函数的周期性**

1. 正弦和余弦函数是周期性的，在训练数据中，位置通常在一个相对较小的范围内，&#x5982;**`0`**&#x5230;**`2048`**，这些位置的编码值会保持在周期的某一部分。

2. 当位置超出这个范围时，如位置变&#x4E3A;**`3000`**，编码值会进入正弦和余弦函数的另一个周期。由于这些函数的周期性，这些位置的编码值可能与训练数据中的编码值非常不同，导致模型在计算注意力分数时出现剧烈变化。

**高频成分的影响**

1. 在 **RoPE** 编码中，较高维度的编码，即频率较高的正弦和余弦成分，会对较大的位置变化更加敏感。这意味着，随着位置数值的增加，这些高频成分会迅速变化。

2. 对于较大的位置值，正弦和余弦函数的值可能会经历快速变化。这种快速变化会导致 Attention 机制中的 Attention Score 出现显著波动。

为了解决 **RoPE** 直接外推产生的 Attention Score 过大，以及 **RoPE** 嵌入插值时丢失高频信息的问题，**Qwen** 利用&#x4E86;**`动态NTK感知插值`**&#x6765;实现推理阶段的上下文长度扩展。

**NTK-aware 插值**

**`NTK-aware插值`**&#x6838;心思想是：**高频外推，低频内插**。

与 **RoPE** 将每个维度平均缩放一个因子$$S$$不同，**NTK-aware** 插值通过减少高频的缩放和增加低频的缩放来将插值压力分散到多个维度；虽然可以通过许多方法获得这样的变换，但最简单的方法是对$$θ$$的值进行基础更改。

一般来说，内插方法可以写成如下表达式：

$$f'(x_m, m, \theta) = f(x_m, g(m), h(\theta))$$

其中第$$i$$个维度有$$\theta_i=b^{\frac{−2(i−1)}{d}}, b=10000, i=1,2,\cdots, \frac{d}{2}$$，这里从位置和旋转角度两个方面对所有内插方案进行了总结，即所有内插方案都是建立在对二者的变换上的。此时可以将位置插值 **PI** 改写为：

$$f_{PI}(x_m, m, \theta) = f(x_m, g(m)=\frac{m}{S}, h(\theta_i)=\theta_i)$$

即 **PI** 中没有对旋转角度做任何改变，只是将位置索引除以扩展比。基于上式，NTK-aware 的做法可以被表述为：

$$f_{NTK}(x_m, m, \theta) = f(x_m, g(m)=m, h(\theta_i)=(b\cdot S^{\frac{d}{d-2}})^{\frac{−2(i−1)}{d}})$$

即 **NTK-aware** 插值本质上就是将原始 **RoPE** 中的$$\theta_i=b^{\frac{−2(i−1)}{d}}$$改为$$h(\theta_i)=(b\cdot S^{\frac{d}{d-2}})^{\frac{−2(i−1)}{d}}$$，更本质的区别是将基数 base 乘以了一个和扩展比$$S$$有关的常量$$S^{\frac{d}{d-2}}$$，与位置插值 **PI** 相比，这种方法在扩展非微调模型的上下文大小方面表现得更好。然而这种方法的一个主要缺点是，它不仅仅是一种插值方案，一些维度会被轻微外推到超出边界的值，因此使用 **NTK-aware** 插值进行微调的结果不如 **PI**；此外，由于存在越界值，理论尺度因子$$S$$并不能准确描述真实的上下文扩展尺度。在实践中，对于给定的上下文长度扩展，尺度值$$S$$必须设置得高于预期尺度。

**Dynamic NTK-aware 插值**

在多数情况下，以从 1 到最大上下文大小不等的序列长度进行多次前向传递。一个典型的例子是自回归生成，其中序列长度在每一步之后递增 1，有两种方法可以应用使用比例因子$$S$$的插值方法：

> 1. 整个推理周期中嵌入层固定，包括缩放因子$$S=\frac{L'}{L}$$，$$L'$$是固定数量的扩展上下文大小
>
> 2. 每次前向传递时更新缩放因子：$$S=\max(1,l'/L)$$，$$l'$$是当前序列的序列长度

上述方法中，方法 1 的问题在于模型在长度小于 L 时可能出现性能折扣，当序列长度大于 L′ 时可能出现突然退化，因此提出了方法 2，这种推理方法为动态缩放方法，当与 NTK-aware 插值相结合时，称之为 Dynamic NTK-aware 插值。

**LogN-Scaling**

根据熵不变性以及一些合理的假设，可以得到一个新的缩放因子，从而得到一种 **Scaled Dot-Product Attention**：

$$\text{Attention}(Q,K,V)=\text{softmax}({\frac{\kappa\log n}{d} QK^\top})V$$

这里的$$\kappa$$是一个跟$$𝑛,𝑑$$都无关的超参数。LogN-Scaling 可以根据上下文长度与训练长度的比值，对$$Q$$和$$V$$的点积进行重新缩放，确保注意力值的熵随着上下文长度的增长而保持稳定。

**分层窗口 Self-Attention**

分层窗口将注意力限制在一个上下文窗口内，防止模型关注到太远的内容。**Qwen** 团队观察到模型在处理长上下文时在不同层次上的建模能力存在差异，较低的层次相对于较高的层次更加敏感于上下文长度的扩展；为此，为每个层分配了不同的窗口大小，对较低的层使用较短的窗口，对较高的层使用较长的窗口。

综合这些技术，**Qwen&#x20;**&#x6A21;型在推理阶段可以处&#x7406;**`8192`**（相较于训练所使用&#x7684;**`2048`**，扩展了4倍）个 token 的长序列，外推能力优异。同时，实验结果表明，应用以上所述的关键技术可以使模型在上下文长度增加时始终保持低困惑度，这表明这些技术在增强模型理解和生成长文本的能力方面发挥了重要作用。

* **对齐**

**Qwen&#x20;**&#x9884;训练的模型在实际应用中表现与人类行为不一致，这里展示 **Qwen** 模型如何进行 **SFT** 和 **RLHF** 提高语言模型进行自然对话的能力。

**SFT 数据**

在 SFT 数据处理过程中，**Qwen** 主要采用了以下手段：

1. **使用 ChatML 格式**：**Qwen** 采用 **ChatML** 格式来进行模型训练。它利用特殊符号表示不同类型信息，如系统设置、用户输入、助手输出等，这有助于模型有效区分信息

![]()

2. **对话流式风格**：采用会话流式对话风格，而不是简单的问答形式，使模型学会真实的人机交互

3. **任务的多样性**：通过专注于不同任务的自然语言生成来提高模型的有用性

4. **去除格式化数据**：为了确保模型能够泛化到广泛的场景，特别排除了在提示模板中格式化的数据，因为这些数据可能会限制模型功能

5. **安全性**：通过注释与安全问题相关的数据（如暴力、偏见和色情）来优先考虑语言模型的安全性

**SFT 训练**

在SFT的训练过程中，**Qwen** 采用了以下手段：

> * **训练目标**：与预训练一致，使用 Next Token Prediction
>
> * **损失函数**：在训练过程中对系统和用户输入应用 loss mask，将用户、系统的输入 mask 掉，不计算loss，仅对 **Qwen** 的回复部分计算 loss
>
> * **优化器**：使&#x7528;**`AdamW`**&#x4F18;化器，超参数$$β_1, β_2, ϵ$$为别为 0.9、0.95 和 1e−8，学习率先增后恒定
>
> * **批大小**：序列长度限制在 2048，训练 batch size=128
>
> * **学习率**：训练 4000 步,在前 1430 步中，学习率逐渐增加，达到 2e−6 的峰值
>
> * **防过拟合**：为了防止过拟合，权重衰减的值设置为 0.1，dropout 设置为 0.1，梯度裁剪的限制为 1.0

**Reward Model 训练**

在构建偏好模型时，分成了两步：偏好模型预训练、偏好模型微调。

预训练偏好模型 **PMP**（**P**reference **M**odel **P**retraining）：

> 1. **比较数据集**：使用了一个庞大的比较数据集，该数据集由样本对组成，每个样本对包含对单个查询的两个不同的响应及其相应的人类偏好
>
> 2. **池化层**：奖励模型由同等大小 **Qwen** 模型+池化层得来，用特殊的句子结束标记经过池化层的映射值作为模型奖励值

微调奖励模型：

> 1. **分类系统**：为确保 Prompt 具备一定的多样性和复杂性，创建了一个包含约 6600 详细标签的分类系统
>
> 2. **平衡采样**：采用了一种平衡采样算法，以在选择提示时兼顾多样性和复杂性
>
> 3. **多样采样**：为了生成多样的回复，实验过程使用了不同规模和采样策略的 **Qwen** 模型，因为多样化的回复有助于降低标注难度并提高奖励模型的性能

模型在训练过程中，学习率恒为 3e−6，批次大小为 64，最大长度为 2048，训练一个 epoch。最终得到的 **PMP** 模型在分布外数据上表现出较高的泛化能力，奖励模型在 **Qwen** 奖励数据集上表现出显著的改进。

**PPO 强化学习**

**PPO** 阶段共包含四个模型：**`policy模型`**、**`value模型`**、**`reference模型`**、**`reward模型`**。在开始 PPO 流程之前，暂停 policy 模型的更新，先对 value 模型训练 50 步预热，从而确保 value 模型能够有效地适应不同的 reward 模型。在 **PPO** 过程中，policy 模型和 value 模型的学习率分别为 1e−6 和 5e−6。为了增强训练的稳定性，裁剪值设置为 0.15。

在进行推理时，生成策略的 top-p 值设置为 0.9。研究结果表明，虽然熵值略低于 top-p=1.0 时的值，但奖励的增加速度更快，最终在类似条件下始终能够获得较高的评估奖励。此外，**Qwen** 还采用了预训练梯度来减缓所谓的对齐税。研究表明，在这种特定的奖励模型下，KL 惩罚在非严格代码或数学性质的基准测试中足以弥补对齐税。与 **PPO** 数据相比，预训练梯度必须使用更大量的预训练数据，以确保预训练梯度的有效性。此外，实证研究表明，过大的系数值会大大阻碍与奖励模型的匹配，最终影响匹配，而过小的系数值对减缓对齐税的效果微不足道。

### 4.4.2 Qwen1.5

**Qwen1.5** 是在 2024 年 2 月份推出的，开源了包&#x62EC;**`0.5B`**、**`1.8B`**、**`4B`**、**`7B`**、**`14B`**、**`32B`**&#x548C;**`72B`**&#x5171;计 7 个不同规模的 Base 和 Chat 模型，, 以及 **Qwen&#x20;**&#x7CFB;列的首个 **MoE** 模&#x578B;**`Qwen1.5-MoE-A2.7B`**。为了提升模型的推理效率，**Qwen1.5** 提供了多种量化方案，支&#x6301;**`Int4`**、**`Int8`**、**`AWQ`**、**`GGUF`**&#x7B49;量化模型。量化技术的应用，特别是&#x5728;**`GPTQ`**&#x548C;**`AWQ`**&#x91CF;化的引入，使得模型可以在不牺牲性能的前提下，显著减少存储和计算资源消耗。

本次更新在对齐最新的 **Qwen1.5** 系列时有效地采用了直接策略优化 **DPO** 和近端策略优化 **PPO** 等技术，着重提升了 Chat 模型与人类偏好的对齐程度，并且显著增强了模型的多语言处理能力。在序列长度方面，所有规模模型均已实&#x73B0;**`32768`**&#x4E2A; tokens 的上下文长度范围支持。同时，预训练 Base 模型的质量也有关键优化。**Qwen1.5** 的性能在多个标准基准测试中得到了验证，包括基础能力（如语言理解、代码生成、推理等）、多语言能力、对齐能力、智能体 **Agent** 能力以及检索增强生成 **RAG** 能力：

> 1. **通用能力**：在不同模型尺寸下，**Qwen1.5** 都在评估基准中表现出强劲的性能。**Qwen1.5-72B** 在所有基准测试中都远远超越&#x4E86;**&#x20;**&#x4C;LaMA-2-70B，展示了其在语言理解、推理和数学方面的卓越能力。参数规模低于 70 亿的 **Qwen1.5** Base 模型，与业界领先的小型模型相比具有很强的竞争力。
>
> 2. **对齐能力**：尽管落后于 GPT-4-Turbo，但最大的 **Qwen1.5** 模型 **Qwen1.5-72B-Chat** &#x5728;**`MT-Bench`**&#x548C;**`Alpaca-Eval v2`**&#x4E0A;都表现出不俗的效果，超过了 Claude-2.1、GPT-3.5-Turbo-0613、Mixtral-8x7b-instruct 和 TULU 2 DPO 70B，与 Mistral Medium 不相上下。
>
> 3. **多语言能力**：**Qwen1.5** Base 模型在 12 种不同语言的多语言能力方面表现出色，在考试、理解、翻译和数学等各个维度的评估中，均展现优异结果。不论阿拉伯语、西班牙语、法语、日语，还是韩语、泰语，**Qwen1.5&#x20;**&#x5747;展示了在不同语言环境中理解和生成高质量内容的能力。
>
> 4. **长序列能力**：即使像 **Qwen1.5-7B-Chat** 这样的小规模模型，在多个任务中表现出与 GPT3.5-turbo-16k 类似的性能。最好的模型 **Qwen1.5-72B-Chat**，仅略微落后于 GPT4-32k。
>
> 5. **外部能力**：较大的 **Qwen1.5-Chat** 模型通常优于较小的模型，接近 GPT-4 的工具使用性能。不过，在数学解题和可视化等代码解释器任务中，即使是最大的 **Qwen1.5-72B-Chat** 模型，也会因编码能力而明显落后于 GPT-4。

**Qwen1.5 MoE**

**Qwen1.5-MoE-A2.7B**，仅拥有 27 亿个激活参数，但性能却能与当时最先进的 70 亿参数模型，如 Mistral 7B 和 Qwen1.5-7B 相媲美。相较于包含 65 亿个 Non-Embedding 参数的 **Qwen1.5-7B**，**Qwen1.5-MoE-A2.7B** 只有 20 亿个 Non-Embedding 参数，约为原模型大小的三分之一。此外，相比 **Qwen1.5-7B**，**Qwen1.5-MoE-A2.7B** 的训练成本降低&#x4E86;**`75%`**，推理速度则提升&#x81F3;**`1.74`**&#x500D;。

**Qwen1.5-MoE** 采用了特别设计的MoE架构。通常情况下，如 Mixtral 等，每个 Transformer block 中的 MoE 层会配备 8 个 expert，并采用 top-2 门控策略进行 Routing，但这种配置还存在很大的优化空间。**Qwen1.5-MoE** 对这一架构进行了多项改进：

> 1. **Finegrained Experts**：DeepSeek-MoE 和 DBRX 已经证明了 Finegrained Experts 的有效性。从 FFN 层过渡到 MoE 层时，一般只是简单地复制多次 FFN 来实现多个 Expert。而 Finegrained Experts 的目标是在不增加参数数量的前提下生成更多 Expert，将单个 FFN 分割成几个部分，每个部分作为一个独立的 Expert。**Qwen1.5-MoE** 设计了具有总共 64 个 Expert 的 MoE，这个实现能达到效果和效率的最优。
>
> 2. **初始化**：实验表明从零开始训练 MoE 模型可能效率低下，且难以提升至预期的最优性能水平。因此首先利用已有的 Qwen-1.8B，将其改造为 **Qwen1.5-MoE-A2.7B**。此外在初始化阶段引入随机性可以显著加快收敛速度，并在整个预训练过程中带来更好的整体性能表现。
>
> 3. **新的 Routing 机制**：目前一个明显的趋势是在 MoE 中实现共享 Expert 与 Routing Expert。从宏观角度看这是一种广义的 Routing 方法，因为在没有共享 Expert 的情况下，实际上就退化为传统的 MoE 路由设置。**Qwen1.5-MoE-A2.7B** 在其中整合&#x4E86;**`4`**&#x4E2A;总是被激活的共享 Expert 和每次只激活其&#x4E2D;**`4`**&#x4E2A;&#x7684;**`60`**&#x4E2A; Routing Expert，这种方式非常灵活，并且效率最佳。







### 4.4.3 Qwen2

2024 年 6 月 7 日，阿里巴巴发布了最新的大模型 **Qwen2**，迎来了 **Qwen** 系列模型从 **Qwen1.5** 到 **Qwen2** 的重大升级。相比 **Qwen1.5**，**Qwen2** 在大规模模型实现了非常大幅度的效果提升。**Qwen2-7B** 的数学能力，甚至可以比肩 **Qwen1.5-110B**。

![]()

**Qwen2** 系列具备以下特点：

> 1. 5 个尺寸的预训练和指令微调模型, 包&#x62EC;**`Qwen2-0.5B`**、**`Qwen2-1.5B`**、**`Qwen2-7B`**、**`Qwen2-57B-A14B`** 以&#x53CA;**`Qwen2-72B`**
>
> 2. 在中文英语的基础上，训练数据中增加了 27 种语言相关的高质量数据
>
> 3. 多个评测基准上的领先表现
>
> 4. 代码和数学能力显著提升
>
> 5. 增大了上下文长度支持，最高达到 128K tokens（**`Qwen2-72B-Instruct`**）

5 个模型的参数规模如下表所示：

| **模型**            | **0.5B** | **1.5B** | **7B** | **57B-A14B** | **72B** |
| ----------------- | -------- | -------- | ------ | ------------ | ------- |
| 参数量               | 0.49B    | 1.54B    | 7.07B  | 57.41B       | 72.71B  |
| 非Embedding参数      | 0.35B    | 1.31B    | 5.98B  | 56.32B       | 70.21B  |
| Tie Embedding     | True     | True     | False  | False        | False   |
| 上下文长度             | 32K      | 32K      | 128K   | 64K          | 128K    |
| Hidden Size       | 896      | 1,536    | 3,584  | 3,584        | 8,192   |
| Layers            | 24       | 28       | 28     | 28           | 80      |
| Query Heads       | 14       | 12       | 28     | 28           | 64      |
| KV Heads          | 2        | 2        | 4      | 4            | 8       |
| Head Size         | 64       | 128      | 128    | 128          | 128     |
| Intermediate Size | 4,864    | 8,960    | 18,944 | 2,560        | 29,568  |
| Embedding Tying   | True     | True     | False  | False        | False   |
| Trained Tokens    | 12T      | 7T       | 7T     | 4.5T         | 7T      |

其中 **Qwen2-57B-A14B&#x20;**&#x4E2D;的专家参数如下：Routed Experts = 64，Activated Experts = 8，Shared Experts = 8。

* **模型结构**

在 **Qwen1.5** 系列中，只有 32B 和 110B 的模型使用了 **GQA**。这一次，所有尺寸的模型都使用了 **GQA**，带来了推理加速和显存占用降低的优势。针对小模型，由于 embedding 参数量较大，**Qwen2** 使用了 tie embedding 的方法让输入和输出层共享参数，增加非 embedding 参数的占比。

上下文长度方面，所有的预训练模型均在 32K tokens 的数据上进行训练，并且其在 128K tokens 时依然能在 PPL 评测中取得不错的表现。而在使&#x7528;**`YARN`**&#x8FD9;类方法时，Qwen2-7B-Instruct 和 Qwen2-72B-Instruct 均实现了长达 128K tokens 上下文长度的支持。

**tokenizer**

**Qwen2** 使用 BPE 分词器，这种分词器在 **Qwen1.5** 中已经使用过，BPE 具有高编码效率，这得益于它比其他替代方案有更好的压缩比率。这种高效的编码方式对于处理大型语言模型尤其重要，它可以减少模型训练和推理时的内存和计算需求。

tokenizer 的设计支持多语言能力，这对于 **Qwen2** 模型来说是一个关键特性，因为它需要能够处理包括英语、中文、西班牙语、法语、德语、阿拉伯语、俄语、韩语、日语、泰语和越南语等在内的约 30 种语言。所有模型使用了一个共有的词汇表，包&#x542B;**`151,643`**&#x4E2A;常规 tokens &#x548C;**`3`**&#x4E2A; control tokens：**`<|endoftext|>`**, **`<|im_start|>`**, **`<|im_end|>`**。这种统一的词汇表有助于保持不同模型规模之间的一致性。

**Qwen2 Dense**

**Qwen2 Dense** 基于 Transformer 架构， 与前一代 **Qwen** 模型相比，**Qwen2 Dense** 在多个方面进行了改进，以提高模型的性能和效率：

> 1. **分组查询注意力 GQA**：**Qwen2** 采用&#x4E86;**`GQA`**&#x66FF;代了 MHA。GQA 通过优化 KV Cache 使用，在推理过程中显著提高了吞吐量
>
> 2. **DCA** **与 YARN**：为了扩大模型的上下文窗口，**Qwen2** 实现了双块注意力机&#x5236;**`DCA`**，将长序列分割成可管理的长度块，此&#x5916;**`YARN`**&#x7528;于重新调整注意力权重，以更好地处理不同长度的序列
>
> 3. **其他技术**：**Qwen2** 模型还使用&#x4E86;**`SwiGLU`**&#x4F5C;为激活函数，**`RoPE`**&#x4F5C;为位置编码，**`QKV bias`**&#x6765;改善注意力机制，以&#x53CA;**`RMSNorm`**&#x548C;**`Pre-Norm`**&#x6280;术来提高训练稳定性

**双块注意力** **DCA**（**D**ual **C**hunk **A**ttention）

**DCA** 是一种用于自然语言处理任务的注意力机制，旨在高效处理长序列数据。**DCA** 通过将输入序列分块来减少计算复杂度，同时保留全局信息和局部信息。**DCA** 通常分为两个阶段：局部注意力和全局注意力。

**局部注意力阶段**：输入序列被分成若干个块，每个块内的元素之间进行注意力计算。这种方式可以有效减少计算复杂度，因为注意力计算只在较小的块内进行。

**全局注意力阶段**：每个块的表示被用来计算全局注意力，这个表示可以是该块内所有元素的平均值或最大值。然后，这些块表示之间进行注意力计算，从而捕捉全局信息。

**例**：假设有一个长度为 12 的输入序列：

$$\text{Input} = [x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_{10}, x_{11}, x_{12}]\\$$

将这个序列分成3个块，每个块包含4个元素：

$$\text{Chunk}_1 = [x_1, x_2, x_3, x_4] , \text{Chunk}_2 = [x_5, x_6, x_7, x_8], \text{Chunk}_3 = [x_9, x_{10}, x_{11}, x_{12}]$$

**局部注意力**：在每个块内计算注意力。对于第一个块：

$$\text{Attention}(x_i, x_j) = \frac{\exp(x_i \cdot x_j)}{\sum_{k=1}^{4} \exp(x_i \cdot x_k)}\\$$

这里 $$\cdot$$表示点积操作。然后对每个块进行类似的操作，得到每个块内的注意力表示。

**全局注意力**：对于每个块，计算一个全局表示。假设使用平均值作为块的表示：

$$R_1 = \frac{1}{4} \sum_{i=1}^{4} x_i, R_2 = \frac{1}{4} \sum_{i=5}^{8} x_i, R_3 = \frac{1}{4} \sum_{i=9}^{12} x_i \\$$

然后在这些块表示之间计算全局注意力：

$$\text{Global Attention}(C_i, C_j) = \frac{\exp(C_i \cdot C_j)}{\sum_{k=1}^{3} \exp(C_i \cdot C_k)} \\$$

这里$$C_i$$表示第$$i$$个块的表示。

**DCA** 通过分块的方式，将长序列的注意力计算分解为局部注意力和全局注意力两部分，从而有效减少计算复杂度，同时保留全局和局部信息。这种方法特别适用于处理长序列数据的任务，如长文本的自然语言处理。

**YARN**（**Y**et **A**nother **R**escaling **M**ethod）

**YARN** 用于重新调整注意力权重，以改善模型对不同长度序列的处理能力。它通过重新调整注意力权重，使得模型能够更好地处理长序列和短序列，避免在处理长序列时注意力权重过于分散，从而提升模型的整体表现。

**YARN** 的核心思想是对注意力权重进行重新缩放，以便在处理不同长度的序列时，注意力机制能够更有效地聚焦于重要的信息。具体来说，**YARN** 通过以下步骤实现这一目标：

> 1. **计算注意力权重**：计算原始的注意力权重矩阵，一般通过点积注意力机制或其他注意力机制实现
>
> 2. **计算缩放因子**：根据序列的长度计算缩放因子，这个因子通常是与序列长度相关的函数，用于调整注意力权重的分布
>
> 3. **应用缩放因子**：将计算得到的缩放因子应用到原始的注意力权重矩阵上，重新调整权重的分布
>
> 4. **归一化**：对调整后的注意力权重进行归一化处理，确保权重的总和为 1

**例**：假设有一个序列长度为 5 的输入序列，模型计算得到的原始注意力权重矩阵为：

$$A = \begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.2 & 0.2 \\ 0.2 & 0.1 & 0.2 & 0.3 & 0.2 \\ 0.3 & 0.2 & 0.1 & 0.2 & 0.2 \\ 0.2 & 0.3 & 0.2 & 0.1 & 0.2 \\ 0.2 & 0.2 & 0.2 & 0.2 & 0.2 \\ \end{bmatrix}$$

假设使用一个简单的缩放因子$$\alpha = \frac{1}{\sqrt{L}}$$，其中$$L$$是序列的长度。在这个例子中，序列长度$$L=5$$，因此缩放因子$$\alpha = \frac{1}{\sqrt{5}} \approx 0.447$$。

将缩放因子应用到原始注意力权重矩阵上：

$$A' = \alpha A = 0.447 \cdot \begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.2 & 0.2 \\ 0.2 & 0.1 & 0.2 & 0.3 & 0.2 \\ 0.3 & 0.2 & 0.1 & 0.2 & 0.2 \\ 0.2 & 0.3 & 0.2 & 0.1 & 0.2 \\ 0.2 & 0.2 & 0.2 & 0.2 & 0.2 \\ \end{bmatrix} = \begin{bmatrix} 0.0447 & 0.0894 & 0.1341 & 0.0894 & 0.0894 \\ 0.0894 & 0.0447 & 0.0894 & 0.1341 & 0.0894 \\ 0.1341 & 0.0894 & 0.0447 & 0.0894 & 0.0894 \\ 0.0894 & 0.1341 & 0.0894 & 0.0447 & 0.0894 \\ 0.0894 & 0.0894 & 0.0894 & 0.0894 & 0.0894 \\ \end{bmatrix}$$

最后，对调整后的注意力权重矩阵进行归一化处理，使得每一行的权重总和为 1：

$$A'' = \text{softmax}(A')$$

通过这个过程，**YARN** 机制能够有效地调整注意力权重，使得模型在处理不同长度的序列时，能够更好地聚焦于重要的信息，从而提升模型的性能。

**Qwen2 MoE**

**Qwen2 MoE** 模型的架构与 **Qwen1.5-MoE-A2.7B** 非常相似。作为原始 **FFN** 的替代，**MoE FFN** 由$$n$$个单独的 **FFN** 组成，每个 **FFN** 充当一个专家。根据门控网络$$G$$分配的概率，每个 token 被定向到特定的 expert $$E_i$$进行计算：

$$p=\text{softmax}\left( G\left( x \right) \right)$$

$$y=\sum_{i\in\text{top}_k\left( p \right)}E_i\left( x \right)$$

下面是 **Qwen2 MoE** 和其它 **MoE** 模型的主要区别：

**专家粒度**： **MoE** 与 **Dense** 之间的一个关键结构差异在于，**MoE** 层包含多个 **FFN**，每个 **FFN** 作为一个独立的专家。因此从 **Dense** 架构过渡到 **MoE** 架构的一个直接策略是将每个专家的参数设置为原始 **Dense** 模型中单个 **FFN** 的参数。与这个策略不同地是 **Qwen2** 的模型采用了细粒度专家，同时创建规模更小的专家并激活更多的专家。在专家参数总数和激活参数数相等的情况下，细粒度专家提供了更丰富的专家组合。通过利用这些细粒度专家，**Qwen2 MoE** 促进了更多样化和动态的专家利用，从而提高了整体性能和适应性。

**专家路由**：设计专家路由机制对于提高 **MoE** 模型的性能至关重要。最近的工作出现了一个明显的趋势，即在 **MoE** 层中整合共享专家和特定路由专家。**Qwen2** 采用了这种方法，因为它便于在各种任务中应用共享专家，同时保留其他专家用于特定的路由场景选择性使用。引入共享专家和专用专家提供了一种更适应性强、效率更高的方法来开发 **MoE** 路由机制。

**专家初始化**：给定指定专家的中间维度大小$$h_E$$、专家数量$$n$$以及原始 **FFN** 中间维度大小$$h_{FFN}$$，**FFN** 将复制&#x20;

$$\lceil n \times h_E / h_{FFN} \rceil$$次。这种复制确保了与指定专家数量的兼容性，同时适应任何任意的专家中间维度大小。为了促进每个 **FFN** 副本内部的多样性，参数会沿着中间维度进行洗牌。这保证了每个细粒度专家即使在不同的 **FFN** 副本中也展现出独特的特征。随后，从 **FFN&#x20;**&#x526F;本中提取这些专家，并丢弃剩余的维度。对于每个细粒度专家，其 50% 的参数将被随机重新初始化。这个过程在专家初始化中引入了额外的随机性，可能增强了模型在训练期间的探索能力。

* **预训练**

在 **Qwen2** 的预训练中，主要重点放在完善数据集和研究有效处理扩展上下文长度的方法上：

**预训练数据**

**Qwen2** 模型的预训练涉及到开发一个新的、大规模的、高质量的多语言数据集。这个数据集相比之前 **Qwen** 和**Qwen1.5** 使用的数据集有了改进，增强了预训练数据的规模、质量和多样性，主要在以下几个关键领域：

**质量增强：**&#x8FC7;滤算法已经通过额外的启发式和基于模型的方法进行了改进，包括使用 **Qwen** 来筛选出低质量的数据。此外，这些模型还被用来合成高质量的预训练数据。

**数据扩展：Qwen2** 收集了更大量的高质量代码、数学和多语言数据，增强了模型在这些领域的能力。这个新数据集支持大约 30 种语言，如英语、中文、法语、德语等。

**分布改进**：为了确保模型学习到类似于人类学习的分布，**Qwen2** 在小模型上进行实验，以优化来自不同来源和领域的数据混合。基于这些增强，预训练数据从 **Qwen1.5** 的 3 万亿个 token 扩展到了 7 万亿个 token。尝试进一步放宽质量阈值，产生了一个 12 万亿个 token 的数据集。但在这个数据集上训练的模型并没有显示出比 7 万亿个 token 更好的效果。除了 **Qwen2-0.5B** 之外，所有的 **Qwen2 Dense** 模型都是在这个超过 7 万亿个 token 的大规模数据集上进行预训练的。**Qwen2-0.5B** 是在使用 12 万亿个 token 的数据集上进行预训练的。**MoE** 模型根据提升再利用的原则，另外接收了 4.5 万亿个 token 的预训练。与之前的 **Qwen** 类似，高质量的多任务指令数据被整合到 **Qwen2** 的预训练过程中，以增强上下文学习和指令跟随能力。

**长文本训练**

为了增强 **Qwen2** 处理长文本的能力，在预训练的最后阶段，**Qwen2** 将上下文长度从 4,096 个 token 增加到了32,768 个token。这一扩展通过引入大量高质量的长文本数据得到了补充。为了配合这些增强，**Qwen2** 将 **RoPE** 的频率从 10,000 调整到了 1,000,000，以优化长上下文场景下的性能。为了充分利用模型的外推潜力，**Qwen2** 采用了 **YARN** 机制和双块注意力机制 **DCA**。这些策略使模型能够处理长达 131,072 个 token 的序列，同时保持高性能。

* **后训练**

在进行了广泛的大规模预训练之后，**Qwen2** 进行了后训练阶段。这一过程对于提高其在包括编程、数学、逻辑推理、指令遵循和多语言理解在内的效果至关重要。此外，它确保了模型的生成与人类价值观相符合。与严重依赖大量人类监督的传统方法不同，**Qwen2** 的方法侧重于可扩展的对齐，最小化人类注释。具体来说，**Qwen2** 研究了获取高质量指令和偏好数据的方法，用于 **SFT** 和 **RLHF**，旨在在最大化数据质量和可靠性的同时，最小化对人类标记的需求。

**后训练数据**

后训练数据主要由两个组成部分构成：问题-答案数据集$$D = \left\{ (x_i, y_i) \right\}$$和偏好数据集$$P = \{(x_i, y_i^{+}, y_i^{-})\}$$，其中$$x_i$$代表指令，$$y_i$$代表回答，$$y_i^+$$和$$y_i^-$$是针对$$x_i$$的两个响应，前者表示回答更好。数据集$$D$$用于 SFT，而数据集 $$P$$用于RLHF。

训练数据的构建涉及两步过程：协作数据注释和自动化数据合成。首先，从大规模指令语料库中提取数据，从而得到广泛和多样化的高质量指令集合。这些指令被系统性地增强，以包含更大的复杂性。通过人工注释，获得目标响应$$y_i$$及其正面和负面的回答$$(y^+_i, y^−_i)$$。随后，采用多种自动化对齐策略来合成大量人工标注的数据，涵盖代码、数学、指令遵循、创作、角色扮演和安全等领域。

**协作数据注释**

**自动提取**：首先应&#x7528;**`InsTag`**，一个开放式细粒度标记器，从小规模指令数据集中提取底层本体

**指令选择**：每个带有标签的指令都根据标签多样性、语义丰富性、复杂性和意图完整性进行评估，基于这些标准，选择一组代表性的指令

**指令演化**：为了丰富指令数据集，采用自我演化策略，促使 **Qwen** 模型向现有指令添加约束或要求，从而增加它们的复杂性，并确保数据集中不同难度级别的多样性

**人工注释**：使用不同的生成策略和不同规模的 **Qwen** 模型获得对指令的多种响应，注释者根据他们的偏好对这些响应进行排名，确保最佳响应满足既定标准，产生示范和偏好数据

**自动化数据合成**

**拒绝采样**：对于具有明确答案的数学类任务，应用拒绝采样来提高数据质量，大模型为每个指令生成多个响应，即推理路径，那些得出正确回答并且被模型认为是合理的路径将被保留，作为示范数据。通过对比正确和错误的路径来生成偏好数据。

**执行反馈**：对于编程任务，LLM 被用来生成解决方案和相关的测试用例。通过编译和执行这些解决方案来对抗测试用例，从而评估这些解决方案的有效性，由此创建示范数据和偏好数据。这种方法也适用于评估指令遵循。对于每个带有约束的指令，例如`长度限制`，LLM 被指派生成一个 Python 验证函数，以确保响应符合指令要求。

**数据再利用**：在文学写作任务中创造好的回答对于没有专门训练的注释者来说是一个挑战。为了解决这个问题，**Qwen2** 收集了来自公共领域的高质量文学作品，并利用大模型来制定不同详细程度的指令。这些指令与原始作品配对，作为示范数据。例如，`为了生成生动有趣的角色扮演数据，从维基百科这样的知识库中获取详细的人物档案，并指导大模型生成相应的指令和响应`。这个过程类似于阅读理解任务，确保了人物档案的完整性得以保持。

**SFT**

**Qwen2** 收集了一个包含超过 50 万个示例的广泛指令数据集，涵盖了指令遵循、编程、数学、逻辑推理、角色扮演、多语言能力和安全等技能。对模型进行两轮 **SFT**，序列长度为 32,768。为了优化学习过程，学习率从 7e-6 逐渐降低到 7e-7。为了解决过拟合问题，应用了 0.1 的权重衰减，并将梯度裁剪在最大值 1.0。

**RLHF**

RLHF 训练方案包括两个连续的阶段：离线训练和在线训练。在离线训练阶段，使用预编译的偏好数据集$$P$$，通过**DPO** 来最大化$$y_i^+$$和$$y_i^-$$之间的差异。在在线训练阶段，模型利用 reward model 进行实时反馈，迭代地优化其性能。具体来说，从当前的策略模型中采样多个响应，Reward Model 选择最优和最差的响应，形成 **DPO** 的偏 pair 对。此外，**Qwen2** 采用了 [**Online Merging Optimizer**](https://arxiv.org/pdf/2405.17931) 来减轻对齐成本。

### 4.4.4 Qwen2.5

与之前的版本相比，**Qwen2.5&#x20;**&#x5728;预训练和后训练阶段都有显著改进。预训练数据从 7T tokens 扩展&#x4E3A;**`18T`&#x20;**&#x74;okens，为常识、专业知识、推理能力提供了坚实的基础。后训练阶段包括在超&#x8FC7;**`1M`**&#x6570;据上进行的 SFT 和多阶段 RL：**`offline DPO`**&#x548C;**`online GRPO`**。后训练显著增强了人类偏好，改善了长文本生成、结构数据分析和指令跟踪能力。

**Qwen2.5** 开源了各个规模的 Base 和 Instruct 模型，包括 **`0.5B`**、**`1.5B`**、**`3B`**、**`7B`**、**`14B`**、**`32B`**、**`72B`**，以及量化版本。闭源模型目前包括两个 **MoE** 变体：**`Qwen2.5-Turbo`** 和 **`Qwen2.5-Plus`**。**Qwen2.5** 在语言理解、推理、数学、代码、人类偏好对齐等广泛 benchmark 上展现了顶级性能。**Qwen2.5-72B-Instruct** 优于许多其他开源和闭源模型。**Qwen2.5-Turbo** 和 **Qwen2.5-Plus** 的成本也与 **GPT-4o-mini** 和 **GPT-4o** 相比具有竞争力。

**Qwen2.5** 在支持训练领域专家模型的训练中发挥了重要作用，包&#x62EC;**`Qwen2.5-Math`**、**`Qwen2.5-Coder`**、**`QwQ`** 和多模态模&#x578B;**`Qwen2.5-VL`**。总的来说，**Qwen2.5** 有几个关键特征：

> 1. **模型规模**：相比上一代 **Qwen2**，新增了 3B、14B、32B 模型。Qwen2.5-Turbo 和 Qwen2.5-Plus 在准确率、延迟、成本之间提供了很好的平衡
>
> 2. **数据规模**：预训练数据增加到了 18T tokens，关注知识、代码、数学。分阶段进行预训练以允许不同数据混合之间的过渡。后训练数据超过 1M 样本，涵盖 SFT、DPO、GRPO
>
> 3. **模型使用**：相比上一代，生成长度从 2K 提升至 8K。更好地支持结构化输入和输出，更易于使用工具。此外，Qwen2.5-Turbo 支持 1M 的上下文长度

对于 Dense 模型，保持 Transformer Decoder 架构。其他关键组件也与前几代相同：用于高效 KV cache &#x7684;**`GQA`**、**`SwiGLU`**&#x6FC0;活函数、**`RoPE`**&#x4F4D;置编码、注意力机制&#x4E2D;**`QKV bias`**、**`RMSNorm`**&#x5F52;一化。各个参数规模的参数如右表：

![]()

在 Dense 模型架构的基础上，通过用 **MoE** 层替换标准 **FFN** 层来扩展到 **MoE** 架构。与 **Qwen1.5-MoE** 相同，进行细粒度专家划分 [**DeepSeekMoE**](https://arxiv.org/pdf/2401.06066) 和共享专家路由 [**DeepSpeed-MoE**](https://proceedings.mlr.press/v162/rajbhandari22a/rajbhandari22a.pdf)。提供了下游任务中模型性能的实质提升。

Tokenizer &#x4E3A;**`BBPE`**，词表大&#x5C0F;**`151,643`**。与之前版本相比，control tokens 的数量从 3 增加&#x5230;**`22`**，加入&#x4E86;**`2`**&#x4E2A;用于工具调用的新 token。

* **预训练**

预训练阶段由多个关键部分组成：

> 1. 通过复杂的过滤和打分机制，结合数据混合策略，仔细收集高质量预训练数据
>
> 2. 对超参数进行了大量研究，以有效进行各个规模模型的训练
>
> 3. 加入专门的长上下文预训练来增强模型对长文的能力

**数据**

与上一代相比，**Qwen2.5** 预训练数据质量的提升：

> 1. **数据过滤**：数据质量评估和过滤是 pipeline 的关键部分。使用 **Qwen2-Instruct** 作为过滤器，对数据进行全面、多维度地评估和打分。由于 **Qwen2** 在更大的多语言语料库上进行了训练，因此过滤相比 **Qwen2** 有了显著提升，提高了高质量数据的保留，有效过滤了跨多个语言的低质数据。
>
> 2. **数学和代码数据**：结合了 **Qwen2.5-Math** 和 **Qwen2.5-Coder** 的训练数据。这种数据集成策略被证明非常有效。
>
> 3. **合成数据**：同时使用 **Qwen2-72B-Instruct** 和 **Qwen2-Math-72B-Instruct** 进行高质量合成数据生成。通过专有的通用 **RM** 和 **Qwen2-Math-RM-72B** 进行严格过滤。
>
> 4. **数据混合**：使用 **Qwen2-Instruct** 对不同领域的内容进行分类和平衡。分析显示，电商、社交媒体、娱乐等领域数据过多，这些数据通常包含重复、基于模版、机器生成等内容。相反，科技、学术研究等领域数据虽然包含更高质量信息，但是占比较低。通过下采样和上采样，进行更优的数据混合。

通过这些方法，得到了更大更高质量的 18T 数据集。

**超参数**

基于预训练数据进行超参数 **scaling law** 研究。在先前的给定计算资源下的最优模型规模的 **scaling law** 的基础上，进行跨模型架构的最优超参数识别。对于不同大小的 **Dense** 模型和 **MoE** 模型来确定 batch size 和学习率等关键超参数。

通过大量实验系统研究了模型架构与最优超参数之间的关系。分析了最优**学习率**$$μ_\text{opt}$$和 **batch size** $$B_\text{opt}$$如何随着模型规模 N 和预训练数据规模 D 的变化而变化。实验包&#x62EC;**`44M`**&#x5230;**`14B`**&#x89C4;模的 **Dense** 模型以&#x53CA;**`44M`**&#x5230;**`1B`**&#x89C4;模的 **MoE** 模型，&#x4ECE;**`0.8B`**&#x5230;**`600B`** tokens 数据集上训练。利用最优超参数预测，将最终的 loss 建模为模型架构和训练数据规模的函数。

此外，利用 **scaling law** 来预测和比较不同规模 **MoE** 模型及其 **Dense** 模型的表现，指导 **MoE** 模型的超参数配置，使得能够通过调整激活参数和总参数来实现与特定 **Dense** 模型变体的性能等价。

**长上下文预训练**

两阶段训练方法：

初始采&#x7528;**`4,096`**&#x4E0A;下文长度，之后在预训练的最后阶段扩展&#x5230;**`32,768`**&#x4E0A;下文长度（除了 **Qwen2.5-Turbo**）。同时，利&#x7528;**`ABF`**&#x6280;术将 **RoPE** 的基频从 10,000 提高到 1,000,000。

对于 **Qwen2.5-Turbo**，进行 4 个阶段的渐进上下文长度扩展策略：**`32,768`**、**`65,536`**、**`131,072`**、**`262,144`**。RoPE 基频为 1,000,000。在每个阶段，仔细调整训练数据来涵盖 40% 序列满足当前最大长度，60% 为更短的序列。

采&#x7528;**`YARN`**&#x548C;**`DCA`**&#x6765;增强模型在推理过程中处理长序列的能力。通过这些创新，实现了模序列长度容量的 4 倍增长，**Qwen2.5-Turbo** 能够处理 1M tokens，其他模型能够处理 131,072 tokens。

* **后训练**

与 **Qwen2** 相比，**Qwen2.5** 在后训练设计上有两个改进：

1. **扩展 SFT 数据覆盖**

利用了百万级别的高质量 **SFT** 数据，数据扩展专门针对先前模型的不足领域，如长序列生成、数学问题、代码、指令跟随、结构化数据理解、逻辑推理、跨语言迁移、鲁棒的系统指令。

* **两阶段强化学习**

**Qwen2.5** 的 RL 过程分为两个阶段：**Offline RL** 和 **Online RL**：

**Offline RL**

关注 **RM** 较难评估的能力的开发，如推理、事实、指令跟随。通过精心构造和验证训练数据，确保 Offline RL 信号既是可学习的又是可靠的。

**Online RL**

利用 **RM** 检测输出质量细微差别的能力，包括 truthfulness、helpfulness、简明性、相关性、无害性、无偏性。

**SFT**

**Qwen2.5** 在 **SFT** 阶段主要进行了 9 个领域的关键改进：

**长序列生成**

**Qwen2.5** 上下文长度可达 8,192 个token，显著超过了通常的后训练响应长度。为弥补这一差距，开发了长响应数据集。通过回译技术从预训练语料库生成长文本数据的查询，施加输出长度限制，并使用**Qwen2** 过滤掉低质量的配对数据。

**数学**

引入了 **Qwen2.5-Math** 的链式思考数据，涵盖了多种查询来源，包括公共数据集、K-12 问题集合和合成问题。为确保高质量的推理，采用了拒绝采样方法，并结合奖励建模和标注答案进行指导，生成逐步推理过程。这种方法保证了推理过程的准确性和详细性。

**代码**

整合了 **Qwen2.5-Coder** 的指令调优数据，利用多语言代理生成近 40 种编程语言的高质量指令对。通过合成代码相关问答网站的新示例和收集 **GitHub** 上的算法代码片段来扩展数据集。采用多语言沙箱进行静态代码检查，并通过自动化单元测试确保代码质量与正确性。

**指令跟随**

实施了一个严格的基于代码的验证框架。大型语言模型生成指令及其对应的验证代码，并附带全面的单元测试以进行交叉验证。通过基于执行反馈的拒绝采样，精心筛选用于监督微调的训练数据，从而保证模型忠实遵循预期指令。

**结构数据理解**

开发了一个全面的结构化理解数据集，涵盖表格问答、事实验证、错误纠正和结构理解的传统任务和涉及结构化和半结构化数据的复杂任务。通过在模型响应中引入推理链，显著增强了从结构化数据中推断信息的能力，从而提升在这些多样化任务中的表现。

**逻辑推理**

引入了涵盖多个领域的 70,000 个新的查询，包括选择题、判断题和开放式问题。模型经过训练，能够系统地解决问题，采用多种推理方法，如演绎推理、归纳概括、类比推理、因果推理和统计推理。通过迭代精炼，系统地过滤掉包含错误答案或有缺陷推理过程的数据。

**翻译**

将高资源语言指令转换为多种低资源语言，生成候选回复。为确保回复的准确性和一致性，评估每个多语言响应与其原始版本之间的语义对齐。这保留了原始响应的逻辑结构和风格细节，从而在不同语言中保持其完整性和一致性。

**系统指令**

构建了数百个通用系统提示，以提高后训练中系统提示的多样性，并确保系统提示与对话之间的一致性。通过不同系统提示进行的评估表明，模型保持了良好的性能并减少了方差，显示出更强的鲁棒性。

**回复过滤**

采用了多种自动标注方法，包括专用的批评模型和多代理协作评分系统。响应经过严格评估，只有在所有评分系统中都被认为无瑕疵的响应才会被保留。这种方法确保输出内容维持最高质量标准。

最终，构建了一个包含超&#x8FC7;**`1M`**&#x6570;据的 **SFT** 数据集。&#x5728;**`32,768`**&#x5E8F;列长度下微调了 2 个 epochs。学习率从 7e−6 逐渐降低到 7e−7 。为了解决过拟合问题，应用了 0.1 的 weight decay 并进行最大值为 1.0 的梯度裁剪。

**Offline DPO**

与 **Online RL** 相比，**Offline RL** 能够预先准备训练数据，适用于存在标准答案单但 **RM** 难以评估的任务。

这里主要关注客观 query 领域，比如数学、代码、指令遵循、逻辑推理，在这些领域获得准确的 evaluation 可能很复杂。在 **SFT** 中，大规模应用执行反馈和答案匹配等策略来确保回复的质量。在 **Offline RL** 阶段重用这一 pipeline，使用 **SFT** 模型对一组新的 query 进行重新采样。

在 **DPO** 中，通过质量检查的回复为正例，未通过的为负例。通过人工和自动检查过程来进一步提高训练数据的可靠性和准确性，同时确保数据不仅是可学习的，也符合人类的期望。最终，构建了规模约&#x4E3A;**`150,000`** pairs 的训练数据。&#x7528;**`Online Merging Optimizer`**&#x8BAD;练了 1 个 epoch，学习率为 7e−7 。

**Online GRPO**

为了开发一个用于 **Online RL** 的强大 **RM**，采用了一套精心制定的标注标准。这套标准确保模型生成的回复不仅是高质量的，而且符合道德并且以用户为中心。数据标注具体标准如下：

> 1. **Truthfulness**：回复必须以事实准确性为基础，真实的反应提供的背景和指令。模型应避免生成错误或给出数据不支持的信息。
>
> 2. **Helpfulness**：模型回复应该是真正有用的，有效地解决用户的查询，同时提供积极、吸引人、有教育意义和相关的内容。它应该精确地遵循给定的说明，为用户提供价值。
>
> 3. **Conciseness**：回复应简明扼要，避免不必要的冗长。目标是清晰有效地传达信息，而不会用过多的细节压倒用户。
>
> 4. **Relevance**：回复的所有部分都应该与用户的查询、对话历史和助手的上下文直接相关。模型应调整其输出，以确保其完全符合用户的需求和期望。
>
> 5. **Harmlessness**：模型必须优先考虑用户安全，避免任何可能导致非法、不道德或有害行为的内容。它应该始终促进道德行为和负责任的沟通。
>
> 6. **Debiasing**：模型应产生无偏见的回复，包括但不限于性别、种族、国籍和政治。它应该平等、公平地对待所有话题，遵守广泛接受的道德和伦理标准。

用于训练 **RM** 的 query 来自两个不同的数据集：开源数据和具有更高复杂性的专有数据。

回复是由 **Qwen** 模型生成的，**Qwen** 模型来自 **SFT**、**DPO**、**RL** 等不同阶段的训练。为了引入多样性，用不同温度进行采样。偏好对是由人工和自动化标注过程构建的，**DPO** 训练数据也囊括&#x5230;**&#x20;RM&#x20;**&#x6570;据中。

**Online RL** 框架采用了 **GRPO**。用于训练 **RM** 的提问集和 **RL&#x20;**&#x8BAD;练的提问集是相同的。训练过程中提问的处理顺序由其回复评分的方差决定，该方差是由奖励模型评估得出的，回复方差较大的提问先被训练。对每个提问采&#x6837;**`8`**&#x4E2A;回复。

所有模型都使&#x7528;**`2048`**&#x7684;全局批量大小和每个阶&#x6BB5;**`2048`**&#x4E2A;样本进行训练，其中将一对提问和回复视为一个样本。

**长上下文微调**

为了进一步扩展 **Qwen2.5-Turbo** 的上下文长度，在后训练期间引入了更长的 **SFT** 数据，使得模型在长提问下与人类偏好保持一致。这里使用了两阶段的 SFT：

> 1. **第一阶段**：短指令微调，每个指令最&#x591A;**`32,768`** tokens
>
> 2. **第二阶段**：结合短指令（最&#x591A;**`32,768`** tokens）和长指令（最&#x591A;**`262,144`** tokens）

在 **RL** 阶段，使用类似其他 **Qwen2.5** 模型的训练策略，只关注短指令。这种设计的主要原因有：

1. **RL** 训练对于长下文来说成本很高

2. 目前缺乏能为长下文任务提供合适奖励信号的 **RM**

3. 仅在短指令上进行 **RL** 仍然可以显著增强模型与人类在长下文任务上的偏好一致性

### 4.4.5 Qwen3

* **模型结构**

Qwen3 系列包含：

> * 6 个 **Dense** 模型：**`Qwen3 - 0.6B`**、**`Qwen3 - 1.7B`**、**`Qwen3 - 4B`**、**`Qwen3 - 8B`**、**`Qwen3 - 14B`**&#x548C; **`Qwen3 - 32B`**
>
> * 2 个 **MoE** 模型：**`Qwen3 - 30B - A3B`**&#x548C;**`Qwen3 - 235B - A22B`**，旗舰模型 Qwen3 - 235B - A22B 共有 2350 亿个参数，其中激活参数为 220 亿个

**Qwen3 Dense 模型**的架构与 Qwen2.5 类似，包括使用分组查询注意力机制 GQA、门控线性单元 SwiGLU、旋转位置嵌入 RoPE，以及采用预归一化的均方根归一化 RMSNorm。

![]()

![]()

除此之外，Qwen3 还去除了 Qwen2 中使用的 QKV 偏差，并在注意力机制中引入了 QK 归一化 QK-Norm，以确保 Qwen3 的稳定训练。模型架构的关键信息见上表。

**Qwen3 MoE 模型**与 Qwen3 密集模型具有相同的基本架构。模型架构信息如右表：

![]()

Qwen3 MoE 沿用了 Qwen2.5 - MoE 的设计，并实施了细粒度的专家划分。Qwen3 混合专家模型共有 128 个专家，每个 token 会激活 8 个专家。但是与 Qwen2.5 - MoE 不同的一点是，Qwen3 - MoE 的设计中没有共享专家。除此之外，Qwen Team 还采用了全局批处理负载均衡损失来促进专家的专业化，提升了模型在下游任务中的性能。

![]()

* **预训练**

Qwen3 的数据集相比 Qwen2.5 有了显著扩展。Qwen2.5是&#x5728;**`18T`**&#x4E2A; token 上进行预训练的，而 Qwen3 使用的数据量几乎翻倍，达到&#x4E86;**`36T`**&#x4E2A; token，为了构建这个数据集，Qwen Team 不仅从网络上收集数据，还从 PDF 文档中提取信息。并且使用 Qwen2.5-VL 从这些文档中提取文本，并用 Qwen2.5 改进提取内容的质量。为了增加数学和代码数据的数量，利用 Qwen2.5-Math 和 Qwen2.5-Coder 这两个数学和代码领域的专家模型合成数据，合成了包括教科书、问答对以及代码片段等多种形式的数据。

| 语系    | 语种&方言                                                                                                                                                                                                                                                                                                                                                                 |
| ----- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 印欧语系  | 英语、法语、葡萄牙语、德语、罗马尼亚语、瑞典语、丹麦语、保加利亚语、俄语、捷克语、希腊语、乌克兰语、西班牙语、荷兰语、斯洛伐克语、克罗地亚语、波兰语、立陶宛语、挪威语（博克马尔语）、挪威尼诺斯克语、波斯语、斯洛文尼亚语、古吉拉特语、拉脱维亚语、意大利语、奥克语、尼泊尔语、马拉地语、白俄罗斯语、塞尔维亚语、卢森堡语、威尼斯语、阿萨姆语、威尔士语、西里西亚语、阿斯图里亚语、恰蒂斯加尔语、阿瓦德语、迈蒂利语、博杰普尔语、信德语、爱尔兰语、法罗语、印地语、旁遮普语、孟加拉语、奥里雅语、塔吉克语、东意第绪语、伦巴第语、利古里亚语、西西里语、弗留利语、撒丁岛语、加利西亚语、加泰罗尼亚语、冰岛语、托斯克语、阿尔巴尼亚语、林堡语、罗马尼亚语、达里语、南非荷兰语、马其顿语僧伽罗语、乌尔都语、马加希语、波斯尼亚语、亚美尼亚语 |
| 汉藏语系  | 中文（简体中文、繁体中文、粤语）、缅甸语                                                                                                                                                                                                                                                                                                                                                  |
| 亚非语系  | 阿拉伯语（标准语、内志语、黎凡特语、埃及语、摩洛哥语、美索不达米亚语、塔伊兹-阿德尼语、突尼斯语）、希伯来语、马耳他语                                                                                                                                                                                                                                                                                                           |
| 南岛语系  | 印度尼西亚语、马来语、他加禄语、宿务语、爪哇语、巽他语、米南加保语、巴厘岛语、班加语、邦阿西楠语、伊洛科语、瓦雷语（菲律宾）                                                                                                                                                                                                                                                                                                        |
| 德拉威语  | 泰米尔语、泰卢固语、卡纳达语、马拉雅拉姆语                                                                                                                                                                                                                                                                                                                                                 |
| 突厥语系  | 土耳其语、北阿塞拜疆语、北乌兹别克语、哈萨克语、巴什基尔语、鞑靼语                                                                                                                                                                                                                                                                                                                                     |
| 壮侗语系  | 泰语、老挝语                                                                                                                                                                                                                                                                                                                                                                |
| 乌拉尔语系 | 芬兰语、爱沙尼亚语、匈牙利语                                                                                                                                                                                                                                                                                                                                                        |
| 南亚语系  | 越南语、高棉语                                                                                                                                                                                                                                                                                                                                                               |
| 其他    | 日语、韩语、格鲁吉亚语、巴斯克语、海地语、帕皮阿门托语、卡布维尔迪亚努语、托克皮辛语、斯瓦希里语                                                                                                                                                                                                                                                                                                                      |

Qwen3 通过三阶段流程进行预训练：

1. **通用阶段 General Stage, S1**：所有 Qwen3 模型均使&#x7528;**`4096`**&#x7684;序列长度，&#x5728;**`30T`** token 上进行训练。在此阶段，模型已完成语言能力和通用世界知识的全面预训练，训练数据覆&#x76D6;**`119`**&#x79CD;语言和方言。

2. **推理阶段 Reasoning Stage, S2**：为进一步提升推理能力，通过增加STEM、编码、推理和合成数据的比例来优化此阶段的预训练语料库。模型使&#x7528;**`4096`**&#x7684;序列长度，在&#x7EA6;**`5T`**&#x9AD8;质量token上进一步预训练，同时在此阶段加速学习率衰减。

3. **长上下文阶段 Long Context Stage**：作者团队收集高质量长上下文语料库以扩展 Qwen3 模型的上下文长度。所有模型使&#x7528;**`32,768`**&#x7684;序列长度，在数百亿 token 上进行预训练。长上下文语料库中，**`75%`**&#x7684;文本长度&#x5728;**`16,384`**&#x81F3;**`32,768`** token之间，**`25%`**&#x7684;文本长度&#x5728;**`4,096`**&#x81F3;**`16,384`** token之间。继续延续 Qwen2.5 的做法，使&#x7528;**`ABF`**&#x5C06; **RoPE** 的基础频率&#x4ECE;**`10,000`**&#x63D0;高&#x81F3;**`1,000,000`**。同时引&#x5165;**`YARN`**&#x548C;双块注意&#x529B;**`DCA`**，以在推理时将序列长度处理能力提升四倍。

> 与 Qwen2.5 类似，Qwen3 基于上述三个预训练阶段开发了用于预测最优超参数的缩放定律，研究了模型架构、训练数据、训练阶段与最优训练超参数之间的关系，为每个模型设定了预测的最优学习率和批量大小策略

由于模型架构的改进、训练数据的增加以及更有效的训练方法，Qwen3 Dense 基础模型的整体性能与参数更多的 Qwen2.5 基础模型相当。例如，Qwen3-1.7B/4B/8B/14B/32B-Base 分别与 Qwen2.5-3B/7B/14B/32B/72B-Base 表现相当。特别是在 STEM、编码和推理等领域，Qwen3 Dense 基础模型的表现甚至超过了更大规模的 Qwen2.5 模型。而 Qwen3 MoE 在仅使用 10% 激活参数的情况下达到了与 Qwen2.5 Dense 基础模型相似的性能。

* **后训练**

Qwen3 的后训练包含两个核心目标：

> 1. **思维控制 Thinking Control**：涉及两种不同模式的集成，即 Non-Thinking 模式和 Thinking 模式，user 能够灵活选择模型是否进行推理，并通过为 Thinking 过程指定 token 来控制 Thinking 深度。（后续又进行了拆分）
>
> 2. **强到弱蒸馏 Strong-to-Weak Distillation**：精简和优化轻量级模型的后训练流程，通过利用大规模模型的知识，大幅降低了构建较小规模模型所需的计算成本和开发工作量

> **混合推理模型**，这个概念最早&#x662F;**`Claude3.7`**&#x63D0;出来的，**`Gemini2.5 Flash`**&#x6700;新也支持了。其实就是一个模型既可以推理，也可以不推理。主要就是解决，在简单问题，或者是对实效性要求较高的情况下，可以通过控制，不生成思考过程，在不怎么影响效果的情况下，更快生成回复。
>
> 之前基本上是没有太好的办法直接让推理模型不生成思考过程，只能训练，提示词基本上控制不了。而这次 Qwen3 使用的控制办法是，硬切换设&#x7F6E;**`enable_thinking`**&#x4E3A;**`True`**&#x6216;&#x8005;**`False`**，当为 True 时，还可以二次软切换，通过文本后面&#x52A0;**`/no_think`**&#x6216;&#x8005;**`/think`**&#x6765;控制。
>
> 这里 Qwen Team 也给出了建议的参数配置，即：
>
> 1. **Thinking 模式**：**`Temperature=0.6, TopP=0.95, TopK=20, MinP=0`**
>
> 2. **Non-Thinking 模式**：**`Temperature=0.7, TopP=0.8, TopK=20, MinP=0`**

![]()

Qwen3 系列的旗舰模型包含四阶段训练流程。前两个阶段培养模型的思考能力，后两个阶段则将强大的非思考功能集成到模型中。

> 将教师模型的 logits 输出直接蒸馏到轻量级学生模型中，能够在保持对推理过程细粒度控制的同时有效提升其性能。这种方法无需为每个小规模模型单独执行详尽的四阶段训练流程，不仅能通过更高的Pass@1分数，体现出更优的即时性能，还能通过改善的长序列探索能力增强模型的推理扩展性。另一方面，可以以更高的训练效率实现了这些提升，仅需四阶段训练方法$$\frac{1}{10}$$的 GPU 时长。

**Stage 1：长思维链冷启动**

在这个阶段，Qwen Team 整理了一个涵盖广泛类别的综合数据集，包括数学、代码、逻辑推理和通用 STEM 问题。数据集中的每个问题都配有经过验证的参考解答或基于代码的测试用例。这个数据也是长思维链训练冷启动阶段的基础。

数据集构建涉及严格的两阶段过滤流程：Query 过滤和 Response 过滤。在 Query 过滤阶段，使用 Qwen2.5-72B-Instruct 模型识别并移除不易验证的 Query，包括包含多个子问题或请求通用文本生成的 Query。而且还排除了Qwen2.5-72B-Instruct 不使用思维链推理即可正确回答的 Query，这可以防止模型依赖表面猜测，确保数据集中仅包含需要深度推理的复杂问题。同时使用 Qwen2.5-72B-Instruct 为每个 Query 标注领域标签，以保持数据集中各领域的均衡分布。

在保留验证集后，使用 QwQ-32B 模型为每个剩余 Query 生成 N 个候选 Response。当 QwQ-32B 持续无法生成正确解决方案时，人工标注员会手动评估 Response 的准确性。对于通过率（Pass@N）为正的 Query ，使用更严格的过滤标准移除 Response。随后从数据集中挑选子集，用于推理模式的初始冷启动训练，目标是在模型中灌输基础推理模式，而不过度强调即时推理性能。这种方法确保模型的潜力不受限制，从而在后续强化学习阶段具备更大的灵活性和提升空间。为有效实现这一目标，在准备阶段最好尽量减少训练样本数量和训练步骤。

**Stage 2：推理强化学习**

推理强化学习阶段使用的 Query - Verifier 对必须满足四个标准：

`未在冷启动阶段使用过`、`冷启动模型可从中学习`、`尽可能具有挑战性`、`覆盖广泛的子领域`

最终收集了总&#x5171;**`3,995`**&#x4E2A; Query - Verifier 对，并采&#x7528;**`GRPO`**&#x66F4;新模型参数。

> 使用大批次大小、每个 Query 进行高次数的展开模拟，以及通过离策略训练提高样本效率，均对训练过程有益。
>
> 通过控制模型的熵值稳步增加或保持稳定，对于维持训练的稳定性至关重要。因此在单次强化学习运行过程中，无需对超参数进行任何人工干预，即可实现训练奖励和验证性能的持续提升。

**Stage 3：思维模式融合**

思维模式融合阶段的目标是将 Non-Thinking 能力集成到先前训练的 Thinking 模型中。这种方法使开发人员能够管理和控制推理行为，同时降低为 Thinking 和 Non-Thinking 任务部署独立模型的成本和复杂性。这一阶段对推理强化学习模型进行持续监督微调，并设计聊天模板融合两种模式：

![]()

SFT数据融合了 Thinking 和 Non-Thinking 两类数据。

> 为确保第二阶段模型的性能不会因额外的 SFT 而受损，Thinking 数据通过第二阶段模型自身对第一阶段 Query 进行拒绝采样生成
>
> Non-Thinking 覆盖编码、数学、指令遵循、多语言任务、创意写作、问答和角色扮演等多样化任务

此外采用自动生成的数据来评估 Non-Thinking 数据的 Response 质量。为了提升低资源语言任务的性能，特别增加了翻译任务的比例。

针对 Thinking 和 Non-Thinking 的样本，分别在用户 Query 或系统消息中引&#x5165;**`/think`**&#x548C;**`/no_think`** token。这使模型能够根据用户输入选择相应的思考模式：对于 Non-Thinking 模式样本，Response 中会保留空的思考块。这确保了模型内部格式的一致性，可通过在聊天模板中拼接空思考块来阻止模型执行推理行为。默认情况下模型处于 Thinking 模式，因此添加部分未包&#x542B;**`/think`&#x20;**&#x74;oken 的用户 Query 作为训练样本。对于更复杂的多轮对话，在用户 Query 中随机插入多&#x4E2A;**`/think`**&#x548C;**`/no_think`** token，模型 Response 遵循最后一个遇到的 token。

思考模式融合的另一个优势在于，一旦模型学会在 Thinking 和 Non-Thinking 两种模式下的 Response，它自然会形成处理中间情况的能力——基于不完整的思考过程生成 Response。

> 当模型的思考内容长度达到用户定义的阈值时，会手动终止思考过程，并插入停止思考指令：**`Considering the limited time by the user, I have to give the solution based on the thinking directly now.\n.\n\n`**。插入该指令后，模型会基于截至该时刻积累的推理内容继续生成最终 Response。这种能力并非通过显式训练获得，而是应用思考模式融合后自然涌现的结果。

**Stage 4：通用强化学习**

用来提升模型在多样化场景中的能力和稳定性。Qwen Team 建立了一个复杂的奖励系统，覆盖20多个不同的任务，每个任务都有定制的评分标准。这些任务专门针对这些核心能力提升：

> * **指令遵循**：确保模型准确理解并遵循用户指令，包括与内容、格式、长度及结构化输出使用相关的要求，从而生成符合用户期望的 Response
>
> * **格式遵循**：除了显式指令外，还期望模型遵循特定的格式规范。例如，模型应通过切换思考模式与非思考模式，&#x5BF9;**`/think`**&#x548C;**`/no_think`** token 做出恰当 Response，并在最终输出中始终使用指定 token，&#x5982;**`<thinking>`**&#x548C;**`</thinking>`**&#x6765;分隔思考内容与回答内容
>
> * **偏好对齐**：对于开放式 Query ，偏好对齐侧重于提升模型的实用性、吸引力和风格适配性，最终提供更自然且令人满意的用户体验
>
> * **Agent 能力**：通过指定接口正确调用工具。在强化学习过程中，允许模型与真实环境执行反馈进行完整的多轮交互循环，从而提升其在长程决策任务中的性能和稳定性
>
> * **专业场景能力**：在更专业的场景中，针对特定上下文设计任务。例如在 RAG 中引入奖励信号引导模型生成准确且符合上下文的 Response，从而最大限度降低幻觉风险

> Qwen3 在工具调用上做了专门训练，并且 Qwen-Agent 支持 MCP。
>
> ![]()
>
> ![]()

为上述任务提供反馈时，我们使用了三种不同类型的奖励：

**基于规则的奖励**

基于规则的奖励对指令遵循和格式遵守等通用任务很有用，能够高精度评估模型输出的正确性，避免奖励破解等问题

**带参考回答的基于模型的奖励**

为每个 Query 提供一个参考回答，并提示 Qwen2.5-72B-Instruct 模型根据此参考答案对模型的响应进行评分。这种方法可以更灵活地处理各种任务，不需要严格的格式，从而避免了纯粹基于规则的奖励可能出现的误报

**不带参考回答的基于模型的奖励**

利用人类偏好数据，训练一个奖励模型来为模型的响应分配标量分数。这种方法不依赖于参考答案，可以处理更广泛的 Query，同时有效增强模型的参与度和实用性

**强到弱的蒸馏**

强弱蒸馏流程是为优化轻量级模型设计的，包含5个 Dense 模&#x578B;**`Qwen3-0.6B`**、**`Qwen3-1.7B`**、**`Qwen3-4B`**、**`Qwen3-8B`**、**`Qwen3-14B`**&#x548C;一个 MoE 模&#x578B;**`Qwen3-30B-A3B`**。在提升模型性能的同时，赋予了模型稳健的模式切换能力。蒸馏过程主要分为两个阶段：

> 1. **Off-policy Distillation**：初始将教师模型通&#x8FC7;**`/think`**&#x548C;**`/no_think`**&#x4E24;种模式生成的输出结果结合起来进行 Response 蒸馏。这可以让轻量级学生模型培养基础推理能力和不同 Thinking 模式的切换能力，为下一阶段的 On-policy 策略训练奠定基础
>
> 2. **On-policy Distillation**：学生模型生成用于微调的 On-policy 策略序列。首先对 Prompt 进行采样，然后学生模型&#x4EE5;**`/think`**&#x548C;**`/no_think`**&#x6A21;式生成 Response 。随后，通过将学生模型的 logits 与教师模&#x578B;**`Qwen3-32B`**&#x6216;**`Qwen3-235B-A22B`**&#x5BF9;齐，以最小化KL散度对学生模型进行微调

### 4.4.6 QwQ

## 4.5 DeepSeek 系列

> **论文**：[**DeepSeek-V1**](https://arxiv.org/pdf/2401.02954)  **[DeepSeek-V2](https://arxiv.org/pdf/2405.04434)  [DeepSeek-V3](https://arxiv.org/pdf/2412.19437)  [DeepSeekMoE](https://arxiv.org/pdf/2401.06066)**
>
> **代码**：[**DeepSeek-V1**](https://github.com/deepseek-ai/DeepSeek-LLM)  **[DeepSeek-V2](https://github.com/deepseek-ai/DeepSeek-V2)  [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3)**

### 4.5.1 DeepSeek-V1

**DeepSeek&#x20;**&#x6DF1;入研究了规模定律，并提出了自己独特的发现，这些发现有助于在两种流行的开源配&#x7F6E;**`7B`**&#x548C;**`67B`**&#x4E2D;扩展大型模型，并从长远的角度推进了开源语言模型的发展。

为了支持预训练阶段，**DeepSeek&#x20;**&#x5F00;发了一个包&#x542B;**`2T`**&#x4E2A; token 的数据集，并且还在不断扩大。作者还对**DeepSeek LLM Base&#x20;**&#x6A21;型进行了 **SFT** 和直接偏好优化 **DPO**，从而创建了 **DeepSeek** **Chat** 模型。评估结果表明，**`DeepSeek LLM 67B`**&#x5728;各种基准测试中超过了 LLaMA-2 70B，特别是在代码、数学和推理领域。此外，与 GPT-3.5 相比，**`DeepSeek LLM 67B Chat`**&#x8868;现出更优越的性能。

* **模型结构**

**微观设计**：大体上遵循了 LLaMA 的设计，采用了带&#x6709;**`RMSNorm`**&#x51FD;数&#x7684;**`Pre-Norm`**&#x7ED3;构，并使&#x7528;**`SwiGLU`**&#x4F5C;为前馈网络 FFN 的激活函数，其中间层维度为$\frac{8}{3}d_\text{model}$。使用旋转嵌&#x5165;**`RoPE`**&#x7528;于位置编码。为了优化推理成本，**`67B`**&#x6A21;型使用了分组查询注意&#x529B;**`GQA`**，而不是传统的多头注意力 MHA。与大多数使用 GQA 的工作不同，**DeepSeek LLM** **67B&#x20;**&#x6A21;型的参数扩展在网络深度上，而不是通常做法中拓宽 FFN 层的中间宽度，旨在获得更好的性能。

**宏观设计**：**DeepSeek LLM&#x20;**&#x4E0E;其他不同：

**`DeepSeek LLM 7B`**&#x662F;**`30`**&#x5C42;的网络

**`DeepSeek LLM 67B`**&#x662F;**`95`**&#x5C42;的网络

![]()

这些层次调整不仅保持了与其他开源模型的参数一致性，也有助于模型流水线划分，以优化训练和推理过程。这种设计策略在保证计算效率的同时，也提高了模型的表现力和灵活性，使其在处理复杂任务时具有优势。

* **预训练**

**数据**

**DeepSeek&#x20;**&#x4E3B;要目标是全面增强数据集的丰富性和多样性。为了实现这些目标，**DeepSeek&#x20;**&#x5C06;方法分为三个基本阶段：**重复数据删除**、**过滤**和**重新混合**。重复数据删除和重新混合阶段通过对唯一实例进行采样来确保数据的多样化表示。过滤阶段提高了信息密度，从而实现了更高效、更有效的模型训练。

**去重阶段**：**DeepSeek&#x20;**&#x91C7;用了一种激进的去重策略，扩大了去重的范围。

![]()

**DeepSeek&#x20;**&#x53D1;现对整个 Common Crawl 数据集进行去重处理比仅在一部分数据内进行去重要去除更多的重复内容。右表展示了通过对 91 个不同批次的数据进行全面去重，所删除的重复文档数量是只对一个批次数据进行去重时的四倍。

**过滤阶段**：专注于制定稳健的文档质量评估标准。这结合语言和语义评估的详细分析，从个体和全局角度提供数据质量视图。

**混合阶段**：调整方法以解决数据不平衡问题，重点是增加代表性不足的领域的存在。这一调整旨在实现更加平衡和包容的数据集，确保充分代表不同的观点和信息。

在分词器中，**DeepSeek&#x20;**&#x4F7F;用了基于 Huggingface 团队的 tokenizers 库&#x7684;**`BBPE`**&#x7B97;法。预分词被用于防止来自不同字符类别的标记的合并，如新行、标点符号和中文-日语-韩语（**`CJK`**）符号，类似于 GPT-2。并且将数字拆分为单个数字。将词汇表中的传统标记数量设置&#x4E3A;**`100000`**。分词器在大&#x7EA6;**`24GB`**&#x7684;多语言语料库上进行训练，并且在最终词汇表中增加&#x4E86;**`15`**&#x4E2A;特殊标记，使总大小达&#x5230;**`100015`**。为了确保训练期间的计算效率并为将来可能需要的任何其他特殊标记预留空间，将模型词汇表的大小配置&#x4E3A;**`102400`**&#x7528;于训练。

**基建**

**DeepSeek&#x20;**&#x4F7F;用一个高效且轻量级的训练框架，名&#x4E3A;**`HAI-LLM`**（萤火平台，高效且轻量的大模型训练工具），来训练和评估大型语言模型。**数据并行**、**张量并行**、**序列并行**和**1F1B流水线并行**等并行策略都集成到了这个框架中。还利&#x7528;**`Flash Attention`**&#x6280;术来提高硬件利用率。**`ZeRO-1`**&#x88AB;用于分区状态的优化，以减少数据并行级别的通信开销。努力使计算和通信重叠，以最小化额外的等待开销，包括最后一个微批的反向传播过程和 ZeRO-1 中&#x7684;**`reduce-scatter`**&#x64CD;作，以及序列并行中&#x7684;**`GEMM`**&#x8BA1;算&#x548C;**`all-gather`**/**`reduce-scatter`**。

一些层操作被融合在一起以加速训练，包括 LayerNorm、GEMM 和 Adam 更新。为了提高模型训练的稳定性，使&#x7528;**`BF16`**&#x7CBE;度训练模型，但在计算梯度时使&#x7528;**`FP32`**&#x7CBE;度。还执行就地交叉熵以减少 GPU 内存消耗，即：在交叉熵 CUDA 内核中实时将 BF16 logits 转换为 FP32 精度，而不是在 HBM 中转换，计算相应的 BF16 梯度，并用其梯度覆盖 logits。

模型权重和优化器状态&#x6BCF;**`5`**&#x5206;钟异步保存一次，即使在偶尔出现硬件或网络故障的最坏情况下，也只会丢失不超过 5 分钟的训练进度。这些临时的模型检查点会定期清理，以避免占用过多的存储空间。还支持从不同的 **3D** 并行配置中恢复训练，以应对计算集群负载的动态变化。对于评估，在生成任务中采&#x7528;**`vLLM`**，在非生成任务中采用连续 batch 处理，以避免手动调整 batch 大小和减少 tokens 填充。

* **后训练**

收集了&#x7EA6;**`1.5M`**&#x6761;英语和中文指令数据实例，涵盖了广泛的帮助和无害主题。有用数据包&#x542B;**`1.2M`**&#x4E2A;实例，其中一般语言任务的分布&#x4E3A;**`31.2%`**，数学问题的分布&#x4E3A;**`46.6%`**，编程练习的分布&#x4E3A;**`22.2%`**。安全数据包&#x542B;**`300K`**&#x4E2A;实例，涵盖各种敏感主题。对齐流程包含两个阶段：

**监督微调**

对 **7B** 模型进行了 4 个 epoch 的微调，但对 **67B** 模型只进行了 2 个 epoch 的微调，因为 **67B** 模型存在严重的过拟合问题。**`GSM8K`**&#x548C;**`HumanEval`**&#x5728; **7B** 模型上得到了持续改进，而 **67B** 模型很快达到了上限。**7B&#x20;**&#x548C; **67B** 模型的学习率分别&#x4E3A;**`1e-5`**&#x548C;**`5e-6`**。除了监控基准准确度外，还评估了微调过程中聊天模型的重复率。总共收集&#x4E86;**`3868`**&#x4E2A;中文和英文提示，并确定了生成响应中无法终止并无限重复文本序列的比例。观察到重复率随着数学 **SFT** 数据的数量增加而上升，这可以归因于数学 **SFT** 数据偶尔包含类似的推理模式。因此，较弱的模型难以掌握这种推理模式，导致重复性响应。为了解决这个问题，尝试了两阶段微调和 **DPO**，这两种方法都可以保持基准分数并显著减少重复。

**直接偏好优化 DPO**

为了进一步提高模型的能力，**DeepSeek&#x20;**&#x4F7F;用了直接偏好优化算法，该算法被证明是一种简单而有效的 LLM 对齐方法。根据有用性和无害性构建了 **DPO** 训练所需的偏好数据。对于有用性数据，收集了多语言提示，涵盖了创意写作、问答、指令跟随等类别。然后，使用 **DeepSeek Chat** 模型作为候选响应生成响应。类似的操作也应用于无害性偏好数据构建。**DeepSeek** 进行了一个 DPO epoch 的训练，学习率&#x4E3A;**`5e-6`**，batch\_size &#x4E3A;**`512`**，使用了学习率预热和余弦学习率调度器。**DeepSeek** 发现 DPO 可以增强模型的开放式生成技能，同时在标准基准测试中的性能差异很小。

* **缩放定律**

缩放定律表明，随着计算预算$C$、模型规模$N$和数据规模$D$的增加，模型性能可以得到可预测的改善。当模型规模由模型参数表示，数据规模由 token 数表示时，计算预算$C$可以近似为$C = 6ND$。

为了降低实验成本和拟合难度，**DeepSeek** 采用&#x4E86;**`Chinchilla`**&#x4E2D;&#x7684;**`IsoFLOP profile`**&#x65B9;法来拟合缩放曲线。为了更准确地表示模型规模，使用一种新的模型规模表示方法，即非嵌入 FLOPs / token **`M`**，替换了先前使用的模型参&#x6570;**`N`**，并将近似计算预算公式$C = 6ND$替换为更精确的$C = MD$。实验结果提供了关于最佳模型/数据扩展分配策略和性能预测的见解，并准确地预测了 **DeepSeek LLM 7B&#x20;**&#x548C; **67B** 模型的预期性能。

**DeepSeek** 在缩放定律方面的贡献和发现可以总结如下：

> 1. 建立了超参数的缩放定律，为确定最佳超参数提供了一个经验框架
>
> 2. 采用非嵌入 FLOPs / token M 来表示模型规模，以及更精确的最佳模型/数据扩展分配策略和大规模模型的泛化损失的更好预测
>
> 3. 提出了一个基于计算预算和数据规模的模型扩展策略，该策略在计算预算有限的情况下提高了模型的泛化能力
>
> 4. 证明了缩放定律的普遍适用性，并预测了未来更大规模模型的性能
>
> 5. 持续关注数据质量和其对缩放定律的影响，并进行更深入的分析

**超参数的缩放定律**

**DeepSeek** 最初在计算预算&#x4E3A;**`1e17`**&#x7684;小规模实验上进行了 batch 大小和学习率的网格搜索，并使&#x7528;**`177M FLOPs/token`**&#x7684;特定模型大小，结果如右图(a)所示。从图中可以看出，泛化误差在 batch 大小和学习率的各种选择范围内保持稳定。这表明在相对较宽的参数空间内可以实现接近最优的性能。

![图1  批大小和学习率在1e17和1e20的FLOP上进行实验]()

然后 **DeepSeek** 利用上述多步学习率调度器，通过重用第一阶段，有效地训练了不同 batch 大小、学习率和计算预算&#x4ECE;**`1e17`**&#x5230;**`2e19`**&#x7684;多个模型。考虑到参数空间中的冗余，将泛化误差不超过最小&#x503C;**`0.25%`**&#x7684;模型参数视为近最优超参数。然后我们拟合batch大小$B$和学习率$\eta$与计算预算$C$之间的关系。拟合结果如下图所示，表明**最优 batch 大小$B$随着计算预算$C$的增加而逐渐增加，而最优学习率$\eta$则随着计算预算$C$的增加而逐渐减小**。这与直观的模型扩展时 batch 大小和学习率的经验设置相一致。此外，所有近最优超参数都落在一个宽的带宽范围内，表明在此区间内选择近最优参数相对容易。拟合的最终学习率和 batch 大小公式如下：

![图2  Batch size 和学习率的缩放曲线]()





$$\eta_\text{opt} = 0.3118\cdot C^{−0.1250}$$

$$B_\text{opt} = 0.2920\cdot C^{0.3271}$$

然后在具&#x6709;**`1e20`**&#x8BA1;算预算的一系列模型上验证了上述公式，&#x5728;**`2.94B FLOPs/token`**&#x6A21;型大小上实验的结果上图(b)所示，这表明拟合的参数集中在最优参数空间中，并且 **DeepSeek LLM 7B** 和 **67B** 模型拟合的参数同样取得了良好的性能。

**模型和数据缩放评估**

在先前的工作中，模型规模通常由模型参数表示，其中非嵌入参数为$N_1$，完整参数为$N_2$。计算预算$C$与模型/数据规模之间的关系可以近似描述为$C=6ND$，这意味着可以使用$6N_1$或$6N_2$来近似模型规模。然而，由于$6N_1$和$6N_2$都没有考虑到注意力操作的计算开销，并且$6N_2$还包括词汇计算，这对模型的容量贡献较小，因此在某些设置下，两者都有显著的近似误差。

为了减小这些误差，**DeepSeek&#x20;**&#x5F15;入了一种新的模型规模表示：非嵌入 **FLOPs / token** **`M`**。$M$包括注意力操作的计算开销，但不考虑词汇计算。使用$M$表示模型规模时，计算预算$C$可以简化为$C=MD$。$6N_1, 6N_2$和$M$之间的具体差异如下：

$$6N_1 = 72 n_{\text{layer}} d_{\text{model}}^2$$

$$6N_2 = 72 n_{\text{layer}} d_{\text{model}}^2 + 6 n_{\text{vocab}} d_{\text{model}}$$

$$M = 72 n_{\text{layer}} d_{\text{model}}^2 + 12 n_{\text{layer}} d_{\text{model}} l_{\text{seq}}$$

![]()

其中，$n_{\text{layer}}$表示层数，$d_{\text{model}}$表示模型宽度，$n_{\text{vocab}}$是词汇量大小，而$l_{\text{seq}}$是序列长度。&#x20;

在评估不同规模模型之间的差异时，**DeepSeek&#x20;**&#x53D1;现$6N_1$和$6N_2$要么高估要么低估了不同规模模型的计算成本。这种差异在小型模型中尤为明显，差异率高&#x8FBE;**`50%`**。这种不准确性在拟合缩放曲线时会导致较大的统计误差。模型规模表示间的差异以及$6N_1$和$6N_2$与$M$之间的差异如上表所示。

在采用$M$表示模型规模后，目标可以更清晰地描述为：**给定计算预算$C=MD$，找到最优模型规模$M_\text{opt}$和数据规模$D_\text{opt}$，以最小化模型的泛化误差**。这个目标可以形式化为：

$$M_{\text{opt}}(C), D_{\text{opt}}(C) = \underset{M, D \text{ s.t. } C = MD}{\arg\min} L(N, D)$$

为了降低实验成本和拟合难度，**DeepSeek** 采用 **IsoFLOP profile** 方法来拟合缩放曲线，并选择&#x4E86;**`8`**&#x4E2A;不同的计算预算，范围&#x4ECE;**`1e17`**&#x5230;**`3e20`**，并为每个预算设计了大&#x7EA6;**`10`**&#x79CD;不同的模型/数据规模分配。每个预算的超参数$B$和$\eta$由之前的讨论确定，泛化误差在类似的独立验证集上计算，该验证集的分布与训练集类似，包&#x542B;**`100M`**&#x4E2A; token。

右图展示了 IsoFLOP 曲线和模型 / 数据缩放曲线，这些曲线是通过使用每个计算预算的最优模型 / 数据分配来拟合的。最优非嵌入 FLOPs / token $M_{\text{opt}}$和最优令牌数 $D_{\text{opt}}$的具体公式如下：

$$M_{\text{opt}} = M_{\text{base}}C^a$$

$$D_{\text{opt}} = D_{\text{base}}C^b$$

![图3  IsoFLOP 曲线和最佳模型 / 数据分配]()

其中，$M_{\text{base}} = 0.1715, \quad a = 0.5243, \quad D_{\text{base}} = 5.8316, \quad b = 0.4757 $

**不同数据的缩放定律**

在 **DeepSeek LLM** 的开发过程中，数据集经过多次迭代优化，调整了不同数据源的比例，同时提高了整体质量。这进一步分析了不同数据集对缩放定律的影响。

这里使用三种不同的数据集研究了缩放定律：**早期的内部数据**、**当前的内部数据**和 **OpenWebText2**（先前用于缩放定律研究的文本数据集）。内部数据评估表明，当前内部数据的数据质量高于早期内部数据。此外，由于其规模较小，OpenWebText2 的数据质量甚至超过了当前内部数据，这使其能够进行更加细致的处理。

分析得出这三个数据集之间的最优模型/数据扩展分配策略与数据质量一致。如右表所示，随着数据质量的提高，模型扩展指数$a$逐渐增加，而数据扩展指数$b$逐渐减少，这表明**计算预算的增加应更多地分配给模型而不是数据**。这一发现解释了早期规模定律研究中观察到的最优模型/数据扩展分配的显著差异。

![]()

对于这一发现的一个直观猜测是，高质量的数据通常意味着逻辑清晰和经过充分训练后预测难度较低。因此，在增加计算预算时，扩展模型大小更有优势。

**总结**

**DeepSeek LLM** 是一系列基于2T token 的英中大数据集训练的开源模型。工作深入探讨了超参数的选择、扩展规律及微调尝试，并校准了扩展定律，提出了新的资源分配策略以优化模型/数据扩展。此外，还提出了一种方法来预测给定计算预算下的 Batch Size 和 Learning Rate，指出扩展定律与数据质量相关，影响不同研究工作的扩展行为。通过最佳超参数进行预训练并全面评估。

**DeepSeek Chat&#x20;**&#x5B58;在一些局限性，如知识更新滞后、可能生成非事实信息及幻觉倾向。特别是初版中文数据不详尽，导致在特定中文主题上的表现不佳，且对非中英文的语言支持较弱。未来，**DeepSeek** 计划发布关于代码和 **MoE** 的技术报告，并致力于创建更高质量的数据集以增强下一版本在推理、中文知识、数学和编码方面的能力。**DeepSeek** 对齐团队正探索如何提供更加有用、诚实和安全的模型，初步实验显示强化学习可提升复杂推理能力。

### 4.5.2 DeepSeek-V2

**Deepseek-V2** 采用了包括多头潜在注意力 **MLA** 和 **DeepSeekMoE** 在内的创新架构。**MLA** 通过将 KV 缓存大幅压缩成一个潜在向量来保证高效的推理过程，而 **DeepSeekMoE** 则采用大量的小参数专家进行建模，同时在训练和推理上加入了更多的优化。沿袭了一贯的作风，**DeepSeek** 对模型进行了完全的 MIT 协议开源，可以商用。对于算力不是那么充足的开发者，官方提供了 API 调用的方案，费用更是达到了全场最低。

**Deepseek-V2** 模型参数量方面达&#x5230;**`236B`**，每次处理 token 时激活其中&#x7684;**`21B`**&#x4E2A;，同时由于模型小专家混合的特性，模型在推理时的激活参数很少，可以实现高推理速度。在通用能力的表现上，模型&#x5728;**`MMLU`**&#x591A;选题 benchmark 上取得了第二名，**Deepseek-V2** 在众多开源模型中表现仅次于 70B 的 LLaMA-3，超过了此前发布的 **Deepseek-V1 67B** 的非 MoE 模型。在成本效率方面，相比 **V1** 的稠密模型，**V2** 模型节约&#x4E86;**`42.5%`**&#x7684;训练成本，减少了推理&#x65F6;**`93.3%`**&#x7684; KV 缓存占用，将生成的吞吐量也提升到了原来&#x7684;**`5.76`**&#x500D;。借&#x52A9;**`YaRN`**&#x4F18;化的长度外推训练方法，模型的上下文能力得以扩展到&#x4E86;**`128k`**&#x5927;小。

**Deepseek-V2** 在一个包&#x542B;**`8.1T`** token 的高质量多源语料库上进行了预训练，并进一步进行了监督微调 SFT 和强化学习 RL，以充分挖掘其潜力。评估结果显示，即使仅激活了 **21B** 个参数，**DeepSeek-V2** 及其 **Chat** 版本在开源模型中依然达到了顶级性能。

* **模型结构**

对于 **Deepseek-V2&#x20;**&#x7684;模型结构来说，主要有多头潜在注意力 **MLA** 和 **DeepSeekMoE** 两个方面，如右图，这在本书“**模型架构**”章节的“**注意力改进**”及“**FFN 改进**”部分有详细的介绍，这里不再赘述。

**MHA** 中 kv cache 会成为推理瓶颈。**MQA** 和 **GQA** 可以一定程度减少 kv cache，但效果上不如 **MHA**。**DeepSeek-V2** 设计了 **MLA**，通过低秩 key-value 联合压缩，实现了比 **MHA** 更好的效果并且需要的 kv cache 要小很多。

![]()

**DeepSeekMoE** 引入了两个主要策略：

> 1. **细粒度专家分割**：通过将每个 FFN 专家进一步细分，这允许模型在保持参数总数不变的情况下，激活更多的、更细粒度的专家。这种策略使得各个专家能够专注于更细致的知识领域，提高了专家的专业化程度。
>
> 2. **共享专家隔离**：设置一部分专家作为“共享专家”，这些专家总是被激活，用于捕捉和整合常见的跨上下文知识。这样可以减少路由专家之间的知识冗余，每个路由专家可以更专注于独特的知识领域。

这两个策略是对缓解传统 **MoE** 的缺陷进行的改进：

> 1. **知识杂糅**：传统的 **MoE** 模型中，每个专家往往需要处理多种类型的知识，这使得专家难以形成专门化的知识结构
>
> 2. **知识冗余**：不同的专家在处理不同的输入时可能需要相同的知识，导致多个专家中存在重复的知识，浪费了模型参数

**Deepseek-V2 中，**&#x54;ransformer 层的数量&#x4E3A;**`60`**，隐藏维度设置&#x4E3A;**`5120`**。所有可学习的参数均以标准差&#x4E3A;**`0.006`**&#x8FDB;行随机初始化。在 **MLA** 中，注意力头的数量$n_h=128$，每个头的维度$d_h=128$。KV 压缩维度$d_c=512$，Query 压缩维度$d_c'=1536$。对于解耦的 Query 和 Key，每个头的维度$d^R_h=64$。除第一层外，用 **MoE** 层替换了所有的 **FFN** 层。每个 **MoE&#x20;**&#x5C42;包&#x542B;**`2`**&#x4E2A;共享专家&#x548C;**`160`**&#x4E2A;路由专家，每个专家的中间隐藏维度&#x4E3A;**`1536`**。在路由专家中，每个 token 将激&#x6D3B;**`6`**&#x4E2A;专家。此外，低秩压缩和细粒度专家分割会影响一层的输出规模，因此在压缩的潜在向量后使用额外&#x7684;**`RMSNorm`**&#x5C42;，并在宽度瓶颈处（即压缩的潜在向量和路由专家的中间隐藏状态）乘以额外的比例因子，以确保训练的稳定性。

* **预训练**

**数据**

**Deepseek-V2&#x20;**&#x5728;保持与 **DeepSeek-V1 67B** 相同的数据处理阶段的同时，增加了数据量并提升了数据质量。为了扩大预训练语料库，**DeepSeek&#x20;**&#x63A2;索了互联网数据的潜力，并优化了清理流程，恢复了大量被误删的数据。此外还加入了更多的中文数据，以更好地利用中文互联网上的语料资源。除了增加数据量外，也注重数据质量，通过多种来源的高质量数据丰富预训练语料库，并改进基于质量的过滤算法，确保去除大量非有益数据，同时保留有价值的数据。同时过滤掉了有争议的内容，以减少特定区域文化带来的数据偏差。

**Deepseek-V2&#x20;**&#x91C7;用了与 **DeepSeek-V1 67B** 相同的基于字节级字节对编&#x7801;**`BBPE`**&#x7B97;法构建的分词器，其词汇量&#x4E3A;**`100K`**。经过分词处理的预训练语料库包&#x542B;**`8.1T`**&#x4E2A;标记，其中中文标记比英文标记多大&#x7EA6;**`12%`**。

**超参数**

**Deepseek-V2&#x20;**&#x4F7F;&#x7528;**`AdamW`**&#x4F18;化器进行训练，其超参数设置为$\beta_1 = 0.9$，$\beta_2 = 0.95$，权重衰减设置&#x4E3A;**`0.1`**。学习率采用预热和阶梯衰减策略进行调度。初始阶段，在&#x524D;**`2K`**&#x6B65;中，学习率线性增加从0到最大值。随后，在训练大&#x7EA6;**`60%`**&#x7684;标记后，学习率乘&#x4EE5;**`0.316`**；在训练大&#x7EA6;**`90%`**&#x7684;标记后再乘&#x4EE5;**`0.316`**。最大学习率设置为$2.4 \times 10^{-4}$，梯度裁剪范数设置&#x4E3A;**`1.0`**。同时使用了批量大小调度策略，在&#x524D;**`225B`**&#x4E2A; token 的训练过程中，批量大小逐渐&#x4ECE;**`2304`**&#x589E;加&#x5230;**`9216`**，之后保持在 9216 不变。我们将最大序列长度设置&#x4E3A;**`4K`**，并&#x5728;**`8.1T`**&#x4E2A;标记上训练 **DeepSeek-V2**。

为了部署模型的不同层，采用流水线并行策略，将不同层分布在不同的设备上，对于每一层，路由专家均匀分布&#x5728;**`8`**&#x4E2A;设备上，即$D = 8$。至于设备限制的路由，每个标记最多发送&#x5230;**`3`**&#x4E2A;设备，即$M = 3$。对于损失函数的平衡，设置$\alpha_1=0.003$，$\alpha_2=0.05$，$\alpha_3=0.02$。在训练期间，采用了丢弃 token 的策略以加速训练，但在评估时不丢弃任何 token。

**基建平台**

**DeepSeek-V2&#x20;**&#x57FA;&#x4E8E;**`HAI-LLM`**&#x6846;架进行训练，这是 **DeepSeek&#x20;**&#x5185;部开发的高效轻量级训练框架。它采用&#x4E86;**`16`**&#x8DEF;零气泡流水线并行、**`8`**&#x8DEF;专家并行&#x548C;**`ZeRO-1`**&#x6570;据并行。鉴于 **DeepSeek-V2** 激活的参数相对较少，并且部分操作符通过重新计算以节省激活内存，因此可以在不需要张量并行的情况下进行训练，从而减少了通信开销。此外，为了进一步提高训练效率，将共享专家的计算与专家并行的全对全通信重叠。并且为不同专家之间的通信、路由算法和融合线性计算定制了更快的 CUDA 内核。此外，**MLA&#x20;**&#x4E5F;基于改进版的 **FlashAttention-2&#x20;**&#x8FDB;行了优化。所有的实验都在配备 **NVIDIA H800 GPU** 的集群上进行。

**长上下文**

在 **DeepSeek-V2** 的初步预训练之后，使&#x7528;**`YaRN`**&#x5C06;默认的上下文窗口长度&#x4ECE;**`4K`**&#x6269;展&#x5230;**`128K`**。**YaRN** 特别应用于解耦的共享键$k^R_t$，因为它负责携带 **RoPE**。对于 **YaRN**，尺度$s=40$，$\alpha=1$，$\beta=32$，目标最大上下文长度设置&#x4E3A;**`160K`**。在这种设置下，可以预期模型&#x5728;**`128K`**&#x7684;上下文长度下表现良好。由于注意力机制与原&#x59CB;**`YaRN`**&#x6709;所不同，为了调节注意力熵，对长度缩放因子进行了调整。计算因子$\sqrt{t} = 0.0707 \ln s + 1$，旨在最小化困惑度。

此外，在序列长度&#x4E3A;**`32K`**&#x548C;批量大小&#x4E3A;**`576`**&#x4E2A;序列的情况下，对模型额外进行&#x4E86;**`1000`**&#x6B65;的训练。尽管训练仅在 32K 的序列长度上进行，但当在 128K 的上下文长度下评估时，模型仍然表现出强大的性能。实验结果表明，**DeepSeek-V2** 在所有上下文窗口长度上的表现都非常好。这表明即使在较长的上下文长度下，**DeepSeek-V2&#x20;**&#x4E5F;能保持良好的性能和稳定性。

* **后训练**

**监督微调 SFT**

**DeepSeek&#x20;**&#x6574;理了指令调优数据集，包&#x542B;**`1.5M`**&#x4E2A;实例，其&#x4E2D;**`1.2M`**&#x4E2A;实例用于提升帮助性，**`0.3M`**&#x4E2A;实例用于提升安全性。与初始版本相比，提高了数据质量，以减少幻觉响应并增强写作能力。使&#x7528;**`2`**&#x4E2A; epoch 对 **DeepSeek-V2&#x20;**&#x8FDB;行微调，学习率设置为$5 \times 10^{-6}$。

在评估 **DeepSeek-V2 Chat (SFT)** 时，主要采用了基于生成的基准测试，除了几个具有代表性的选择题任&#x52A1;**`MMLU`**&#x548C;**`ARC`**，还使用指令跟随评&#x4F30;**`IFEval`**&#x5BF9; **DeepSeek-V2 Chat (SFT)** 进行了评估，采用提示级别的宽松准确率作为评估指标。此外还使&#x7528;**`LiveCodeBench`**&#x7684;问题来评估聊天模型。除了标准基准测试，**DeepSeek&#x20;**&#x8FD8;在开放式对话基准上进一步评估了模型，包&#x62EC;**`MT-Bench`**、**`AlpacaEval 2.0`**&#x548C;**`AlignBench`**。为了进行比较，在相同的评估框架和设置下评估了 Qwen1.5 72B Chat、LLaMA-3-70B Instruct 和 Mistral-8x22B Instruct。**DeepSeek 67B Chat** 则直接引用先前发布的评估结果。这样全面的评估方法确保了 **DeepSeek-V2** 在多种任务上的表现得到充分验证。

**强化学习**

为了节省RL的训练成本，**DeepSeek-V2&#x20;**&#x91C7;用了组相对策略优化 **GRPO**（**G**roup **R**elative **P**olicy **O**ptimization），这种方法放弃了通常与策略模型大小相同的评价模型，而是通过组评分来估计基线。具体来说，对于每个问题$q$，**GRPO** 从旧策略$\pi_{\theta_{old}}$中采样一组输出$\{o_1, o_2, \cdots, o_G\}$，然后通过最大化以下目标函数来优化策略模型$\pi_{\theta}$：

$$\mathcal{J}_{GRPO}(\theta) = \mathbb{E}[q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{old}}(O|q)]$$

$$\frac{1}{G} \sum_{i=1}^G \left( \min \left( \frac{\pi_\theta(o_i|q)}{\pi_{\theta_{old}}(o_i|q)} A_i, \text{clip} \left( \frac{\pi_\theta(o_i|q)}{\pi_{\theta_{old}}(o_i|q)}, 1-\varepsilon, 1+\varepsilon \right) A_i \right) - \beta \mathbb{D}_{KL}(\pi_\theta || \pi_{ref}) \right)$$

$$\mathbb{D}_{KL}(\pi_\theta || \pi_{ref}) = \frac{\pi_{ref}(o_i|q)}{\pi_\theta(o_i|q)} - \log \frac{\pi_{ref}(o_i|q)}{\pi_\theta(o_i|q)} - 1$$

其中，$\varepsilon$和$\beta$是超参数；$A_i$是优势值，使用每个组内输出对应的奖励组 $\{r_1, r_2, \ldots, r_G\}$ 进行计算：

$A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \cdots, r_G\})}{\text{std}(\{r_1, r_2, \cdots, r_G\})}$

**GRPO** 和 **PPO** 的区别如右图所示：

![]()

在初步实验中，**DeepSeek&#x20;**&#x53D1;现针对推理数据，如代码和数学提示，进行的强化学习训练表现出与一般数据训练不同的独特特征。例如在数学和编程能力方面可以在较长的训练步骤中持续改进。因此**采用了一个两阶段的强化学习训练策略，首先进行推理对齐，然后进行人类偏好对齐**。在第一阶段的推理对齐中，**DeepSeek&#x20;**&#x8BAD;练了一个专门用于代码和数学推理任务的奖励模型$\text{RM}_\text{reasoning}$，并使用该奖励模型的反馈来优化策略模型$\pi_\theta$：

$$r_i = \text{RM}_\text{reasoning}(o_i)$$

在第二阶段的人类偏好对齐中，采用了一个多奖励框架，该框架从一个有用性奖励模型$\text{RM}_\text{helpful}$、一个安全性奖励模型$\text{RM}_\text{safety}$和一个基于规则的奖励模型$\text{RM}_\text{rule}$中获取奖励。具体来说，每个奖励模型分别评估响应的不同方面：$\text{RM}_\text{helpful}$评估响应的帮助性和相关性；$\text{RM}_\text{safety}$评估响应的安全性和适当性；$\text{RM}_\text{rule}$基于预定义的规则集评估响应的合规性。最终响应$o_i$的奖励是通过结合这些不同来源的奖励来确定的：

$$r_i = c_1\cdot \text{RM}_\text{helpful}(o_i) + c_2\cdot \text{RM}_\text{safety}(o_i) + c_3\cdot \text{RM}_\text{rule}(o_i)$$

其中，$c_1$、$c_2$和$c_3$是相应的系数。

为了获得在强化学习训练中起关键作用的可靠奖励模型，**DeepSeek&#x20;**&#x4ED4;细收集偏好数据，并精心进行质量过滤和比例调整。这里基于编译器反馈获取代码偏好数据，并基于真实标签获取数学偏好数据。对于奖励模型的训练，使用 **DeepSeek-V2 Chat (SFT)** 初始化奖励模型，并使用 **point-wise** 或 **pair-wise** 损失进行训练。强化学习训练能够充分挖掘和激活模型的潜力，使其能够在可能的响应中选择正确且满意的答案。

在极大规模的模型上进行强化学习训练对训练框架提出了很高的要求。它需要精心的工程优化来管理 GPU 内存和 RAM 的压力，同时保持快速的训练速度。为了实现这一目标，**DeepSeek&#x20;**&#x5B9E;施了以下工程优化：

> 1. 提出了一种混合引擎，分别为训练和推理采用不同的并行策略，以实现更高的 GPU 利用率
>
> 2. 利&#x7528;**`vLLM`**&#x4E0E;大批量大小作为我们的推理后端，以加速推理速度
>
> 3) 仔细设计了一种将模型卸载到 CPU 以及从 CPU 加载回 GPU 的调度策略，从而在训练速度和内存消耗之间实现了近乎最优的平衡

**总结**

**DeepSeek-V2** 是一个支持 128K 上下文长度的大规模 MoE 语言模型。得益于其创新的架构，包括 MLA 和 DeepSeekMoE，该模型不仅性能强大，而且训练经济、推理高效。与 **DeepSeek 67B** 相比，**DeepSeek-V2** 显著提升了性能，节省了 42.5% 的训练成本，减少了 93.3% 的 KV 缓存，并将最大生成吞吐量提高了 5.76 倍。评估结果显示，仅激活 21B 参数的 DeepSeek-V2 在开源模型中表现优异，成为最强的开源 MoE 模型。

尽管如此，**DeepSeek-V2** 及其聊天版本仍存在常见 LLM 的局限性，如预训练后缺乏持续的知识更新、可能生成不准确信息以及可能出现幻觉等。此外，由于数据主要由中文和英文内容组成，该模型在其他语言上的表现可能有限。

### 4.5.3 DeepSeek-V2.5

**DeepSeek&#x20;**&#x5C06; **DeepSeek-V2-Chat** 和 **DeepSeek-Coder-V2** 两个模型的合并，形成 **DeepSeek-V2.5**。**DeepSeek-V2.5** 不仅保留了原有 **Chat** 模型的通用对话能力和 **Coder** 模型的强大代码处理能力，还更好地对齐了人类偏好。此外 **DeepSeek-V2.5** 在写作任务、指令跟随等多个方面也实现了大幅提升。

**DeepSeek** 在 6 月份对 **DeepSeek-V2-Chat** 进行了重大升级，用 **DeepSeek-Coder-V2** 的 **Base** 模型替换原有 **DeepSeek-V2** 的 **Base** 模型，显著提升了其代码生成和推理能力，并发布了 **DeepSeek-V2-Chat-0628** 版本。紧接着，**DeepSeek-Coder-V2** 在原有 **Base** 模型的基础上，通过对齐优化，大大提升通用能力后推出了 **DeepSeek-Coder-V2-0724** 版本。最终将 **Chat** 和 **Coder** 两个模型合并，推出 **DeepSeek-V2.5**。

![]()

**DeepSeek-V2.5&#x20;**&#x7684;能力得到了很大的提升：

1. **通用能力**：使用业界通用的测试集对 **DeepSeek-V2.5** 进行测评，在中文和英文四个测试集上，**DeepSeek-V2.5** 均优于之前的 **DeepSeek-V2-0628** 以及 **DeepSeek-Coder-V2-0724**。在 **DeepSeek&#x20;**&#x5185;部的中文评测中，和 **GPT-4o mini**、**ChatGPT-4o-latest** 的对战胜率相较于 **DeepSeek-V2-0628** 均有明显提升，其中裁判为 GPT-4o。此测评中涵盖创作、问答等通用能力等。

2. **安全能力**：在 **DeepSeek-V2.5** 中对模型安全问题的边界做了更加清晰的划分，在强化模型对于各种越狱攻击的安全性的同时，减少了安全策略过度泛化到正常问题中去的倾向。

3. **代码能力**：**DeepSeek-V2.5** 保留了 **DeepSeek-Coder-V2-0724** 的代码能力，&#x5728;**`HumanEval Python`**&#x548C;**`LiveCodeBench`**&#x6D4B;试中显示了较为显著的改进。&#x5728;**`HumanEval Multilingual`**&#x548C;**`Aider`**&#x6D4B;试中 **DeepSeek-Coder-V2-0724** 略胜一筹。&#x5728;**`SWE-verified`**&#x6D4B;试中，两个版本的表现都较低，表明在此方面仍需进一步优化。&#x5728;**`FIM`**&#x8865;全任务上，内部评测&#x96C6;**`DS-FIM-Eval`**&#x7684;评分提升&#x4E86;**`5.1%`**。在内部的主观评&#x6D4B;**`DS-Arena-Code`**&#x4E2D;，**DeepSeek-V2.5** 对战竞品的胜率取得了显著提升，裁判仍为 GPT-4o。

### 4.5.4 DeepSeek-V3

**DeepSeek-V3** 也是 **MoE** 语言模型，总共&#x6709;**`671B`**&#x53C2;数，每次处理标记时激&#x6D3B;**`37B`**&#x53C2;数。为了实现高效的推理和经济的训练，**DeepSeek-V3** 采用了在 **DeepSeek-V2** 中使用过的 **MLA** 和 **DeepSeekMoE** 架构。此外，**DeepSeek-V3** 首创了**无辅助损失的负载均衡策略**，并设定了**多 Token 预测训练目标**以增强性能。

**DeepSeek-V3** &#x5728;**`14.8T`**&#x4E2A;多样化且高质量的 token 上进行预训练，随后通过监督微调和强化学习阶段来充分发挥其能力。实验显示 **DeepSeek-V3** 超越了其他开源模型，并达到了与领先的闭源模型相当的性能，而且 **DeepSeek-V3** 的完整训练仅&#x9700;**`2.788M H800 GPU`**&#x5C0F;时。

* **模型结构**

**DeepSeek-V3** 采用 **MLA** 实现高效推理，并用 **DeepSeekMoE** 实现经济的训练。与 **DeepSeek-V2** 相比，额外引入了无辅助损失的负载均衡机制。除此之外还使用了多 Token 预测 **MTP**（**M**ulti-**T**oken **P**rediction）训练目标，这可以提升在评估基准上的整体性能。这里依次介绍这两方面。

**无辅助损失的负载均衡策略**

**MoE 基础架构**：对于前馈网络 FFN，**DeepSeek-V3** 采用了 **DeepSeekMoE** 架构。与传统 MoE 架构相比，**DeepSeekMoE** 使用更细粒度的专家并将部分专家隔离为共享专家。设$u_t$表示第$t$个 token 的 FFN 输入，按如下方式计算 FFN 的输出$h_t'$ ：

$$h_t' = u_t + \sum_{i=1}^{N_s} \text{FFN}^{(s)}_i (u_t) + \sum_{i=1}^{N_r} g_{i,t} \text{FFN}^{(r)}_i (u_t)$$

$$g_{i,t} = \frac{g_{i,t}'}{\sum_{j=1}^{N_r} g_{j,t}'}$$

$$g_{i,t}' = \begin{cases} s_{i,t}, & s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \leq j \leq N_r\}, K_r) \\ 0, & \text{otherwise} \end{cases}$$

$$s_{i,t} = \text{Sigmoid}(u_t^\top e_i)$$

其中$N_s$和$N_r$分别表示共享专家和路由专家的数量，$\text{FFN}_i^{(s)}(\cdot)$和$\text{FFN}_i^{(r)}(\cdot)$分别表示第$i$个共享专家和第$i$个路由专家，$K_r$表示激活的路由专家数量，$g_{i,t}$是第$i$个专家的门控值；$s_{i,t}$是 token 到专家的亲和度，$e_i$是第$i$个路由专家的质心向量，而$\text{Topk}(\cdot, K)$表示在所有路由专家中为第$t$个 token 计算的亲和度分数中最高的$K$个分数组成的集合。与 **DeepSeek-V2** 不同的是 **DeepSeek-V3** 使用 Sigmoid 函数计算亲和度分数，并对所有选定的亲和度分数进行归一化以生成门控值。

**无辅助损失的负载均衡策略**：对于 MoE 模型来说，专家负载不平衡会导致路由崩溃并降低专家并行场景下的计算效率。传统解决方案通常依靠辅助损失来避免负载不平衡。然而，过大的辅助损失会损害模型性能。

为了在负载均衡和模型性能之间取得更好的平衡，**DeepSeek-V3&#x20;**&#x63D0;出了一种无辅助损失的负载均衡策略。具体来说，为每个专家引入一个偏置项$b_i$，并将其添加到相应的亲和度分数$s_{i,t}$中，以确定$\text{Topk}$路由：

$$g_{i,t}' = \begin{cases} s_{i,t}, & s_{i,t} + b_i \in \text{Topk}(\{s_{j,t} + b_j | 1 \leq j \leq N_r\}, K_r) \\ 0, & \text{otherwise} \end{cases}$$

这里偏置项仅用于路由，与 FFN 输出相乘的门控值仍然由原始亲和度分数$s_{i,t}$得出。在训练过程中持续监控每个训练步骤中整个批次的专家负载。在每个步骤结束时，如果相应的专家负载过重，将偏置项减少$\gamma$；如果相应的专家负载不足，则增加$\gamma$，其中$\gamma$是一个称为**偏置更新速度**的超参数。通过这种动态调整，**DeepSeek-V3** 在训练过程中保持专家负载均衡，并实现了比纯粹依靠辅助损失来鼓励负载均衡的模型更好的性能。

**互补序列级辅助损失策略**：尽管 **DeepSeek-V3** 主要依靠无辅助损失策略来实现负载均衡，但为了防止任何单个序列内出现极端不平衡，还采用了互补序列级平衡损失：

$$\mathcal{L}_{Bal} = \alpha \sum_{i=1}^{N_r} f_i P_i$$

$$f_i = \frac{N_r}{K_r T} \sum_{t=1}^{T} \mathbb{1}(s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \leq j \leq N_r\}, K_r))$$

$$s_{i,t}' = \frac{s_{i,t}}{\sum_{j=1}^{N_r} s_{j,t}}$$

$$P_i = \frac{1}{T} \sum_{t=1}^{T} s_{i,t}'$$

其中平衡因子$\alpha$是一个超参数，会被赋予一个极小的值，$1(\cdot)$表示指示函数，$T$表示序列中的 token 数量。序列级平衡损失的目的是确保每个序列上的专家负载保持平衡。

**节点限制路由**：与 **DeepSeek-V2** 采用的设备限制路由类似，**DeepSeek-V3** 同样采用受限制的路由机制来控制训练过程中的通信开销：确保每个 token 最多只会被发送&#x5230;**`M`**&#x4E2A;节点，这些节点是根据每个节点上分布的专家的最高$\frac{K_r}{M}$个亲和度分数总和来选择的。在此约束条件下，MoE 训练框架能够实现近乎完整的计算与通信重叠。

**无 Token 丢弃**：基于高效的负载均衡策略，**DeepSeek-V3** 在整个训练过程中都能保持良好的负载平衡状态，因此 **DeepSeek-V3** 在训练过程中不会丢弃任何 token。而且还实施了专门的部署策略来确保推理时的负载均衡，使得 **DeepSeek-V3** 在推理阶段同样不会发生 token 丢弃。

**多 Token 预测 MTP**

**DeepSeek-V3** 探索并设置了多 Token 预测目标，将预测范围扩展到每个位置的多个未来 token。这种方法具有双重优势：一方面，**MTP** 目标能够使训练信号更加密集，有望提升数据使用效率；另一方面，**MTP** 使模型能够预先规划其表示，从而更好地预测未来 token。

![]()

**MTP 模块**：上图展示了 **MTP** 实现方案，这里采用顺序预测额外 token 的方式，并在每个预测深度保持完整的因果链。具体而言，**MTP** 使用$D$个顺序模块来预测$D$个额外的 token。第$k$个 **MTP** 模块由以下组件构成：一个与主模型共享的嵌入层$\text{Emb}(\cdot)$、一个共享的输出头$\text{OutHead}(\cdot)$、一个 Transformer 块$\text{TRM}_k(\cdot)$以及一个投影矩阵$M_k \in \mathbb{R}^{d \times 2d}$。在第$k$个预测深度处理第$i$个输入 token $t_i$ 时，首先将两个表示向量结合起来：第$k-1$深度的第$i$个 token 表示$h_{k-1}^i \in \mathbb{R}^d$和第$i+k$个 token 的嵌入表示$\text{Emb}(t_{i+k}) \in \mathbb{R}^d$。这种结合通过如下线性投影实现：

$$h_i^{'k} = M_k[\text{RMSNorm}(h^{k-1}_i);\text{RMSNorm}(\text{Emb}(t_{i+k}))]$$

其中$[:]$表示向量连接操作。当$k = 1$时，$h_{k-1}^i$是主模型输出的表示。每个 **MTP** 模块的嵌入层都与主模型共享。将组合得到的 $h_i^{'k}$ 输入到第$k$深度的 Transformer 块中，从而生成当前深度的输出表示$h^k_i$：

$$h_{1:T-k}^k = \text{TRM}_k(h_{1:T-k}^{'k})$$

这里的$T$代表输入序列的长度，而$i:j$表示切片操作（包含左右边界）。最后，系统将 $h^k_i$ 输入到共享输出头中，计算第$k$个额外预测 token 的概率分布$p_{i+1+k}^k \in \mathbb{R}^V$（其中 $V$ 表示词汇表大小）：

$$p_{i+1+k}^{k} = \text{OutHead}(h^k_i)$$

输出头$\text{OutHead}(\cdot)$首先将表示向量线性映射为 logits，随后通过$\text{Softmax}(\cdot)$函数计算第$k$个额外 token 的预测概率，这里每个 **MTP** 模块的输出头也与主模型共享。

**MTP 训练目标**：在每个预测深度，计算如下交叉熵损失$\mathcal{L}^k_{\text{MTP}}$：

$$\mathcal{L}^k_{\text{MTP}} = \text{CrossEntropy}(p_{2+k:T+1}^k, t_{2+k:T+1}) = -\frac{1}{T} \sum_{i=2+k}^{T+1} \log p_i^k[t_i]$$

其中，$T$代表输入序列的长度，$t_i$表示第$i$个位置的真实 token，而$p_i^k[t_i]$则表示第$k$个 **MTP** 模块对$t_i$的预测概率。最终通过计算所有深度 **MTP** 损失的平均值，并与权重因子$\lambda$相乘，得到总体 **MTP** 损失$\mathcal{L}_{\text{MTP}}$，这作为 **DeepSeek-V3** 的补充训练目标：

$$\mathcal{L}_{\text{MTP}} = \frac{\lambda}{D} \sum_{k=1}^D \mathcal{L}^k_{\text{MTP}}$$

**MTP 的推理**：设计 **MTP** 策略的主要目的是提升主模型的性能，因此在实际推理阶段可以直接移除 **MTP** 模块，让主模型独立运行。而且还可以将这些 **MTP** 模块重新用于推测解码，从而进一步降低生成延迟。

* **基建**

**DeepSeek-V3** 是在一个配&#x5907;**`2048`**&#x4E2A;**`NVIDIA H800 GPU`**&#x7684;集群上训练的。H800 集群中的每个节点包含 8 个通过 NVLink 和节点内的 NVSwitch 连接的 GPU。在不同节点之间，使用 **IB**（**I**nfini**B**and）互连来促进通信。

**训练框架**

**DeepSeek-V3** 的训练由 HAI-LLM 框架支持，这是 **DeepSeek&#x20;**&#x7684;工程师从头开始精心打造的高效轻量级训练框架。总体而言，**DeepSeek-V3** 应用了 16 路**流水线并行** **PP**、跨越 8 个节点的 64 路**专家并行** **EP** 以及 **ZeRO-1** **数据并行** **DP**。

为了促进 DeepSeek-V3 的高效训练，**DeepSeek&#x20;**&#x5B9E;施了细致的工程优化：

1. **设计了 DualPipe 算法来实现高效的流水线并行**

![]()

**DeepSeek-V3** 的跨节点专家并行因通信开销导致计算与通信效率比率低至 1:1。为此，提出了一种名为 DualPipe 的流水线并行算法，旨在通过重叠前向和后向计算-通信阶段来提升训练效率，并减少流水线中的气泡。DualPipe 创新性地在每个数据块中划分出**注意力机制**、**all-to-all 分发**、**MLP** 和 **all-to-all 组合**四个部分，其中后向数据块的注意力机制和 MLP 还进一步细分为输入和权重的反向传播。引入 PP 通信组件，并通过重新排列这些组件及手动调整 GPU SM 分配给计算与通信的比例，实现了 all-to-all 和 PP 通信的有效隐藏，如上图所示。采用双向流水线调度策略，从两端馈送微批次，以完全重叠大部分通信，确保随着模型规模扩大仍能保持恒定的计算与通信比率，实现接近零的 all-to-all 通信开销，一个 DualPipe 调度如下图所示，包&#x542B;**`8`**&#x4E2A; PP 等级&#x548C;**`20`**&#x4E2A;微批次，分为两个方向。

![]()

即使在通信负担较轻的场景下，DualPipe 相较于 ZB1P 和 1F1B 等方法也展示了显著的效率优势，如右表所示，大幅减少了流水线气泡，同时仅轻微增加了峰值激活内存。

![]()

尽管需要维护两个副本的模型参数，但由于使用较大的 EP 大小进行训练，这并不会显著增加内存消耗。此外，与 Chimera 方法相比，DualPipe 只要求流水线阶段和微批次数量可被 2 整除，且随微批次数量增加时不会额外增加气泡或激活内存。

* **开发了高效的跨节点 all-to-all 通信内核**

为确保 DualPipe 的计算性能，定制了高效的跨节点 all-to-all 通信内核（包括分发和组合），以减少分配给通信的 SM 数量。这些内核与 MoE 门控算法及集群网络拓扑共同设计。具体来说，集群中的跨节点 GPU 通过 IB 完全互联，而节点内部通信则通过 NVLink 处理，后者提&#x4F9B;**`160 GB/s`**&#x7684;带宽，约为IB（50 GB/s）&#x7684;**`3.2`**&#x500D;。为了有效利用 IB 和 NVLink 的不同带宽，每个 token 最多被分发到4个节点，从而减少 IB 流量。当一个 token 的路由决策确定后，首先通过 IB 传输到目标节点上具有相同节点内索引的 GPU，到达目标节点后，立即通过 NVLink 转发至特定 GPU，避免被后续到达的令牌阻塞。这样，IB 和 NVLink 的通信完全重叠，每个 token 能高效选择每节点平均 3.2 个专家，不增加 NVLink 额外开销。这意味着虽然 **DeepSeek-V3** 实际上仅选择 8 个路由专家，但可以扩展到最多13个专家（4节点×3.2专家/节点），同时保持相同的通信成本。总体而言，采用这种通信策略，仅需 20 个 SM 即可充分利用 IB 和 NVLink 的带宽。

详细来说，通过使&#x7528;**`warp specialization`**&#x6280;术，将20个SM分为10个通信通道。在分发过程中，(1) IB发送、(2) IB 到 NVLink 转发、(3) NVLink 接收分别由相应的 warp 处理，根据实际工作负载动态调整分配给每个通信任务的 warp 数量。类似地，在组合过程中，(1) NVLink 发送、(2) NVLink 到 IB 转发和累加、(3) IB 接收和累加也由动态调整的 warp 处理。此外，分发和组合内核与计算流重叠，因此还需考虑它们对其他 SM 计算内核的影响。具体措施包括使用定制的 PTX 指令并自动调优通信块大小，这显著减少了 L2 缓存的使用及对其他 SM 的干扰。这种方法不仅提高了通信效率，还优化了整体系统性能。

* **精心优化了训练期间的内存占用**

**RMSNorm 和 MLA 上投影的重计算**。 在反向传播期间重新计算所有 RMSNorm 操作和 MLA 上投影，从而无需持久存储它们的输出激活。通过少量开销，此策略显著减少了存储激活所需的内存。

**CPU 中的指数移动平均**。 在训练期间保留模型参数的指数移动平均 (EMA)，以便在学习率衰减后尽早估计模型性能。EMA 参数存储在 CPU 内存中，并在每个训练步骤后异步更新。此方法允许我们维护 EMA 参数，而不会产生额外的内存或时间开销。

**多 Token 预测的共享嵌入和输出头**。 通过 DualPipe 策略，将模型的最浅层（包括嵌入层）和最深层（包括输出头）部署在相同的 PP 秩上。这种安排使得 MTP 模块和主模型之间能够物理共享参数和梯度，包括共享的嵌入和输出头。这种物理共享机制进一步提高了我们的内存效率。

**FP8 训练**

基于低精度训练的进展，**DeepSeek&#x20;**&#x63D0;出了一种利&#x7528;**`FP8`**&#x6570;据格式的细粒度混合精度框架来训练 **DeepSeek-V3**。尽管低精度训练前景广阔，但其应用常受限于激活值、权重和梯度中的异常值。尽管推理量化已取得显著进展，但在大规模语言模型预训练中成功应用低精度技术的例子较少。

为应对这一挑战并扩展 **FP8** 格式的动态范围，采用了一种细粒度量化策略：使用$1×N_c$元素的 tile-wise 分组或$N_c×N_c$元素的 block-wise 分组。高精度累积过程中，反量化开销显著减少，这对实现精确的 **FP8** 通用矩阵乘法 GEMM 至关重要。此外，为了降低 MoE 训练中的内存和通信开销，激活值以 **FP8** 格式缓存和分派，而优化器状态则&#x4EE5;**`BF16`**&#x683C;式存储。该混合精度框架在与 **DeepSeek-V2-Lite&#x20;**&#x548C; **DeepSeek-V2** 相似规模的模型上进行了验证，训练了大&#x7EA6;**`1T`**&#x4E2A;Token。结果显示，相比 **BF16** 基线，**FP8** 训练模型的相对损失误差保持&#x5728;**`0.25%`**&#x4EE5;下，这在训练随机性的可接受范围内。

1. **混合精度框架**

基于低精度训练的广泛应用技术，**DeepSeek&#x20;**&#x63D0;出了一种用于 **FP8** 训练的混合精度框架。在此框架中，大多数计算密集型操作在 **FP8** 精度下执行，而少数关键操作则保留其原始数据格式，以平衡训练效率和数值稳定性。

![]()

整体框架如右图所示，大部分核心计算内核以 **FP8** 精度实现，这些操作接收 **FP8** 张量作为输入，并产生 **BF16** 或 **FP32** 格式的输出。具体来说，与线性算子相关的三个 **GEMM** 操作——前向传&#x64AD;**`Fprop`**、激活反向传&#x64AD;**`Dgrad`**&#x548C;权重反向传&#x64AD;**`Wgrad`**——都在 **FP8** 精度下执行。这种设计理论上使计算速度相比原始 **BF16** 方法提高一倍。此外，**FP8 Wgrad GEMM** 允许激活值以FP8格式存储，从而显著减少内存消耗。

尽管 **FP8** 格式具有效率优势，但某些算子由于对低精度计算的敏感性，仍需要更高的精度支持。因此，经过分析，以下组件保持了原始精度，如 **BF16** 或 **FP32**：嵌入模块、输出头、MoE 门控模块、归一化算子和注意力算子。这些高精度组件确保了 **DeepSeek-V3** 的稳定训练动态。为进一步保证数值稳定性，主权重、权重梯度和优化器状态均以更高精度存储。这种方法不仅利用了 **FP8** 的效率优势，同时也通过有针对性地保留高精度部分，确保了模型训练的稳定性和准确性。

* **量化和乘法带来精度提升**

基于混合精度FP8框架，**DeepSeek&#x20;**&#x63D0;出了几种策略以提高低精度训练的准确性，重点在于量化方法和乘法过程：

**细粒度量化**：在低精度训练中，由于 **FP8** 格式动态范围有限，溢出和下溢是常见挑战。标准做法是将输入张量的最大绝对值缩放到 **FP8&#x20;**&#x7684;最大可表示值。然而，这种方法对激活异常值敏感，可能降低量化精度。为此，引入了细粒度量化方法：激活值&#x6309;**`1x128`** tile 分组，即每个 token 128个通道，权重则&#x6309;**`128x128`**&#x5757;分组，即128个输入通道和128个输出通道，如右图。

![]()

这种细化的缩放策略确保量化过程能更好地适应异常值。此外在 **GEMM** 操作的内部维度引入每组缩放因子，结合高精度 **FP32** 累积策略，提高了计算效率。

**提高累加精度**：低精度 GEMM 操作常遇到下溢问题，依赖于高精度累加。然而，在 NVIDIA H800 GPU 上，FP8 GEMM 的累加精度限制在&#x7EA6;**`14`**&#x4F4D;，低于FP32精度。为解决此问题，**DeepSeek&#x20;**&#x91C7;用了提升到 CUDA Cores 进行更高精度累加的策略：Tensor Cores 执行 MMA 时，中间结果用有限位宽累加，达到间&#x9694;**`K`**&#x540E;，部分结果复制到CUDA Cores上的 FP32 寄存器进行全精度累加。该策略通过高效地在 CUDA Cores 上相乘缩放因子实现去量化，同时保持高利用率。

**尾数优于指数**：不同于先前工作采用混合FP8格式（Fprop 使&#x7528;**`E4M3`**，Dgrad 和 Wgrad 使&#x7528;**`E5M2`**），所有张量均采&#x7528;**`E4M3`**&#x683C;式以获得更高精度。细粒度量化策略使得在较小元素组间共享指数位，减轻了有限动态范围的影响。

**在线量化**：为了简化框架并确保尺度准确，针对每&#x4E2A;**`128x128`**&#x7684;激活图块或权重块在线计算最大绝对值，并推导出缩放因子。基于此最大绝对值，在线将激活或权重量化为 **FP8** 格式，避免了延迟量化技术带来的复杂性。

* **低精度存储与通信**

结合 FP8 训练框架，通过将缓存的激活值和优化器状态压缩为低精度格式，进一步降低了内存消耗和通信开销：

**低精度优化器状态**：采用BF16数据格式来跟踪AdamW优化器中的一阶和二阶矩，未观察到明显的性能下降。然而，主权重和梯度仍保留在FP32中，以确保数值稳定性。

**低精度激活**：Wgrad操作在FP8中执行，激活值以FP8格式缓存用于线性算子的反向传播，以减少内存消耗。特别地：

> 1. **注意力算子之后线性算子的输入**：这些激活值对精度敏感，因此采用了定制&#x7684;**`E5M6`**&#x6570;据格式，并在反向传播中&#x4ECE;**`128x128`**&#x91CF;化块转换&#x4E3A;**`128x1`**&#x91CF;化块。所有缩放因子均进行舍入缩放，即 2 的整数次幂，以避免额外的量化误差。
>
> 2. **MoE 中 SwiGLU 算子的输入**：为了进一步降低内存成本，缓存 SwiGLU 算子的输入并在反向传播中重新计算其输出。这些激活值使用细粒度量化方法以 FP8 格式存储，在内存效率和计算精度之间取得平衡。

**低精度通信**：通信带宽是 MoE 模型训练的关键瓶颈。为此，在 MoE 投影层之前将激活值量化为 FP8，然后应用分发组件，与 MoE 投影层中的 FP8 前向传播兼容。类似地，激活值的缩放因子设置为 2 的整数幂。对于 MoE 下投影层之前的激活梯度，采用相同策略。前向和反向组合组件则保留在 BF16 中，以在训练管道的关键部分保持训练精度。

**推理和部署**

在 H800 集群上部署 **DeepSeek-V3**，每个节点内的 GPU 通过 NVLink 互联，整个集群的所有 GPU 则通过 IB 完全互联。为了确保在线服务的服务级别目标 SLO 和高吞吐量，采用了以下部署策略，将预填充阶段和解码阶段分离：

1. **预填充**

预填充阶段的最小部署单元由4个节点组成，每个节点配备32个GPU。注意力部分采用4路张量并行（TP4）与序列并行（SP）结合，并结合8路数据并行（DP8）。MoE部分使用32路专家并行（EP32），确保每个专家处理足够大的批次大小以提高计算效率。MoE的all-to-all通信首先通过IB在节点之间传输Token，然后通过NVLink在节点内的GPU之间转发。浅层中使用1路张量并行以节省TP通信开销。为了实现负载均衡，引入冗余专家策略，复制高负载专家并定期调整其部署（例如每10分钟一次）。每个GPU除了托管原始8个专家外，还托管一个额外的冗余专家。为提高吞吐量，同时处理两个具有相似计算负载的微批次，重叠注意力机制和MoE计算与分发和组合操作。此外，正在探索一种动态冗余策略，其中每个GPU托管更多专家（如16个），但在每个推理步骤中仅激活9个专家。

* **解码**

解码过程中，共享专家被视为一个被路由的专家，每个Token选择9个专家，其中共享专家总是被选中。解码阶段的最小部署单元由4日讯道组成，共计320个GPU。注意力部分采用TP4与SP结合，并结合DP80，而MoE部分使用EP320。每个GPU仅托管一个专家，64个GPU负责托管冗余专家和共享专家。所有到所有通信通过IB上的直接点对点传输执行，利用IBGDA技术进一步减少延迟并提高通信效率。根据在线服务的统计专家负载，定期确定冗余专家集合，但无需重新排列专家。正在探索用于解码的动态冗余策略，需优化全局最优路由方案算法及与调度内核的融合以减少开销。

* **预训练**

**数据**

**优化预训练语料库**：与 **DeepSeek-V2** 相比，**DeepSeek-V3&#x20;**&#x901A;过增加数学和编程样本的比例优化了预训练语料库，并扩展多语言覆盖范围至英语和中文之外。改进的数据处理流程减少了冗余，同时保持语料库的多样性。采用文档打包方法确保数据完整性，但在训练期间不使用跨样本注意力掩码。**DeepSeek-V3** 的训练语料库包&#x542B;**`14.8T`**&#x9AD8;质量和多样化的token。

**中间填充策略**：在 **DeepSeekCoder-V2&#x20;**&#x8BAD;练过程中观察到，中间填充 **FIM**（**F**ill-**I**n-**M**iddle）策略不会损害下一个 token 的预测能力，反而使模型能够根据上下文线索准确预测中间文本。因此，在 **DeepSeek-V3** 的预训练中也采用了 **FIM&#x20;**&#x7B56;略。具体实现采用前缀-后缀-中间 **PSM**（**P**refix-**S**uffix-**M**iddle）框架：

**`<|fim_begin|>`**$$f_\text{pre}$$**`<|fim_hole|>`**$$f_\text{suf}$$**`<|fim_end|>`**$$f_\text{middle}$$**`<|eos_token|>`**

此结构作为文档级别预打包过程的一部分应用，FIM策略的应用率&#x4E3A;**`0.1`**。

**Tokenizer 改进**：**DeepSeek-V3** 的 tokenizer 采&#x7528;**`BBPE`**，词汇量扩展&#x81F3;**`128K`**&#x4E2A; token。新的预分词器经过修改，优化了多语言压缩效率，并引入了结合标点符号和换行符的 token。然而，这种技术可能导致没有结尾换行符的多行提示词出现 token 边界偏差，特别是在少样本学习评估提示词中。为解决这一问题，在训练期间随机拆分一定比例的此类组合 token，使模型接触到更广泛的特殊情况，减轻了偏差。

**超参数**

**模型超参数**：**DeepSeek-V3** 的 Transformer 层数量设置&#x4E3A;**`61`**，隐藏层维度&#x4E3A;**`7168`**。所有可学习参数&#x4EE5;**`0.006`**&#x7684;标准差随机初始化。在 MLA 中，注意力头数设置&#x4E3A;**`128`**，每个头的维度&#x4E3A;**`128`**。KV 压缩维度&#x4E3A;**`512`**，查询压缩维度&#x4E3A;**`1536`**。对于解耦的 Query 和 Key，每个头的维度设置&#x4E3A;**`64`**。除前三层外，所有 FFN 层被 MoE 层替换，每个MoE层包&#x542B;**`1`**&#x4E2A;共享专家&#x548C;**`256`**&#x4E2A;路由专家，每个专家的中间隐藏层维度&#x4E3A;**`2048`**。每个 token 激&#x6D3B;**`8`**&#x4E2A;专家，并确保每个 token 最多发送&#x5230;**`4`**&#x4E2A;节点。**MTP&#x20;**&#x6DF1;度设置&#x4E3A;**`1`**。**DeepSeek-V3** 采用额外的 RMSNorm 层并乘以缩放因子。模型总参数量&#x4E3A;**`671B`**，其中每个 token 激&#x6D3B;**`37B`**&#x53C2;数。

**训练超参数**：使&#x7528;**`AdamW`**&#x4F18;化器，参数设置为$β_1=0.9，β_2=0.95，\text{weight\_decay}=0.1$。预训练的最大序列长度&#x4E3A;**`4K`**，&#x5728;**`14.8T`**&#x4E2A; token 上进行预训练。学习率调度策略如下：&#x524D;**`2K`**&#x6B65;线性增加至$2.2×10^{-4}$，然后保持该值直到消&#x8017;**`10T`**&#x4E2A; token。随后，&#x5728;**`4.3T`**&#x4E2A; token 内逐渐衰减至$2.2×10^{-5}$，遵循余弦衰减曲线。最&#x540E;**`500B`**&#x4E2A; token 中，&#x524D;**`333B`**&#x4FDD;持$2.2×10^{-5}$的学习率，剩&#x4F59;**`167B`**&#x5207;换至$7.3×10^{-6}$。梯度裁剪范数设&#x4E3A;**`1.0`**。批量大小从训练初期&#x7684;**`3072`**&#x9010;步增加&#x5230;**`15360`**，并在剩余训练过程中保持不变。模型的不同层通过流水线并行部署在不同的 GPU 上，每层的路由专家均匀分布&#x5728;**`64`**&#x4E2A; GPU 上，属&#x4E8E;**`8`**&#x4E2A;节点。每个 token 最多发送&#x5230;**`4`**&#x4E2A;节点。无辅助损失的负载平衡偏差更新速度设置为&#x524D;**`14.3T`**&#x4E2A; token 为`0.001`，其&#x4F59;**`500B`**&#x4E3A;**`0`**。平衡损失权重设&#x4E3A;**`0.0001`**，**MTP** 损失权重&#x524D;**`10T`**&#x4E3A;**`0.3`**，剩&#x4F59;**`4.8T`**&#x4E3A;**`0.1`**。

**上下文扩展**

**DeepSeek-V3** 通过与 **DeepSeek-V2** 相似的方法获得了长上下文处理能力。预训练后，应&#x7528;**`YaRN`**&#x65B9;法进行上下文扩展，并执行两个额外的训练阶段，每个阶段包&#x542B;**`1000`**&#x6B65;，逐步将上下文窗口&#x4ECE;**`4K`**&#x6269;展&#x5230;**`32K`**，再扩展&#x5230;**`128K`**。**YaRN** 配置与 **DeepSeek-V2** 一致，仅应用于解耦的共享键 k。训练阶段超参数：

> **第一阶段**：序列长度&#x4E3A;**`32K`**，批量大小&#x4E3A;**`1920`**
>
> **第二阶段**：序列长度增加&#x5230;**`128K`**，批量大小减少&#x5230;**`480`**

两个阶段的超参数保持相同：$s=40，α=1，β=32$，缩放因子$\sqrt{t}=0.1\ln s + 1$。学习率均设置为$7.3×10^{-6}$，与预训练阶段的最终学习率匹配。经过这两个阶段的扩展训练，**DeepSeek-V3&#x20;**&#x80FD;够处理长度高达 **128K&#x20;**&#x7684;输入，同时保持强大的性能。在大海捞针测试中，**DeepSeek-V3&#x20;**&#x5C55;示了显著的性能提升，表明其在长达 **128K** 的上下文窗口长度上具有一致的鲁棒性。

* **后训练**

**监督微调 SFT**

**DeepSeek&#x20;**&#x7CBE;心策划了指令微调数据集，其中包含跨多个领域&#x7684;**`1.5M`**&#x4E2A;实例，每个领域都采用针对其特定需求量身定制的独特数据创建方法。

> 1. **推理数据**：对于与推理相关的数据集，包括那些专注于数学、代码竞赛问题和逻辑谜题的数据集，通过利用内部的 **DeepSeek-R1** 模型来生成数据。虽然 **R1** 生成的数据表现出很高的准确性，但它存在诸如过度思考、格式不佳和长度过长等问题。目标是平衡 **R1** 生成的推理数据的高准确性以及格式规范的推理数据的清晰度和简洁性。
>
> 2. **非推理数据**：例如创意写作、角色扮演和简单问答，利用 **DeepSeek-V2.5** 生成回复，并聘请人工标注员来验证数据的准确性和正确性。

**DeepSeek-V3** 使用 **SFT** 数据集对 **DeepSeek-V3-Base** 进行两个 epoch 的微调，采用余弦衰减学习率调度，初始学习率为$5 \times 10^{-6}$，并逐渐降低至$1 \times 10^{-6}$。在训练过程中，每个单独的序列由多个样本打包而成。**DeepSeek&#x20;**&#x91C7;用样本掩码策略，以确保这些示例保持隔离且相互不可见。

**奖励模型 RM**

在强化学习过程中，采用了基于规则的奖励模型和基于模型的奖励模型：

1. **基于规则的奖励模型**

对于可以使用特定规则验证的问题，采用基于规则的奖励系统来确定反馈。例如，某些数学问题具有确定的结果，则要求模型在指定的格式内提供最终答案，从而允许应用规则来验证其正确性。同样，对于 LeetCode 问题，利用编译器根据测试用例生成反馈。通过尽可能利用基于规则的验证，确保了更高水平的可靠性，因为这种方法不易受到操纵或利用。

* **基于模型的奖励模型**

对于具有自由形式的真实答案的问题，依赖奖励模型来确定响应是否与预期的真实答案匹配。相反，对于没有明确的真实答案的问题，例如涉及创意写作的问题，奖励模型则负责根据问题和相应的答案提供反馈。奖励模型以问题和答案作为输入，并从 **DeepSeek-V3 SFT** 检查点进行训练。为了提高其可靠性，**DeepSeek&#x20;**&#x6784;建了偏好数据，这些数据不仅提供最终奖励，还包括导致奖励的思维链。这种方法有助于降低特定任务中奖励被黑客攻击的风险。

**群组相对策略优化 GRPO**

与 **DeepSeek-V2&#x20;**&#x7C7B;似，采用了群组相对策略优化 **GRPO**，它放弃了通常与策略模型大小相同的评论家模型，而是从群组得分中估计基线。具体来说，对于每个问题$q$，**GRPO** 从旧的策略模型$\pi_{\text{old}}$中采样一组输出$\{o_1, o_2, \cdots, o_G\}$，然后通过最大化以下目标来优化策略模型$\pi_b$：

$$\mathcal{J}_\text{GRPO}(\theta)=\mathbb{E}\left[q \sim P(Q),\left\{o_i\right\}_{i=1}^G \sim \pi_{\theta_\text{old}}(O \mid q)\right]$$

$$ \\ \frac{1}{G} \sum_{i=1}^G\left(\min \left(\frac{\pi_\theta\left(o_i \mid q\right)}{\pi_{\theta_\text{old}}\left(o_i \mid q\right)} A_i, \operatorname{clip}\left(\frac{\pi_\theta\left(o_i \mid q\right)}{\pi_{\theta_\text{old}}\left(o_i \mid q\right)}, 1-\varepsilon, 1+\varepsilon\right) A_i\right)-\beta \mathbb{D}_\text{KL}\left(\pi_\theta \| \pi_\text{ref}\right)\right)$$

$$\mathbb{D}_\text{KL}\left(\pi_\theta \| \pi_\text{ref}\right)=\frac{\pi_\text{ref}\left(o_i \mid q\right)}{\pi_\theta\left(o_i \mid q\right)}-\log \frac{\pi_\text{ref}\left(o_i \mid q\right)}{\pi_\theta\left(o_i \mid q\right)}-1$$

其中$\epsilon$和$\beta$是超参数，$\pi_{\text{ref}}$是参考模型，$A_i$是优势，它来自每个组内输出对应的奖励$\{r_1, r_2, \cdots, r_G\}$：

$$A_i=\frac{r_i-\operatorname{mean}\left(\left\{r_1, r_2, \cdots, r_G\right\}\right)}{\operatorname{std}\left(\left\{r_1, r_2, \cdots, r_G\right\}\right)}$$

在强化学习过程中，**DeepSeek-V3&#x20;**&#x878D;入了来自不同领域的提示词，例如编码、数学、写作、角色扮演和问答。这种方法不仅使模型更贴近人类偏好，而且还提高了在基准测试中的性能，尤其是在可用的监督微调数据有限的情况下。

### 4.5.5 DeepSeek-R1
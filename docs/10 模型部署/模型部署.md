
主流模型部署框架总览

本文概览大模型/多模态模型的主流部署方案，并给出各框架的特点、适用场景与快速上手示例。重点关注吞吐、时延、显存占用与易用性，帮助你在本地开发、单机生产与集群场景中做出取舍。


# vLLM

> Paper: https://arxiv.org/abs/2309.06180
>
> Github: https://github.com/vllm-project/vllm
>
> 官网: https://docs.vllm.ai/en/latest/

**vLLM**（Vectorized Large Language Model Serving System）由伯克利等团队发起，核心通过 PagedAttention 与连续批处理提升吞吐并降低显存碎片，提供开箱即用的 OpenAI API 兼容服务。

- 核心特性：
  - 高吞吐与低碎片：PagedAttention 管理 KV 缓存，配合连续批处理与动态调度
  - API 友好：原生 OpenAI API 兼容（/v1/chat/completions、/v1/completions、/v1/embeddings 等）
  - 并行与扩展：支持张量并行、分布式推理；支持流式输出、推测解码、提示缓存
  - 量化支持：FP16/BF16/FP8、bitsandbytes INT8/INT4、GPTQ/AWQ 等常见方案
  - 模型生态：兼容 Hugging Face，多数热门 Llama/Qwen/DeepSeek/Mistral/Gemma 等模型可直接加载

- 适用场景：
  - 单机/多卡高吞吐问答与嵌入服务
  - 需要快速落地 OpenAI 兼容接口的应用后端

- 快速开始：
  1) 安装
  ```bash
  pip install vllm
  ```
  2) 启动 OpenAI 兼容服务（示例：Qwen2.5-7B-Instruct）
  ```bash
  python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-7B-Instruct \
    --served-model-name qwen2.5-7b \
    --max-model-len 4096 \
    --host 0.0.0.0 --port 8000
  ```
  3) 调用
  ```bash
  curl http://localhost:8000/v1/chat/completions \
    -H 'Content-Type: application/json' \
    -d '{
      "model": "qwen2.5-7b",
      "messages": [{"role": "user", "content": "用一句话介绍 vLLM"}]
    }'
  ```

- 性能要点：
  - 合理设置 `--max-model-len` 与并发请求的生成长度，避免 KV 缓存抖动
  - 批大小与并发请求要匹配业务峰值，长文本建议启用流式返回
  - 尝试开启推测性解码、FP8（H100）或 INT4/INT8 量化以增大吞吐

参考：本仓库下亦有针对具体模型的 vLLM 调用示例，可查看如 `self-llm/models/Qwen2.5/03-Qwen2.5-7B-Instruct vLLM 部署调用.md`。


# SGLang

Github: https://github.com/sgl-project/sglang

官网: https://docs.sglang.ai/

SGLang 是为大语言模型与视觉语言模型打造的高效服务框架，强调“后端运行时 + 前端语言”的协同：

- 核心特性：
  - 快速运行时：内置 RadixAttention 前缀缓存、预填充-解码分离、连续批处理、分页注意力
  - 多种并行：张量并行、流水线并行、专家并行、数据并行；多 LoRA 批次处理
  - 量化兼容：FP4/FP8/INT4/AWQ/GPTQ 等；支持结构化输出与分块预填充
  - 前端语言：提供可编排的高层 API，易于表达链式调用、控制流与并行
  - 模型覆盖：Llama、Qwen、DeepSeek、Gemma、Mistral、GPT 系列及多模态/嵌入/奖励模型

- 适用场景：
  - 需要在服务端精细编排复杂推理流程（函数/工具调用、并行、控制流）
  - 多 LoRA 场景与多模态推理

- 快速开始：
  1) 安装
  ```bash
  pip install "sglang[all]"
  ```
  2) 启动服务（OpenAI 兼容）
  ```bash
  sglang serve \
    --model Qwen/Qwen2.5-7B-Instruct \
    --host 0.0.0.0 --port 30000
  ```
  3) 调用
  ```bash
  curl http://localhost:30000/v1/chat/completions \
    -H 'Content-Type: application/json' \
    -d '{
      "model": "Qwen/Qwen2.5-7B-Instruct",
      "messages": [{"role": "user", "content": "用一句话介绍 SGLang"}]
    }'
  ```

- 性能要点：
  - 将“预填充与解码”分离可在长提示业务下明显提升吞吐
  - RadixAttention 的前缀共享对多用户相似提示非常友好


# Text Generation Inference (TGI)

Github: https://github.com/huggingface/text-generation-inference

官网: https://huggingface.co/docs/text-generation-inference/

Hugging Face 推出的生产级文本生成推理服务，提供容器化发布、动态批处理与高效内核，广泛用于 Inference Endpoints 以及本地/私有化部署。

- 核心特性：
  - 生产可用：稳定的 REST/gRPC 服务，支持流式输出（SSE）与并发控制
  - 高效推理：动态批处理、优化内核（如可用时的 FlashAttention）、张量并行
  - 量化支持：多种量化方案（如 bitsandbytes 4/8-bit、AWQ、GPTQ，视模型/版本而定）
  - 生态融合：与 Hugging Face Hub、safetensors 紧密集成，易于拉起与更新

- 适用场景：
  - 需要标准化容器/DevOps 体验（Docker/K8s），并与 HF 生态深度集成
  - 对 OpenAI 协议无强依赖，或可通过代理层适配 OpenAI 接口

- 快速开始：
  1) 使用 Docker 启动（示例：Mistral 7B Instruct）
  ```bash
  docker run --gpus all --rm -p 8080:80 \
    -e HF_TOKEN=your_hf_token \
    ghcr.io/huggingface/text-generation-inference:latest \
    --model-id mistralai/Mistral-7B-Instruct-v0.2 \
    --num-shard 1
  ```
  2) 同步生成
  ```bash
  curl -s http://localhost:8080/generate \
    -H 'Content-Type: application/json' \
    -d '{
      "inputs": "用一句话介绍 TGI",
      "parameters": {"max_new_tokens": 64, "temperature": 0.7}
    }'
  ```
  3) 流式生成（SSE）
  ```bash
  curl -N http://localhost:8080/generate_stream \
    -H 'Content-Type: application/json' \
    -d '{
      "inputs": "流式介绍一下 TGI 的特点",
      "parameters": {"max_new_tokens": 64, "temperature": 0.7}
    }'
  ```

- 性能要点：
  - 通过 `--num-shard` 开启张量并行；结合 `--max-input-tokens`、`--max-total-tokens` 控制内存
  - 模型与量化方式需匹配：不同模型的 AWQ/GPTQ/4-bit 兼容性以官方与模型卡说明为准


# TensorRT-LLM

> Github: https://github.com/NVIDIA/TensorRT-LLM
>
> 官网: https://nvidia.github.io/TensorRT-LLM/

NVIDIA 面向自家 GPU 的高性能推理库与工具链，可将模型编译为高效的 TensorRT 引擎；支持 FP8/BF16/FP16 与 INT8/INT4 量化，提供极致吞吐与低时延，并可与 Triton Inference Server 集成。

- 核心特性：
  - 最优性能：充分利用 Tensor Cores、KV 缓存优化与 CUDA Graphs
  - 部署生态：可输出引擎交给 Triton；企业可选 NIM 微服务
  - 模型广泛：Llama、Qwen、Mixtral、Phi、Whisper、通用 Transformer 等

- 适用场景：
  - 对吞吐/时延有极致要求的生产集群
  - 明确运行在 NVIDIA GPU 上，愿意投入引擎构建与算子校准

- 快速开始（示例）：
  - 通过预置脚本构建引擎，或使用官方容器
  ```bash
  docker run --gpus all -it --rm nvcr.io/nvidia/tensorrtllm:latest bash
  # 容器内：构建并运行示例
  ```
  - 与 Triton 集成后导出 OpenAI 兼容或自定义 HTTP/gRPC 服务

- 注意：
  - 仅支持 NVIDIA GPU；引擎构建与量化配置需要一定工程经验


# Ollama

官网: https://ollama.com/

Ollama 面向本地与边缘设备的“即装即用”模型运行时，提供统一模型打包/分发格式与简单的 CLI/API，适合开发调试与轻量级服务。

- 核心特性：
  - 开箱易用：一条命令拉起并运行流行开源模型
  - 跨平台：macOS（Metal）、Linux、Windows；可用 CPU/GPU
  - OpenAI 兼容：暴露 /v1/chat/completions 等接口，便于接入现有应用

- 快速开始：
  ```bash
  # 安装后
  ollama run llama3:8b
  # API 调用（默认 11434 端口）
  curl http://localhost:11434/v1/chat/completions \
    -H 'Content-Type: application/json' \
    -d '{
      "model": "llama3:8b",
      "messages": [{"role":"user","content":"你好！"}]
    }'
  ```

- 适用场景：
  - 本地快速验证、桌面应用嵌入、小规模内网服务
  - 需要非常简洁的一体化体验而非极致吞吐


# XInference

Github: https://github.com/xorbitsai/inference

官网: https://inference.readthedocs.io/

XInference（Xinference）是面向多模型形态的统一推理框架，支持 LLM、VLM、ASR、TTS 等，提供 Web UI、OpenAI 兼容 API 与分布式部署能力。

- 核心特性：
  - 多模型类型与后端；可选启用 vLLM 等高性能后端
  - 控制台与 API 统一管理模型生命周期
  - 分布式与多节点调度支持

- 快速开始：
  ```bash
  pip install xinference
  xinference --host 0.0.0.0 --port 9997
  # 打开 Web 控制台按需部署模型，或使用 CLI/API 管理
  ```

# 如何选择

- 吞吐优先：
  - 优先 vLLM 与 SGLang；NVIDIA 集群追求极致可选 TensorRT-LLM(+Triton)
- 易用性/本地优先：
  - 首选 Ollama；需要统一管理与多模型类型可选 XInference
- HF 生态/容器优先：
  - 选 TGI（Docker/K8s 友好，REST/gRPC 稳定，流式与并发控制完善）
- 复杂编排：
  - SGLang 的前端语言有优势，便于表达控制流与并行
- 多 LoRA/长提示：
  - SGLang 的 RadixAttention、vLLM 的 PagedAttention/提示缓存可显著受益
- 硬件限制：
  - NVIDIA CUDA 环境体验最佳；部分框架对 ROCm/CPU 有实验/有限支持（以官方文档为准）


# 实用建议

- 模型与精度：
  - 生产线上优先 BF16/FP16；H100 可评估 FP8；在可接受精度下尝试 INT4/INT8 量化提升吞吐
- KV 缓存与长度：
  - 控制 `max_model_len` 与生成长度，减少显存抖动与 OOM；长对话建议流式输出
- 并发与批处理：
  - 根据 QPS 选择连续批处理策略；将相似提示合并可受益于前缀共享
- 监控与回放：
  - 接入 Prometheus/OTEL，保留请求/响应与参数以便性能调优


# 相关参考

- 本仓库内的 vLLM 实战示例（如 `self-llm/models/Qwen2.5/03-Qwen2.5-7B-Instruct vLLM 部署调用.md`）
- vLLM 文档与示例：https://docs.vllm.ai/
- SGLang 文档与示例：https://docs.sglang.ai/
- TGI 文档与容器：https://huggingface.co/docs/text-generation-inference/
- TensorRT-LLM 文档与容器：https://nvidia.github.io/TensorRT-LLM/
- Ollama 文档与模型库：https://ollama.com/library
- XInference 文档：https://inference.readthedocs.io/

# Prefix-Tuning

**原文：**Prefix-Tuning: Optimizing Continuous Prompts for Generation

**代码：**[https://github.com/XiangLi1999/PrefixTuning.git](https://github.com/XiangLi1999/PrefixTuning.git)

## **摘要**

微调是利用大型预训练语言模型进行下游任务的实际方法。然而，微调修改了所有语言模型参数，因此需要为每个任务存储一个完整的副本。在本文中，我们提出了前缀微调（prefix-tuning），这是一种用于自然语言生成任务的轻量级替代方法，它保持语言模型参数冻结，并优化一系列连续的任务特定向量，我们称之为前缀。前缀微调受提示语言模型的启发，允许后续标记将其视为“虚拟标记”来关注这个前缀。我们将前缀微调应用于 GPT-2 用于表格文本生成和应用于 BART 用于摘要生成。我们展示了通过仅修改 0.1% 的参数，前缀微调在完整数据设置中获得了可比较的性能，在低数据设置中优于微调，并且更好地推广到在训练期间未见过的主题示例。

## 1 Introduction

当今主流范式是利用大型预训练语言模型（LMs）（Radford等，2019年；Devlin等，2019年）进行微调，以执行下游任务，但这需要更新和存储LM的所有参数。因此，要构建和部署依赖大型预训练LM的NLP系统，目前需要为每个任务存储LM参数的修改副本。考虑到当前LM的规模，这可能成本过高；例如，GPT-2具有774M个参数（Radford等，2019年），GPT-3具有175B个参数（Brown等，2020年）。

针对这个问题的一个自然方法是轻量级微调，它冻结大部分预训练参数，并只调整一小部分参数。例如，适配器微调（adapter-tuning）（Rebuffi等，2017年；Houlsby等，2019年）在预训练语言模型的层之间插入额外的任务特定层。适配器微调在自然语言理解和生成基准测试中表现出有希望的性能，在增加约2-4%的任务特定参数的情况下，达到了与微调相媲美的性能（Houlsby等，2019年；Lin等，2020年）。

在极限情况下，可以使用上下文学习（in-context learning）部署GPT-3（Brown等，2020年），这是一种提示形式，而无需修改任何LM参数。在上下文学习中，Brown等人（2020年）在任务输入之前添加了自然语言任务说明（例如，用于摘要的TL;DR）和一些示例，然后从LM生成任务输出。然而，由于Transformer模型只能对有界长度的上下文进行条件化（例如，对于GPT-3为2048个标记），上下文学习限制在非常小的训练集上。

在本文中，我们提出了前缀微调（prefix-tuning），这是一种针对自然语言生成（NLG）任务的轻量级微调替代方法，受提示方法的启发。考虑生成数据表的文本描述的任务，如图1所示，其中任务输入是线性化的表格（例如，“name: Starbucks | type: coffee shop”），输出是文本描述（例如，“Starbucks serves coffee.”）。前缀微调将一系列连续的任务特定向量（我们称之为前缀）添加到输入之前，如图1（底部）中红色块所示。为了生成每个标记，语言模型可以像处理一系列“虚拟标记”一样关注前缀，但与提示不同，前缀完全由不对应于真实标记的自由参数组成。与图1中微调（顶部）相比，微调更新了所有LM参数，因此需要为每个任务存储一个经调整的模型副本，而前缀微调仅优化前缀。因此，我们只需存储一个大型LM的副本和一个学习得到的任务特定前缀，对于每个额外任务的开销非常小（例如，对于表格文本生成，250K个参数）。

![Untitled](Prefix-Tuning%201586c178b89f40b1ba5def77b6c9b484/Untitled.png)

> 图1：微调（顶部）更新所有LM参数（红色Transformer框），并需要为每个任务存储完整的模型副本。我们提出了前缀微调（底部），它冻结LM参数并只优化前缀（红色前缀块）。因此，我们只需为每个任务存储前缀，使得前缀微调具有模块化和高效利用空间的特点。请注意，每个垂直块表示一个时间步长处的Transformer激活。
> 

与完全微调相比，前缀微调也具有模块化的特点：我们训练一个上游前缀来引导一个未修改的LM，因此，单个LM可以同时支持多个任务。在任务对应于用户的个性化背景下（Shokri和Shmatikov，2015年；McMahan等，2016年），我们为每个用户单独训练一个前缀，仅基于该用户的数据进行训练，从而避免数据交叉污染。此外，基于前缀的架构使我们甚至可以在单个批处理中处理来自多个用户/任务的示例，这是其他轻量级微调方法（例如适配器微调）所无法做到的。

我们使用GPT-2进行表格文本生成和使用BART进行摘要生成，评估了前缀微调的性能。在存储方面，与完全微调相比，前缀微调存储的参数数量少了1000倍。在使用完整数据集进行训练时，对于表格文本生成（§6.1），前缀微调和微调的性能相当，而对于摘要生成，前缀微调在性能上略有下降（§6.2）。在低数据设置下，前缀微调在两个任务上均优于微调（§6.3）。此外，前缀微调对于表格文本生成中的未见主题的表格以及摘要生成中的未见主题的文章具有更好的外推性能（§6.4）。

## 2 Related Work

**Fine-tuning for natural language generation.** 当前最先进的自然语言生成系统是基于对预训练语言模型进行微调。对于表格文本生成，Kale（2020年）对序列到序列模型（T5；Raffel等，2020年）进行微调。对于抽取式和生成式摘要，研究人员分别对遮蔽语言模型（例如BERT；Devlin等，2019年）和编码-解码模型（例如BART；Lewis等，2020年）进行微调（Zhong等，2020年；Liu和Lapata，2019年；Raffel等，2020年）。对于其他条件性NLG任务，例如机器翻译和对话生成，微调也是主流范式（Zhang等，2020c年；Stickland等，2020年；Zhu等，2020年；Liu等，2020年）。在本文中，我们专注于使用GPT-2进行表格文本生成以及使用BART进行摘要生成，但原则上，前缀微调可以应用于其他生成任务和预训练模型，例如遮蔽语言模型。

**Lightweight fine-tuning.** 前缀微调属于轻量级微调方法的广泛类别，它冻结了大部分预训练参数，只调整了一小部分参数。关键问题是如何扩充LM架构并决定要调整哪些预训练参数的子集。一个研究方向是学习一个任务特定的参数掩码（Zhao等，2020年；Radiya-Dixit和Wang，2020年）。另一个研究方向是插入具有可训练参数的新模块。例如，Zhang等人（2020a年）训练了一个“辅助”网络，通过求和与预训练模型融合；适配器微调在预训练LM的每一层之间插入任务特定层（适配器）（Houlsby等，2019年；Lin等，2020年；Rebuffi等，2017年；Pfeiffer等，2020年）。与这一研究方向相比，这些方法调整了大约LM参数的3.6%，而我们的方法在维持表格文本任务可比性能的同时，进一步减少了30倍的任务特定参数，仅调整了0.1%的参数。

**Prompting.** 提示（Prompting）是一种利用预训练语言模型的方法，通过在任务输入之前添加说明和一些示例，并从语言模型生成任务输出。对于自回归语言模型，最成功的提示形式是GPT-3的上下文学习（Brown等，2020年），它使用手动设计的提示来适应少量示例下不同任务的生成。对于像BERT和RoBERTa（Liu等，2019年）这样的遮蔽语言模型，提示工程已经在自然语言理解任务中进行了探索（Jiang等，2020年；Schick和Schütze，2020年）。例如，AutoPrompt（Shin等，2020年）搜索一系列离散触发词，并将其与每个输入连接起来，从BERT和RoBERTa中引发情感或事实知识。与AutoPrompt不同，我们的方法优化连续的前缀，这更具表现力（§7.2）；此外，我们专注于语言生成任务。

连续向量已被用于引导语言模型；例如，Subramani等人（2020年）展示了预训练的LSTM语言模型可以通过为每个句子优化一个连续向量来重构任意句子，使得该向量适用于特定输入。相比之下，前缀微调优化了一个适用于该任务的特定前缀，适用于该任务的所有实例。因此，与先前的工作仅限于句子重构不同，前缀微调可以应用于自然语言生成任务。

**Controllable generation.** 可控生成旨在引导预训练语言模型匹配句子级属性（例如，积极情绪或体育）。这种控制可以发生在训练时：Keskar等人（2019年）对语言模型进行预训练（CTRL），以使其依赖于诸如关键词或URL等元数据。控制也可以发生在解码时，通过加权解码（GeDi，Krause等，2020年）或通过迭代更新过去的激活状态（PPLM，Dathathri等，2020年）。然而，目前还没有直接的方法将这些可控生成技术应用于强制对生成内容进行细粒度控制，正如表格文本生成和摘要等任务所要求的那样。

**P*-tuning.** 前缀微调是一种新类方法的实例，这一类方法被我们称为p*-tuning（因为其他显著的实例，如ptuning和prompt-tuning，也以p开头），这些方法都基于优化连续前缀或提示的思想。与我们的工作同时进行的是，Qin和Eisner（2021年）学习软填空提示的混合物，以从BERT和BART等语言模型中引发知识。Hambardzumyan等人（2021年）学习了适用于情感分类的任务特定嵌入，适应了BERT。这两项工作都表明，调整软提示的性能优于优化离散提示的先前工作。P-tuning（Liu等人，2021年）表明，联合更新提示嵌入和LM参数可以提高GPT-2在自然语言理解任务中的性能，无论是在少样本还是完整数据设置下。在后续工作中，Prompt-tuning（Lester等人，2021年）简化了我们的方法，并将其应用于T5（Raffel等人，2020年），展示出随着模型规模增大，微调与p*tuning之间的性能差距逐渐消失。

![Untitled](Prefix-Tuning%201586c178b89f40b1ba5def77b6c9b484/Untitled%201.png)

> 图2显示了前缀微调的示例，分为两个部分：顶部是自回归语言模型的示例，底部是编码器-解码器模型的示例。在示例中，前缀激活 $∀i ∈ P_{idx}$，表示为  $h_i$，来自于可训练的矩阵 $P_θ$。剩余的激活由Transformer计算生成。
> 

## 3 Problem Statement

考虑一个条件生成任务，其中输入$x$是一个上下文，输出$y$是一个标记序列。我们关注两个任务，如图2（右侧）所示：在表格文本生成中，$x$对应于一个线性化的数据表，而$y$是一个文本描述；在摘要生成中，$x$是一篇文章，$y$是一个摘要。

### 3.1 Autoregressive LM

假设我们有一个由参数$φ$（例如，GPT-2；Radford等人，2019年）参数化的自回归神经语言模型 $p_φ(y | x)$。如图2（顶部）所示，设 $z = [x; y]$ 是 $x$  和 $y$ 的连接；设 $X_{idx}$  表示对应于 $x$ 的索引序列，$Y_{idx}$ 表示对应于 $y$ 的索引序列。

在时间步 $i$  处的激活向量为 $h_i ∈ R^d$，其中 $h_i = [h^{(1)}_i ; \ · · · \ ; h^{(n)}_i]$ 是该时间步中所有激活层的连接，$h^{(j)} _i$ 是时间步 $i$ 处第 $j$ 层的激活向量。

一个自回归神经语言模型计算 $h_i$ 如下所示，作为 $z_i$ 和其左侧上下文中过去激活的函数：

\[
h_i = LM_φ(z_i, h_{<i})
\]

在这种情况下，$h_i$ 的最后一层用于计算下一个标记的分布：$p_φ(z_{i+1} | h_{≤i}) = softmax(W_φ h^{(n)}_i)$，其中 $W_φ$  是一个将 $h^{(n)}_i$  映射到词汇表上 logits 的矩阵。

### 3.2 Encoder-Decoder Architecture

我们也可以使用编码器-解码器架构（例如，BART；Lewis等人，2020年）来建模 $p_φ(y | x)$，其中 $x$ 由双向编码器编码，解码器自回归地预测 $y$（在编码后的 $x$  和其左上下文的条件下）。我们使用相同的索引和激活符号表示法，如图2（底部）所示：对于 $i ∈ X_{idx}$，每个 $h_i$ 由双向编码器计算；对于 $i ∈ Y_{idx}$，每个 $h_i$ 由一个自回归解码器使用相同的方程（1）进行计算。

### 3.3 Fine-tuning

在完整微调框架中，我们使用预训练的参数$φ$进行初始化。这里 $p_φ$ 是一个可训练的语言模型分布，我们对以下对数似然目标执行梯度更新：

\[
\max_φ log p_φ(y|x) = \max_φ∑_{i∈Y_{idx}}log p_φ(z_i | h_{<i})
\]

## 4 Prefix-Tuning

我们提出前缀微调作为条件生成任务的一种替代方法，与完整微调相比。在第4.1节中，我们首先提供直觉理解，然后在第4.2节正式定义我们的方法。

### 4.1 Intuition

提示已经证明，在不改变语言模型参数的情况下，通过在正确的上下文条件下对LM进行控制是可行的。例如，如果我们想让LM生成一个词（例如，Obama），我们可以将其常见的搭配作为上下文（例如，Barack），这样LM将会给所需的词分配更高的概率。将这种直觉扩展到生成单个词或句子之外，我们希望找到一个上下文，可以引导LM解决一个NLG任务。直觉上，上下文可以通过指导从任务输入$x$ 中提取什么来影响任务输入$x$ 的编码，它还可以通过引导下一个标记的分布来影响任务输出y的生成。然而，目前尚不清楚是否存在这样的上下文。使用自然语言任务指令（例如，“用一句话总结以下表格”）作为上下文可能会指导人类完成任务，但对于中等规模的预训练LM来说，这种方法并不有效。尝试优化离散指令可能有所帮助，但离散优化在计算上具有挑战性。

与优化离散标记不同，我们可以优化指令作为连续的单词嵌入（word embeddings），其效果将向上传播到所有Transformer激活层，并向右传播到后续的标记。这比被限制在实际单词嵌入上的离散提示更具表现力。前缀微调通过优化所有层的激活，而不仅仅是嵌入层，进一步增加了表现力。作为另一个好处，前缀微调可以直接修改网络中更深层次的表示，因此避免了网络深度中的长计算路径。

### 4.2 Method

前缀微调为自回归语言模型添加前缀以获取 $z = [PREFIX; x; y]$，或者为编码器和解码器均添加前缀以获取 $z = [PREFIX; x; PREFIX′; y]$，如图2所示。这里，$P_{idx}$表示前缀索引序列，我们使用 $|P_{idx}|$ 表示前缀的长度。我们遵循方程（1）中的递推关系，不同之处在于前缀索引的激活是自由参数，由维度为 $|P_{idx}| × dim(h_i)$ 的矩阵 $P_θ$（由 θ 参数化）给出。

\[
hi = \left\{ Pθ[i, :], if i ∈ Pidx, LMφ(zi, h<i), otherwise.\right\}
\]

训练目标与方程（2）相同，但可训练参数集合发生变化：语言模型参数$φ$被固定，而前缀参数$θ$是唯一可训练的参数。在这里，每个 $h_i$ 都是可训练的 $P_θ$ 的函数。当 $i ∈ P_{idx}$ 时，这一点很明显，因为 $h_i$ 直接从 $P_θ$ 复制。当 $i \not \in P_{idx}$  时，$h_i$ 仍然依赖于 $P_θ$，因为前缀激活始终位于左上下文，并因此会影响右侧的任何激活。

### **4.3** Parametrization **of** $P_θ$

根据经验，直接更新 $P_θ$ 参数会导致优化不稳定和轻微性能下降。因此，我们通过一个较小的矩阵（$P'_θ$）与一个大型前馈神经网络（$MLP_θ$）组合，对矩阵 $P_θ$ 进行重新参数化得到$P_θ[i, :] = MLP_θ(P^′_θ[i, :])$。现在，可训练参数包括 $P'_θ$ 和 $MLP_θ$ 的参数。请注意，$P_θ$ 和 $P'_θ$ 具有相同的行数（即前缀长度），但列数不同。一旦训练完成，可以丢弃这些重新参数化参数，只需要保存前缀$P_θ$。

## 5 Experimental Setup

### 5.1 Datasets and Metrics

我们在表格文本任务的三个标准神经生成数据集上进行评估：E2E（Novikova等人，2017年），WebNLG（Gardent等人，2017年）和DART（Radev等人，2020年），如表1所示。这些数据集按照复杂性和规模递增的顺序排列。E2E仅涵盖1个领域（即餐厅评论）；WebNLG涵盖了14个领域；而DART是开放领域的，使用来自维基百科的开放领域表格。对于评估，我们使用官方评估脚本报告指标（详细信息请参见附录A.1）。

对于摘要任务，我们使用了XSUM（Narayan等人，2018年）数据集，这是一个关于新闻文章的抽象式摘要数据集。我们报告了ROUGE-1、ROUGE-2和ROUGE-L指标。

![Untitled](Prefix-Tuning%201586c178b89f40b1ba5def77b6c9b484/Untitled%202.png)

> 表 1：数据集统计。输入和输出长度是每个示例的 BPE 令牌数。对于三个表转文本数据集，输入长度是线性化表的长度（附录 A.1 中的详细信息）。
> 

### 5.2 Methods

对于表格文本生成任务，我们将前缀微调（prefix-tuning）与其他三种方法进行比较：完整微调（FT-FULL）、仅微调顶部2层（FTTOP2）和适配器微调（ADAPTER）。我们还报告了这些数据集上的当前最先进结果：在E2E上，Shen等人（2019年）使用了一个不需要预训练的实用模型。在WebNLG上，Kale（2020年）对T5-large进行了微调。在DART上，目前没有发布在此数据集版本上训练的官方模型。对于摘要任务，我们将与对BART（Lewis等人，2020年）进行微调进行比较。

### 5.3 Architectures and Hyperparameters

对于表格文本生成任务，我们使用了GPT-2MEDIUM和GPT2LARGE。对于摘要任务，我们使用了BARTLARGE。我们的实现基于Hugging Face Transformers（Wolf等人，2020年）。

在训练时，我们使用了AdamW优化器（Loshchilov和Hutter，2019年）和线性学习率调度器，这是根据Hugging Face的默认设置建议的。我们调整的超参数包括epoch数量、批处理大小、学习率和前缀长度。超参数的详细信息请参见附录。默认设置为10个epochs，批处理大小为5，学习率为$5 × 10^−5$，前缀长度为10。表格文本模型是在TITAN Xp或GeForce GTX TITAN X机器上训练的。前缀微调每个epoch在22K个示例上训练需要0.2小时，而微调则需要大约0.3小时。摘要模型是在Tesla V100机器上使用XSUM数据集进行训练，每个epoch需要1.25小时。在时间效率方面，前缀微调比微调快约30%。对于GPU内存效率，批处理大小为1的前缀微调占总GPU内存的18%，而微调占用50%。

在解码时，对于表格文本生成，我们使用束搜索（beam search）并设置束大小为5。对于摘要生成，我们使用束大小为6和长度归一化参数为0.8。对于表格文本生成，解码每个句子需要1.2秒（不使用批处理），而对于摘要生成，解码每个批次需要2.6秒（使用批处理大小为10）。

## 6 Main Results

### 6.1 Table-to-text Generation

我们发现，仅更新了0.1%的任务特定参数，前缀微调在表格文本生成方面非常有效，在性能上表现优于其他轻量级基准模型（ADAPTER和FT-TOP2），即使更新的参数量比这些模型少了30倍，同时也取得了与（完整）微调相当的性能。这种趋势适用于所有数据集：E2E、WebNLG和DART。

如果我们将前缀微调和适配器微调的参数数量匹配到0.1%，表2显示前缀微调明显优于适配器微调（0.1%），平均每个数据集提升了4.1 BLEU。即使与微调（100%）和适配器微调（3.0%）进行比较，后两者更新的参数数量比前缀微调多得多，前缀微调仍然实现了与这两个系统相当或更好的结果。这表明前缀微调在提高生成质量的同时，更加Pareto有效，显著减少了参数。

此外，在 DART 上获得良好的性能表明，前缀调整可以泛化到具有不同域和大量关系的表。我们将在§6.4中更深入地研究外推性能（即，对看不见的类别或主题的泛化）。

总之，前缀调整是使 GPT-2 适应表到文本生成的一种有效且节省空间的方法。在扩展到 GPT-2LARGE 时，它还保持了性能提升，这表明它有可能扩展到具有类似架构的更大模型，例如 GPT-3。

### 6.2 Summarization

正如表3所示，使用2%的参数，前缀微调的性能略低于微调（在ROUGE-L上为36.05与37.25）。而仅使用0.1%的参数，前缀微调的性能不如完整微调（35.05与37.25）。XSUM与三个表格文本数据集之间存在几个差异，这可能解释了为什么前缀微调在表格文本生成中具有比较优势：（1）XSUM的示例数量平均比三个表格文本数据集多4倍；（2）输入文章的平均长度比表格文本数据集的线性化表输入长17倍；（3）摘要比表格文本生成更复杂，因为它需要从文章中选择关键内容。

### 6.3 Low-data Setting

根据表格文本（§6.1）和摘要（§6.2）的结果，我们观察到当训练示例数量较小时，前缀微调具有比较优势。为了更系统地探索低数据设置，我们对完整数据集进行了子采样（对于表格文本是E2E，对于摘要是XSUM），以获得大小为{50, 100, 200, 500}的小型数据集。对于每个大小，我们随机采样了5个不同的数据集，并对2个训练随机种子进行了平均。因此，我们对每个低数据设置进行了10个模型的平均。

图 3（右）显示，除了需要更少的参数外，前缀调整在低数据状态下平均比微调高出 2.9 BLEU，但随着数据集大小的增加，差距会缩小。

从定性上讲，图 3（左）显示了在不同数据级别上训练的前缀调优和微调模型生成的 8 个示例。虽然这两种方法在低数据状态下都倾向于生成不足（缺少表内容），但前缀调整往往比微调更忠实。例如，微调 （100， 200）错误地声称客户评分较低，而真实评分为平均水平，而前缀调整 （100， 200） 会生成忠实于表的描述。

### 6.4 Extrapolation

现在，我们研究了表到文本和摘要的看不见的主题的外推性能。为了构建外推设置，我们拆分了现有数据集，以便训练和测试涵盖不同的主题。对于表转文本，WebNLG 数据集使用表主题进行标记。有 9 个类别出现在训练和开发中，表示为 SEEN，有 5 个类别仅在测试时出现，表示为 UNSEEN。因此，我们通过对 SEEN 类别的训练和对 UNSEEN 类别的测试来评估外推。为了进行汇总，我们构造了两个外推数据拆分：在新闻到体育方面，我们对新闻文章进行训练，对体育文章进行测试。在内部新闻中，我们对{世界、英国、商业}新闻进行培训，并对其余新闻类别（例如健康、科技）进行测试。

在表格转文本和摘要方面，在所有指标下，前缀调整的外推效果都优于微调，如表 4 和表 2 的“U”列（中间）所示。

我们还发现，适配器调优实现了良好的外推性能，可与前缀调优相媲美，如表 2 所示。这一共同趋势表明，保留LM参数确实对外推产生了积极影响。然而，前缀调整如何改善外推是一个悬而未决的问题，我们将在§8中进一步讨论这个问题。

![Untitled](Prefix-Tuning%201586c178b89f40b1ba5def77b6c9b484/Untitled%203.png)

![Untitled](Prefix-Tuning%201586c178b89f40b1ba5def77b6c9b484/Untitled%204.png)

![Untitled](Prefix-Tuning%201586c178b89f40b1ba5def77b6c9b484/Untitled%205.png)

## 7 Intrinsic Evaluation

我们比较了前缀调整的不同变体，以研究各种设计决策的影响。§7.1 研究了前缀长度的影响。§7.2 研究仅调整嵌入层，这更类似于调整离散提示。§7.3 比较了前缀和内缀，后者在 $x$ 和 $y$ 之间插入了可训练的激活。 §7.4 研究了各种前缀初始化策略的影响。§7.5 进一步研究了前缀调优的数据效率。

### 7.1 Prefix Length

较长的前缀意味着更多的可训练参数，因此具有更强的表达能力。图4显示，性能随着前缀长度的增加而增加，直到达到一个阈值（摘要为200，表格文本为10），然后会出现轻微的性能下降。超过阈值的前缀会导致较低的训练损失，但略差的测试性能，表明它们倾向于过度拟合训练数据。

![Untitled](Prefix-Tuning%201586c178b89f40b1ba5def77b6c9b484/Untitled%206.png)

### 7.2 Full vs Embedding-only

回想一下，在§4.1中，我们讨论了优化“虚拟令牌”的连续嵌入。我们实例化这个想法，并将其称为仅嵌入。单词嵌入是自由参数，其余的激活层由 Transformer 计算。表 5（上图）显示性能显著下降，表明仅调整嵌入层的表现力不足。

仅嵌入上限会提高离散提示优化的性能（Shin et al.， 2020），因为离散提示限制了嵌入层与真实单词的嵌入完全匹配。因此，我们有了这条不断增强的表现力链：离散提示<仅嵌入<前缀调整。

![Untitled](Prefix-Tuning%201586c178b89f40b1ba5def77b6c9b484/Untitled%207.png)

### 7.3 Prefix-tuning vs Infix-tuning

我们还调查了可训练激活在序列中的位置如何影响性能。在前缀微调中，我们将它们放置在序列的开头[PREFIX; x; y]。我们也可以将可训练激活放置在x和y之间（即[x; INFIX; y]），称之为中缀微调。表5（底部）显示中缀微调略逊于前缀微调。我们认为这是因为前缀微调可以影响到$x$ 和$y$ 的激活，而中缀微调只能影响到$y$ 的激活。

### 7.4 Initialization

我们发现，前缀的初始化方式在低数据设置中有很大的影响。随机初始化会导致性能低下且方差大。通过激活实词来初始化前缀可以显著改善生成，如图 5 所示。特别是，使用与任务相关的词（如“摘要”和“表到文本”）进行初始化比与任务无关的词（如“大象”和“除法”）获得的性能略好，但使用真实词仍然比随机词好。此外，在全数据设置下，初始化技巧没有影响，随机初始化会带来同样好的性能。

由于我们使用 LM 计算的实词激活来初始化前缀，因此这种初始化策略与前缀调优的理念一致，即尽可能保留预训练的 LM。

![Untitled](Prefix-Tuning%201586c178b89f40b1ba5def77b6c9b484/Untitled%208.png)

### 7.5 Data Efficiency

我们还通过比较前缀调整（无初始化技巧，又名随机初始化）和完全微调在 E2E 任务的 5 种不同数据尺度（10%、20%、40%、60% 和 80%）上的性能来研究它们的数据效率。图 6 显示，当使用超过 20% 的数据时，前缀调整比微调具有更好的性能。对于 10% 的数据规模，使用随机初始化进行前缀调整产生的性能与完全微调相当或略低，需要初始化技巧 （§6.3） 来提高这种低数据状态下的性能。

![Untitled](Prefix-Tuning%201586c178b89f40b1ba5def77b6c9b484/Untitled%209.png)

## 8 Discussion

我们将讨论前缀调优的几个有利属性和一些开放性问题。

**个性化**。正如我们在 §1 中所指出的，当有大量任务需要独立训练时，前缀调优是有利的。一个实际的设置是用户隐私（Shokri 和 Shmatikov，2015 年;McMahan 等人，2016 年）。为了保护用户隐私，需要对每个用户的数据进行分离，并为每个用户独立训练个性化模型。因此，每个用户都可以被视为一个独立的任务。如果有数百万用户，前缀调整可以扩展到此设置并保持模块化，从而通过添加或删除用户前缀来灵活地添加或删除用户，而不会造成交叉污染。

**跨用户批处理**。在相同的个性化设置下，前缀优化允许批处理不同用户的查询，即使它们由不同的前缀支持。当多个用户使用其输入查询云 GPU 设备时，将这些用户放在同一批次中具有计算效率。前缀调整使共享 LM 保持不变;因此，批处理需要一个简单的步骤，即在用户输入前面加上个性化前缀，并且所有剩余的计算都保持不变。相比之下，我们无法在适配器调整中跨不同用户进行批处理，因为适配器调整在共享 Transformer 层之间具有个性化的适配器。

这种批处理优势还有助于创建在同一任务上训练的多个前缀的高效集成（Lester 等人，2021 年）。

**前缀调谐的归纳偏置**。回想一下，微调会更新所有预训练参数，而前缀调整和适配器调整会保留它们。由于语言模型是在通用语料库上预训练的，因此保留 LM 参数可能有助于泛化到训练期间看不见的领域。与这种直觉一致，我们观察到前缀调优和适配器调优在外推设置中都有显着的性能提升 （§6.4）;然而，这些方法如何改善外推是一个悬而未决的问题。

虽然前缀调优和适配器调优都会冻结预训练参数，但它们会调整不同的参数集来影响 Transformer 的激活层。回想一下，前缀调整使 LM 保持不变，并使用前缀和预训练的注意力块来影响后续激活;适配器调谐在 LM 层之间插入可训练模块，直接将残差向量添加到激活中。此外，我们观察到，与适配器调优相比，前缀调优需要的参数要少得多，同时保持了相当的性能。我们认为参数效率的提高是因为前缀调优尽可能地保持预训练的 LM 完好无损，因此比适配器调优更能利用 LM。

Aghajanyan 等人（2020 年）最近的工作使用本征维度来表明存在一种低维重参数化，其微调与完全参数化一样有效。这就解释了为什么只需更新少量参数即可获得下游任务的良好准确性。我们的工作呼应了这一发现，表明通过更新一个非常小的前缀也可以获得良好的生成性能。然而，前缀调优不仅仅是关于可训练参数的大小，更重要的是要修改哪个参数子集。因此，探索其他轻量级微调方法，以实现更好的精度-尺寸权衡，这将是有趣的未来工作。
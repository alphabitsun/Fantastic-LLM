# LoRA

**原文：**LoRA： LOW-RANK ADAPTATION OF LARGE LAN-GUAGE MODELS

**代码:** [https://github.com/microsoft/LoRA](https://link.zhihu.com/?target=https%3A//github.com/microsoft/LoRA)

## 术语：

- low-rank 低秩：
    
    通常指的是矩阵的秩(Rank)比较小，即该矩阵包含的信息量比较少，可以被较少的特征向量或奇异值所表示。在很多应用中，低秩矩阵可以用来表示数据中的共性信息和规律，从而实现数据的压缩、降维和分析等目的。
    
- rank-deficiency 秩亏:
    
    在线性代数中，"rank-deficiency"(秩亏)指的是一个矩阵的秩(Rank)比它的行数或列数小，即矩阵中存在线性相关的行或列，从而导致矩阵的行列式为零，不能被逆运算。通俗地说，秩亏矩阵的行或列中存在冗余信息，可以由其他行或列表示出来，因此该矩阵包含的信息量比较少，不能完全描述数据的特征。秩亏矩阵在实际应用中很常见，例如在图像处理和机器学习中，数据矩阵中可能存在噪声、缺失值或冗余特征，导致矩阵秩降低。在这种情况下，我们需要采用一些方法来处理秩亏矩阵，以提高数据的表现力和预测准确率。常用的秩亏矩阵处理方法包括主成分分析(PCA)、奇异值分解(SVD)、核矩阵分解(Kernel Matrix Factorization)、低秩矩阵补全(Low-Rank Matrix Completion)等。这些方法可以用来降维、去除噪声、填补缺失值或提取数据的共性信息等，从而提高数据的表现力和预测准确率。
    
- full rank 满秩：
    
    满秩矩阵是指矩阵的秩（Rank）等于它的行数或列数中较小值的矩阵。满秩矩阵中的所有行或列都是线性独立的，可以唯一地表示出矩阵的所有元素。满秩矩阵通常具有良好的性质和应用，例如可逆性、有唯一解性和唯一表示性等。满秩矩阵在实际应用中也很常见，例如在线性回归、最小二乘法和矩阵分解等问题中，通常需要满秩条件来保证算法的有效性和稳定性。如果一个矩阵不是满秩矩阵，那么它就是秩亏矩阵（Rank-deficient Matrix），其中至少存在一个线性相关的行或列。秩亏矩阵的性质和应用通常会受到一定的限制和影响，因此在处理实际问题时需要注意矩阵的秩和满秩性质。
    
- Intrinsic dimension 内在维度：
    
    内在维度是指数据集中所包含的最少信息所需要的维数，或者说是数据集中实际独立变化的方向的数量。它是指数据集固有的本质维数，不受数据在高维空间中的嵌入方式或表示方式的影响。具体来说，如果一个数据集可以被嵌入到一个 $d$ 维空间中，而且在这个空间中，数据集的每个样本点只需要 $k$ 个坐标来描述，那么数据集的内在维数就是 $k$ 。内在维度对于数据分析和机器学习来说是非常重要的，因为它可以帮助我们更好地理解数据集的本质结构，并且可以减少计算和存储的开销。例如，当处理高维数据时，内在维度可以帮助我们找到数据中真正有用的特征，从而提高模型的性能和准确度。另外，内在维度还可以用于数据压缩、可视化和降维等领域。
    
- Intrinsic rank(内在秩)
    
    Intrinsic rank(内在秩)是指一个数据集中最重要的线性无关的特征的数量，它可以被看作是数据集的内在维度的一个度量。内在秩的概念通常用于描述低秩矩阵近似问题中，其中我们希望通过一个低秩矩阵来近似一个高秩矩阵,以减少存储和计算的开销。具体来说,如果一个数据集可以表示为一个秩为 $k$ 的矩阵的线性组合,而且这个 $k$ 是数据集中最重要的线性无关特征的数量，那么这个数据集的内在秩就是 $k$ 。换句话说，内在秩是指能够用来描述数据集最重要信息的最少的线性无关特征数量。内在秩和内在维度的概念有些类似，但内在秩更多地关注于矩阵的结构和线性无关特征的数量，而内在维度则更多地关注于数据集的实际维度和独立变化方向的数量。在某些情况下，内在秩和内在维度可能是相同的，但在其他情况下，它们可能会有所不同。
    

## 优点：

- 一个预训练好的模型可以被共享，用来为不同的任务建立许多小的LoRA模块。
- LoRA使训练更加有效，在使用自适应优化器时，硬件门槛降低了3倍，因为我们不需要计算梯度或维护大多数参数的优化器状态。相反，我们只优化注入的、小得多的低秩矩阵。例如：checkpoint的大小大约减少了10000x(从350GB到35MB)，用更少的GPU进行训练，避免l/O瓶颈；可以在部署时以更低的成本切换任务，只交换LoRA的权重，而不是所有的参数；与完全微调相比，速度提高了25%
- 简单的线性设计允许我们在部署时将可训练矩阵与冻结权重合并，与完全微调的模型相比，在结构上没有引入推理延迟。
- LoRA与许多先前的方法是不相关的，并且可以与许多方法相结合，例如前缀微调。

## **一、简介**

自然语言处理的一个重要范式是在一般领域数据上进行大规模的预训练，并适应特定的任务或领域。随着我们对更大的模型进行预训练，重新训练所有模型参数的完全微调变得不太可行。以GPT-3 175B为例--部署独立的微调模型实例，每个都有175B的参数，成本过高。我们提出了**低秩适应（Low-Rank Adaptation），即LoRA，它冻结了预训练的模型权重，并将可训练的秩分解矩阵注入到Transformer架构的每一层，大大减少了下游任务的可训练参数的数量。**与用Adam微调的GPT-3 175B相比，LoRA可以将可训练参数的数量减少10,000倍，对GPU内存的要求减少3倍。LoRA在RoBERTa、DeBERTa、GPT-2和GPT-3上的模型质量表现与微调持平或更好，尽管它的可训练参数更少，训练吞吐量更高，而且与适配器(adapter，一种微调策略)不同，没有额外的推理延时。我们还对语言模型微调中的秩亏进行了实证调查，这说明了LoRA的功效。我们发布了一个便于LoRA与PyTorch模型集成的软件包，并提供了我们对RoBERTa、DeBERTa和GPT-2的实现和模型checkpoint，网址是[https://github.com/microsoft/LoRA](https://link.zhihu.com/?target=https%3A//github.com/microsoft/LoRA)。

自然语言处理中的许多应用都依赖于将一个大规模的、预训练好的语言模型适应于多个下游的应用。这种适应通常是通过全量微调完成的，全量微调的缺点是新模型包含的参数和原始模型一样多。随着更大的模型每隔几个月就会被训练一次，这对于GPT-2或RoBERTa large来说，仅仅是一个 "不便之处"，而对于GPT-3则是一个关键的部署挑战，有1750亿可训练参数。

许多人试图通过只调整一些参数或为新任务学习外部模块来缓解这一问题。这样，除了每个任务的预训练模型外，我们只需要存储和加载少量的特定任务参数，大大提升了部署时的运行效率。然而，现有的技术往往通过扩展模型深度或减少模型的可用序列长度来微调模型，这会引入推理延迟(第3节)。更重要的是，这些方法往往不能与微调基线相匹配，造成了效率和模型质量之间的权衡。

我们从Li等人（2018a）；Aghajanyan等人（2020）那里得到启发，他们表明学到的**过度参数化模型**实际上位于一个低的本征维度上。我们假设模型适应过程中权重的变化也具有较低的 "内在秩"，从而导致我们提出的低秩适应（LoRA）方法。LoRA允许我们通过优化密集层在适应过程中的变化的秩分解矩阵来间接地训练神经网络中的一些密集层，而保持预训练的权重冻结，如图1所示。以GPT-3 175B为例，我们表明，即使满秩（即d）高达12288，一个非常低的秩（即图1中的r可以是1或2）也足够了，这使得LoRA既有存储又有计算效率。

![Untitled](LoRA%204c47c2a3114b4730aadd4db20e49823c/Untitled.png)

> 图1：我们的重构图。我们只训练A和B。
> 

**LoRA拥有几个关键优势：**

- 一个预训练好的模型可以被共享，用来为不同的任务建立许多小的LoRA模块。我们可以冻结共享模型，并通过替换图1中的矩阵A和B来有效地切换任务，从而大大减少存储需求和任务切换的开销。
- LoRA使训练更加有效，在使用自适应优化器时，硬件门槛降低了3倍，因为我们不需要计算梯度或维护大多数参数的优化器状态。相反，我们只优化注入的、小得多的低秩矩阵。
- 我们简单的线性设计允许我们在部署时将可训练矩阵与冻结权重合并，与完全微调的模型相比，在结构上没有引入推理延迟。
- LoRA与许多先前的方法是不相关的，并且可以与许多方法相结合，例如前缀微调。我们在附录E中提供了一个例子。

**术语和惯例** 我们经常提到Transformer架构，并对其维度使用常规术语。我们把Transformer层的输入和输出维度大小称为 $d_{model}$。我们用 $W_q$、$W_k$、$W_v$ 和 $W_o$ 来指代self-attention模块中的查询/键/值/输出投影矩阵。$W$或$W_0$指的是预训练的权重矩阵，$∆W$指的是适应过程中的累积梯度更新。我们用 $r$ 来表示一个LoRA模块的秩。我们遵循（Vaswani等人，2017；Brown等人，2020）规定的惯例，使用Adam（Loshchilov & Hutter，2019；Kingma & Ba，2017）进行模型优化，并使用Transformer MLP前馈维度$d_{ffn}=4×d_{model}$。

## **二、问题陈述**

虽然我们的建议与训练目标无关，但我们专注于语言模型作为我们的激励用例。下面是对语言模型问题的简要描述，特别是对给定特定任务提示的条件概率的最大化。

假设我们得到了一个预训练的自回归语言模型 $P_Φ(y|x)$，并以$Φ$为参数。例如， $P_Φ(y|x)$ 可以是一个通用的多任务学习器，如GPT，基于Transformer架构。考虑将这个预训练的模型调整为下游的条件文本生成任务，如总结、机器阅读理解（MRC）和自然语言转SQL（NL2SQL）。每个下游任务都由上下文-目标对的训练数据集表示。$Z = \{(x_i, y_i)\}_{i=1,..,N}$，其中 $x_i$ 和 $y_i$ 都是token的序列。例如，在NL2SQL中，$x_i$ 是一个自然语言查询，  $y_i$ 是其相应的SQL命令；对于总结， $x_i$ 是一篇文章的内容，$y_i$ 是其摘要。

在全面微调期间，模型被初始化为预训练的权重$Φ_0$，并通过反复遵循梯度来更新为 $Φ_0+ΔΦ$ ，以最大化条件语言模型目标:

$$
\max_{\Phi} ∑_{(x,y)∈Z }∑_{t=1}^{|y|} log (P_Φ(y_t|x, y<t))
$$

完全微调的主要缺点之一是，对于每个下游任务，我们都要学习一组不同的参数$∆Φ$，其维度$|∆Φ|$等于 $|Φ_0|$ 。因此，如果预训练的模型很大（如GPT-3的$|Φ_0|$≈1750亿），存储和部署许多独立的微调模型实例会很有挑战性，即使是可行的。

在本文中，我们采用了一种更有效的参数方法，其中特定任务的参数增量 $ΔΦ=ΔΦ(Θ)$ 被一个尺寸小得多的参数集$Θ$进一步编码，而 $|Θ|≪ |Φ_0|$ 的任务是寻找$∆Φ$，因此变成了对$Θ$进行优化。


在接下来的章节中，我们建议使用低秩表示来编码$∆Φ$，这样既能提高计算效率又能提高记忆效率。当预训练模型为GPT-3 175B时，可训练参数$|Θ|$的数量可以小到$|Φ|$的0.01%。

## **三、现有的解决方案还不够好吗？**

**我们要解决的问题绝不是新问题:** 自从迁移学习开始以来，有几十项工作试图使模型适应性更强的参数和计算效率。参见第6节对一些著名工作的调查。以语言模型为例，当涉及到高效的适应时，有两种突出的策略：**adding adapter layers 增加适应层**（Houlsby等人，2019；Rebuffi等人，2017；Pfeiffer等人，2021；Ru¨ckle´等人，2020）或**优化输入层激活的某些形式**（Li & Liang，2021；Lester等人，2021；Hambardzumyan等人，2020；Liu等人，2021）。然而，这两种策略都有其局限性，特别是在大规模和延迟敏感的生产场景中。

**适配器层引入了推理延迟:** 适配器有很多变体。我们关注Houlsby等人（2019）的原始设计，它在每个Transformer块上有两个适配器层，以及Lin等人（2020）的最新设计，它在每个块上只有一个，但有一个额外的LayerNorm（Ba等人，2016）。虽然人们可以通过修剪层或利用多任务设置来减少整体延迟（Ru¨ckle´等人，2020；Pfeiffer等人，2021），但没有直接的方法来绕过适配器层的额外计算。这似乎不是一个问题，因为适配器层被设计成只有很少的参数（有时<1%的原始模型），其瓶颈维度很小，这限制了它们可以增加的FLOPs。然而，**大型神经网络依靠硬件并行性来保持低延迟，而适配器层必须按顺序处理**。这在在线推理设置中是有区别的，在这种情况下，批次大小通常小到一个。在没有模型并行的一般情况下，比如在单个GPU上运行GPT-2的推理，我们看到使用适配器时延迟明显增加，即使瓶颈维度非常小（表1）。

![Untitled](LoRA%204c47c2a3114b4730aadd4db20e49823c/Untitled%201.png)

> 表1：在GPT-2介质中，以毫秒为单位测量的单次前向通过的Infernece延迟，100次试验的平均数。我们使用NVIDIA Quadro RTX8000。"$|Θ|$"表示适配器层中可训练参数的数量。AdapterL和AdapterH是适配器调整的两种变体，我们在第5.1节中进行了描述。适配器层引入的推理延迟在一个在线的、短序列长度的情况下可能是显著的。见附录B中的完整研究。
> 

当我们需要像Shoeybi等人（2020）；Lepikhin等人（2020）那样对模型进行分片时，这个问题会变得更糟，因为额外的深度需要更多的同步GPU运算，如AllReduce和Broadcast，除非我们多次冗余地存储适配器参数。

**直接优化提示是困难的:** 另一个方向，如前缀优化（Li & Liang, 2021），面临不同的挑战。我们观察到，前缀优化很难优化，其性能在可训练参数中的变化是非单调的，证实了原始论文中的类似观察。更根本的是，为适应保留一部分序列长度必然会减少可用于处理下游任务的序列长度，我们怀疑这使得调整提示与其他方法相比性能较差。我们将对任务性能的研究推迟到第5节。

## **四、我们的方法**

我们描述了LoRA的简单设计和它的实际好处。这里概述的原则适用于深度学习模型中的任何密集层，尽管我们在实验中只关注Transformer语言模型中的某些权重，作为激励的用例。

### **4.1 低秩参数化的更新矩阵**

一个神经网络包含许多密集的层，它们进行矩阵乘法。这些层中的权重矩阵通常具有满秩。当适应一个特定的任务时，Aghajanyan等人（2020）表明，预训练语言模型具有较低的 "本征维度"，尽管随机投影到一个较小的子空间，仍然可以有效地学习。受此启发，我们假设权重的更新在适应过程中也具有较低的 "内在秩"。对于一个预训练好的权重矩阵 $W_0 ∈ R^{d×k}$ ，我们通过用低秩分解 $W_0 + ∆W = W_0 + BA$ 代表后者来约束其更新，其中$B ∈ R^{d×r}, A ∈ R^{r×k}$ 和秩 $r<<min(d, k)$。在训练期间，$W_0$被冻结，不接受梯度更新，而 $A$ 和 $B$ 包含可训练参数。请注意，$W_0$和 $∆W = BA$ 都与相同的输入相乘，它们各自的输出向量按坐标相加。对于 $h = W_0x$ ，我们修改后的前向传递产生了。

$$
h = W_0x + ∆W x = W_0x + BAx
$$

我们在图1中说明了我们的重新参数化。我们对**A使用随机的高斯初始化，B在训练开始时为零，所以 $∆W = BA$ 在训练开始时为零。**。然后我们将 $∆W x$ 缩放 $α/r$ ，其中 $α$ 是  $r$  中的常数。当用Adam进行优化时，如果我们适当地扩展初始化，调整 $α$ 与调整学习率大致相同。因此，我们只需将$α$设置为我们尝试的第一个$r$，而不对其进行调整。这种缩放有助于减少当我们改变  $r$  时重新调整超参数的需要。

**完全微调的一般化:**一种更普遍的微调形式允许训练预训练参数的一个子集。LoRA更进一步，不要求权重矩阵的累积梯度更新在适应过程中具有满秩。这意味着，当对所有权重矩阵应用LoRA并训练所有偏差时，我们通过将LoRA秩 $r$ 设置为预训练权重矩阵的秩，大致恢复了完全微调的表现力。换句话说，随着我们增加可训练参数的数量，训练LoRA大致收敛于训练原始模型，而基于适配器的方法收敛于MLP，基于前缀的方法不能接受长输入序列的模型。

**没有额外的推理延时:**在生产中部署时，我们可以明确地计算和存储 $W = W_0 + BA$，并像往常一样执行推理。请注意，$W_0$和BA的维度都是 $R^{d×k}$ 中。当我们需要切换到另一个下游任务时，我们可以通过减去$BA$，然后增加一个不同的$B'A'$来恢复$W_0$，这是一个快速的运算，内存开销非常小。最关键的是，这保证了我们在推理过程中，与结构上的微调模型相比，不会引入任何额外的延迟。

### **4.2 将LORA应用于transformer**

原则上，我们可以将LoRA应用于神经网络中的任何权重矩阵子集，以减少可训练参数的数量。在Transformer架构中，self-attention模块中有四个权重矩阵 ($W_q, W_k, W_v, W_o$) ，MLP模块中有两个。我们将$W_q$ 或 $W_k$，$W_v$视为维度为 $d_{model} × d_{model}$ 的单一矩阵，即使输出维度通常被切成注意力头。我们将研究限制在只适应下游任务的注意力权重，并冻结MLP模块（因此它们不在下游任务中训练），这既是为了简单，也是为了参数效率。我们在第7.1节中进一步研究了在transformer中适应不同类型注意力权重矩阵的效果。我们把对适应MLP层、LayerNorm层和偏差的实证调查留给未来的工作。

**实际的好处和限制:** 最重要的好处来自于内存和存储用量的减少。对于一个用Adam训练的大型transformer，如果$r << d_{model}$，我们减少了$2/3$的VRAM用量，因为我们不需要存储冻结参数的优化器状态。在GPT-3 175B上，我们将训练期间的VRAM消耗从1.2TB减少到350GB。在 $r=4$ 并且只有查询和值投影矩阵被调整的情况下，checkpoint的大小大约减少了10,000×（从350GB到35MB）。 这使得我们可以用更少的GPU进行训练，避免I/O瓶颈。另一个好处是，我们可以在部署时以更低的成本切换任务，只交换LoRA的权重，而不是所有的参数。这使得我们可以**创建许多定制的模型，这些模型可以在将预训练的权重存储在VRAM中的机器上进行实时切换。**我们还观察到，在GPT-3 175B上训练时，与完全微调相比，速度提高了25%，因为我们不需要为绝大多数的参数计算梯度。

**LoRA也有其局限性:** 例如，如果选择将$A$和$B$ 吸收到$W$中以消除额外的推理延迟，那么在一次前向传递中对不同任务的输入进行批处理是不直接的。尽管在延迟不重要的情况下，可以不合并权重，动态地选择LoRA模块用于批次中的样本。

## **五、经验性实验**

我们在RoBERTa、DeBERTa 和GPT-2 上评估了LoRA的下游任务性能，然后扩展到GPT-3 175B。我们的实验涵盖了广泛的任务，从自然语言理解（NLU）到生成（NLG）。具体来说，我们在GLUE基准上对RoBERTa和DeBERTa进行评估。我们遵循Li & Liang（2021）在GPT-2上的设置进行直接比较，并在GPT-3上增加WikiSQL（NL到SQL查询）和SAMSum（对话总结）进行大规模实验。关于我们使用的数据集的更多细节，见附录C。我们使用NVIDIA Tesla V100进行所有实验。

### **5.1 基准线**

为了与其他基线进行广泛的比较，我们复制了以前的工作所使用的设置，并尽可能地重复使用他们的报告数字。然而，这意味着一些基线可能只出现在某些实验中。

微调（FT）是一种常见的适应方法，在微调过程中，模型被初始化为预训练的权重和偏差，所有模型参数经历梯度更新。一个简单的变体是仅更新某些层，而冻结其他层。我们包括先前工作中报告的GPT-2的一个这样的基线（Li & Liang, 2021），它只适应最后两层（$FT^{Top2}$）。

**Bias-only or BitFit** 是一个基线，我们只训练偏差向量，而冻结其他一切。同时，这种基线也被BitFit研究过（Zaken等人，2021）。

**Prefix-embedding tuning (PreEmbed)** 前缀嵌入调整是在输入token中插入特殊token。这些特殊的token有可训练的词嵌入，一般不在模型的单词表中。在哪里放置这些token会对性能产生影响。我们专注于 "前缀 "和 "后缀"，前者将这些token添加到提示中，后者将其追加到提示后；Li & Liang (2021)中讨论了这两者。我们用$l_p$（resp. $l_i$）表示前缀（resp. infix）token的数量。可训练参数的数量为  $|Θ| = d_{model} × (l_p + l_i)$。

**Prefix-layer tuning (PreLayer)** 前缀层调整是前缀嵌入调整的延伸。我们不是仅仅学习一些特殊token的词嵌入（或者等同于嵌入层之后的激活），而是学习每个transformer层之后的激活。从以前各层计算出来的激活值被简单地替换为可训练的激活值。由此产生的可训练参数的数量是$|Θ| = L × d_{model} × (l_p + l_i)$ ，其中$L$ 是transformer层的数量。

**Adapter tuning** Houlsby等人（2019）提出的适配器优化在self-attention模块（和MLP模块）和随后的残差连接之间插入适配器层。有两个全连接的层，在中间有非线性的适配器层中有偏差。我们称这种原始设计为$Adapter^H$。最近，Lin等人（2020）提出了一个更有效的设计，适配器层只应用在MLP模块之后和一个LayerNorm之后。我们称其为$Adapter^L$。这与Pfeiffer等人（2021）提出的另一个设计非常相似，我们称之为$Adapter^P$。我们还包括另一个称为AdapterDrop的基线，它为提高效率而放弃了一些适配器层（$Adapter^D$）。我们尽可能地引用以前工作中的数字，以最大限度地增加我们比较的基线的数量；它们在第一列中带有星号（*）的行中。在所有情况下，我们都有$|Θ| = {\hat{L}}_{Adpt} × (2 × d_{model} × r + r + d_{model}) + 2 × {\hat{L}}_{LN} × d_{model}$ ，其中 ${\hat{L}}_{Adpt}$ 是适配器层的数量， ${\hat{L}}_{LN}$ 是可训练的LayerNorms的数量。

LoRA在现有权重矩阵的基础上并行增加了可训练的秩分解矩阵对。如第4.2节所述，为了简单起见，我们在大多数实验中只对 $W_q$  和 $W_v$ 应用LoRA。可训练参数的数量由秩 $r$ 和原始权重的形状决定。$|Θ| = 2 × \hat{L}_{LoRA} × d_{model} × r$ 其中 $\hat{L}_{LoRA}$ 是我们应用LoRA的权重矩阵的数量。

### **5.2 RoBERTa Base/Large**

RoBERTa优化了最初在BERT中提出的预训练配方，在没有引入更多可训练参数的情况下提升了后者的任务性能。虽然RoBERTa近年来在NLP排行榜上被更大的模型所超越，但在从业者中，它仍然是一个具有竞争力的、受欢迎的预训练模型，其规模也是如此。我们从HuggingFace Transformers库中提取预训练的RoBERTa base（125M）和RoBERTa large（355M），并评估不同的高效适应方法在GLUE基准任务上的表现。我们还根据Houlsby等人（2019）和Pfeiffer等人（2021）的设置进行了复制。为了确保公平的比较，我们在与适配器比较时，对评估LoRA的方式做了两个关键的改变。首先，我们对所有任务使用相同的批次大小，并使用128的序列长度来匹配适配器基线。其次，我们将模型初始化为MRPC、RTE和STS-B的预训练模型，而不是像微调基线那样已经适应了MNLI的模型。按照Houlsby等人（2019）的这种更严格的设置进行的运行被标注为$†$。结果列于表2。关于所使用的超参数的细节，见D.1节。

![Untitled](LoRA%204c47c2a3114b4730aadd4db20e49823c/Untitled%202.png)

> 表2: RoBERTabase、RoBERTalarge和DeBERTaXXL在GLUE基准上采用不同的适应方法。我们报告了MNLI的总体（匹配和不匹配）准确率，CoLA的Matthew相关度，STS-B的Pearson相关度，以及其他任务的准确率。所有指标都是越高越好。*表示在以前的工作中发表的数字。$†$表示在类似于Houlsby等人（2019）的设置中配置的运行，以进行公平的比较。
> 

### **5.3 DEBERTA XXL**

DeBERTa是BERT的一个较新的变体，它在更大的规模上进行训练，在GLUE和SuperGLUE等基准上的表现非常具有竞争力。我们评估了LoRA是否还能在GLUE上与完全微调的DeBERTa XXL（1.5B）的性能相匹配。结果见表2（底部部分）。关于所使用的超参数的细节，见D.2节。

### **5.4 GPT-2 中型/大型**

在证明了LoRA可以成为NLU上完全微调的一个有竞争力的选择之后，我们希望回答LoRA在NLG模型上是否仍然占优势，比如GPT-2中型和大型模型。我们尽可能地保持我们的设置与Li & Liang（2021）接近，以便进行直接比较。由于篇幅的限制，我们在本节中只介绍了我们对E2E NLG挑战赛的结果（表3）。关于WebNLG（Gardent等人，2017）和DART（Nan等人，2020）的结果见F.1节。我们在D.3节中列出了使用的超参数。

![Untitled](LoRA%204c47c2a3114b4730aadd4db20e49823c/Untitled%203.png)

表3: 在E2E NLG挑战赛上采用不同适应方法的GPT-2中型（M）和大型（L）。对于所有指标，越高越好。LoRA在可训练的参数相当或更少的情况下优于几个基线。信心区间显示的是我们进行的实验。*表示以前的工作中发表的数字。

### **5.5 扩展到GPT-3 175B**

作为LoRA的最后一个压力测试，我们将参数扩大到GPT-3的1750亿。由于训练成本很高，我们只报告了随机种子上给定任务的典型标准偏差，而不是为每个条目提供一个标准偏差。关于所使用的超参数的细节，见D.4节。

如表4所示，LoRA在所有三个数据集上都匹配或超过了微调基线。请注意，并不是所有的方法都能从拥有更多的可训练参数中单调地获益，如图2所示。当我们使用超过256个特殊token进行前缀嵌入调整或使用超过32个特殊token进行前缀层调整时，我们观察到性能明显下降。这印证了Li & Liang（2021）的类似观察。虽然对这一现象的彻底调查超出了这项工作的范围，但我们怀疑有更多的特殊token会导致输入分布进一步偏离预训练的数据分布。另外，我们在F.3节中研究了不同适应方法在低数据体系中的表现。

![Untitled](LoRA%204c47c2a3114b4730aadd4db20e49823c/Untitled%204.png)

> 表4: GPT-3 175B上不同适应方法的性能。我们报告了WikiSQL上的逻辑形式验证精度，MultiNLI-matched上的验证精度，以及SAMSum上的Rouge-1/2/L。LoRA比之前的方法表现得更好，包括完全微调。在WikiSQL上的结果有±0.5%左右的波动，MNLI-m有±0.1%左右的波动，SAMSum有±0.2/±0.2/±0.1左右的三个指标。
> 

![Untitled](LoRA%204c47c2a3114b4730aadd4db20e49823c/Untitled%205.png)

> 图2: GPT-3 175B验证准确率与WikiSQL和MNLI匹配的几种适应方法的可训练参数数的关系。LoRA表现出更好的可扩展性和任务性能。关于绘图数据点的更多细节，见F.2节。
> 

## **六、相关工作**

**Transformer语言模型。**Transformer是一个seq2seq的架构，大量使用了self-attention。Radford等人（a）通过使用Transformer解码器的堆栈将其应用于自回归语言模型。从那时起，基于Transformer的语言模型主导了NLP，在许多任务中达到了最先进的水平。BERT（Devlin等人，2019b）和GPT-2（Radford等人，b）出现了一个新的范式--两者都是在大量文本上训练的大型Transformer语言模型--与直接在特定任务数据上训练相比，在一般领域数据上进行预训练后，在特定任务数据上进行微调会带来明显的性能提升。训练更大的transformer通常会带来更好的性能，这仍然是一个活跃的研究方向。GPT-3（Brown等人，2020）是迄今为止训练的最大的单一Transformer语言模型，具有175B的参数。

**提示工程和微调。**虽然GPT-3 175B可以通过一些额外的训练实例来调整其行为，但其结果在很大程度上取决于输入提示。这就需要一种经验性的艺术来组成和格式化提示，以最大限度地提高模型在预期任务上的表现，这被称为提示工程。微调将一个在一般领域中预训练的模型重新训练到一个特定的任务上。它的变种包括只学习参数的一个子集；然而，从业者经常重新训练所有的参数以最大限度地提高下游性能。然而，GPT-3 175B的艰巨性使得以通常的方式进行微调具有挑战性，因为它产生了大量的checkpoint，并且由于它具有与预训练相同的内存足迹，因此硬件门槛很高。

**参数有效的适应。**许多人提出在神经网络的现有层之间插入适配器层。我们的方法使用类似的瓶颈结构，对权重更新施加低秩约束。关键的功能差异是，我们学习的权重可以在推理过程中与主权重合并，因此不会引入任何延迟，而适配器层则不存在这种情况（第3节）。适配器的临时扩展是COMPACTER（Mahabadi等人，2021年），它基本上是使用克朗克乘积（Kronecker products）与一些预定的权重共享方案对适配器层进行参数化。同样，将LoRA与其他基于张量积的方法相结合，有可能提高其参数效率，这一点我们留待以后的工作。最近，许多人提出优化输入词嵌入以代替微调，类似于提示工程的连续和可微调概括。我们在实验部分包括与Li & Liang（2021）的比较。然而，这一行的工作只能通过在提示中使用更多的特殊token来扩大规模，当位置嵌入被学习时，这些token会占用任务token的可用序列长度。

**深度学习中的低秩结构。**低秩结构在机器学习中非常常见。很多机器学习问题都有某些内在的低秩结构。此外，众所周知，对于许多深度学习任务，特别是那些具有严重过度参数化的神经网络，学习的神经网络在训练后将享有低秩属性。一些先前的工作甚至在训练原始神经网络时明确施加了低秩约束；然而，就我们所知，这些工作中没有一项考虑对冻结模型进行低秩更新以适应下游任务。在理论文献中，众所周知，当底层概念类具有一定的低秩结构时，神经网络的表现优于其他经典的学习方法，包括相应的（有限宽度）神经切线核。Allen-Zhu & Li（2020b）的另一个理论结果表明，低秩适应性对于对抗性训练是有用的。总而言之，我们认为我们提出的低秩适应性更新在文献中得到了很好的启发。

## **七、了解低秩的更新**

给定LoRA的经验优势，我们希望进一步解释从下游任务中学习到的低秩适应的特性。请注意，低秩结构不仅降低了硬件门槛，使我们能够并行地运行多个实验，而且还能**更好地解释更新权重与预训练权重的相关性**。我们把研究重点放在GPT-3 175B上，在那里我们实现了可训练参数的最大减少（高达10,000倍），而没有对任务性能产生负面影响。

我们进行了一系列的实证研究，以回答以下问题。1）在参数预算约束下，我们应该调整预训练的Transformer中的哪一个权重矩阵子集，以使下游性能最大化？2）"最佳 "适应矩阵$∆W$是否真的存在秩亏？如果是的话，在实践中使用的好秩是什么？3) $∆W$和$W$之间有什么联系？$∆W$是否与$W$高度相关？与$W$相比，$∆W$有多大？

我们相信，我们对问题（2）和（3）的回答阐明了在下游任务中使用预训练语言模型的基本原则，这是NLP的一个关键主题。

### **7.1 我们应该对transformer中的哪些权重矩阵应用Lora？**

在有限的参数预算下，我们应该用LoRA调整哪种类型的权重，以获得下游任务的最佳性能？如第4.2节所述，我们只考虑self-attention模块的权重矩阵。我们在GPT-3 175B上设置了18M的参数预算（如果存储在FP16中大约是35MB），针对96层transformer，如果我们适应一种类型的注意力权重，相当于$r=8$，如果我们适应两种类型，相当$r=4$。结果见表5。

![Untitled](LoRA%204c47c2a3114b4730aadd4db20e49823c/Untitled%206.png)

> 表5：在可训练参数量相同的情况下，将LoRA应用于GPT-3中不同类型的注意力权重后，WikiSQL和MultiNLI的验证准确率。对$W_q$ 和$W_v$ 进行调整，总体上给出了最好的性能。我们发现，对于一个给定的数据集，不同随机种子的标准差是一致的，我们在第一栏中报告。
> 

请注意，把所有的参数放在$∆W_q$ 或$∆W_k$ 中会导致性能明显降低，而同时适应$W_q$ 和$W_v$ 会产生最好的结果。这表明，即使是四级的秩也能在$∆W$中捕捉到足够的信息，这样一来，适应更多的权重矩阵比适应具有较大秩的单一类型的权重更可取。

### **7.2 LORA的最佳秩是什么？**

我们把注意力转向秩$r$ 对模型性能的影响。我们对 $\{W_q, W_v\}$、$\{W_q, W_k, W_v, W_c\}$ 和仅$W_q$进行了调整，以进行比较。

表6显示，令人惊讶的是，LoRA在很小的$r$下已经有了很好的表现（$\{W_q, W_v\}$比$W_q$ 更有竞争力）。这表明更新矩阵$∆W$可能具有非常小的“内在秩”。为了进一步支持这一发现，我们检查了由不同选择的$r$ 和不同的随机种子学习的子空间的重叠。我们认为，增加 $r$ 并不能覆盖一个更有意义的子空间，这表明低秩的适应矩阵就足够了。

![Untitled](LoRA%204c47c2a3114b4730aadd4db20e49823c/Untitled%207.png)

> 表6：在WikiSQL和MultiNLI上用不同的秩$r$进行验证的准确性。令我们惊讶的是，在这些数据集上，小到一个等级就足以适应$W_q$和$W_v$，而单独训练$W_q$需要更大的$r$。
> 

**不同$r$之间的子空间相似性:** 给定$A_{r=8}$ 和$A_{r=64}$ 是使用相同的预训练模型学到的秩为$r=8$和$64$ 的适应矩阵，我们进行奇异值分解，得到**右奇异酉矩阵**$U_{A_{r=8}}$和$U_{A_{r=64}}$。 我们希望回答： $U_{A_{r=8}}$对于$1<=i<=8$ 的前$i$ 个奇异向量所跨越的子空间有多少包含在 $U_{A_{r=64}}$ (for $1 ≤ j ≤ 64$) 的前$j$个奇异向量所跨越的子空间中。我们用基于格拉斯曼距离的归一化子空间相似度来衡量这个数量（更正式的讨论见附录G）

$$
φ(A_{r=8}, A_{r=64}, i, j) = \frac{||U^i _{A_{r=8}} U^j _{A_{r=64}} || ^2_F } {min(i, j)} ∈ [0, 1]
$$

其中$U^i_{A_{r=8}}$ 代表 $U_{A_{r=8}}$中对应于前 $i$ 个奇异向量的列。

$φ(⋅)$ 的范围是[0，1]，其中1代表子空间完全重叠，0代表完全分离。由于篇幅限制，我们只看了第48层（共96层），但这个结论对其他层也是成立的，如H.1节所示。

![Untitled](LoRA%204c47c2a3114b4730aadd4db20e49823c/Untitled%208.png)

> 图3：$∆W_q$和$∆W_v$的$A_{r=8}$和$A_{r=64}$的列向量之间的子空间相似性。第三和第四幅图放大了前两幅图中的左下角三角形。$r = 8$ 中的顶部方向包含在 $r = 64$ 中，反之亦然。。
> 

我们从图3中得出了一个重要的观察结果。

与顶部奇异向量相对应的方向在$A_{r=8}$和$A_{r=64}$ 之间有明显的重叠，而其他方向则没有。具体来说，$A_{r=8}$的$∆W_v$（resp. $∆W_q$）和$A_{r=64}$ 的$∆W_v$（resp. $∆W_q$）共享一个维度为1的子空间，归一化相似度大于0.5，这就解释了为什么$r=1$在我们的下游任务中对GPT-3表现得相当好。

由于$A_{r=8}$和$A_{r=64}$都是使用相同的预训练模型学习的，图3表明$A_{r=8}$和$A_{r=64}$的顶部奇异向量方向是最有用的，而其他方向可能主要包含训练中积累的随机噪声。因此，适应矩阵确实可以有一个非常低的秩。

**不同随机种子之间的子空间相似性**。我们通过绘制图4中所示的两个随机种子运行与$r=64$之间的归一化子空间相似性来进一步确认这一点。 $∆W_q$似乎比$∆W_v$有更高的 "内在秩"，因为对于$∆W_q$来说，两个运行都学到了更多共同的奇异值方向，这与我们在表6中的经验观察一致。作为比较，我们还绘制了两个随机高斯矩阵，它们彼此之间没有任何共同的奇异值方向。

![Untitled](LoRA%204c47c2a3114b4730aadd4db20e49823c/Untitled%209.png)

> 图4：左边和中间。来自两个随机种子的$A_{r=64}$的列向量之间的归一化子空间相似性，对于第48层的$∆W_q$ 和$∆W_v$ 都是如此。右图：两个随机高斯矩阵的列向量之间的相同热图。其他层见H.1节。
> 

### **7.3 适应矩阵 $∆W$与$W$相比如何？**

**我们进一步研究$∆W$和$W$之间的关系:** 特别是，$∆W$是否与$W$高度相关？(或者从数学上讲，$∆W$主要包含在$W$的顶部奇异方向上吗？） 另外，与$W$中的相应方向相比，$∆W$有多 "大"？这可以阐明适应预训练语言模型的基本机制。

为了回答这些问题，我们通过计算$U^TWV^T$将$W$投射到$∆W$的 $r$ 维子空间，$U /V$是$∆W$的左/右奇异向量矩阵。然后，我们比较 $‖U^TWV^T‖_F$和 $‖W‖_F$之间的Frobenius范式。作为比较，我们还计算了 $‖U^TWV^T‖_F$，将$U$、$V$替换为$W$的前 $r$ 个奇异向量或一个随机矩阵。

![Untitled](LoRA%204c47c2a3114b4730aadd4db20e49823c/Untitled%2010.png)

> 表7$：U^TW_qV^T$的Frobenius范式，其中$U$和$V$是$ΔW_q$，$W_q$或随机矩阵的左/右顶 $r$ 奇异向量方向。权重矩阵取自GPT-3的第48层。
> 

**我们从表7中得出了几个结论:** 首先，与随机矩阵相比，$∆W$与$W$有更强的相关性，表明$∆W$放大了$W$中已有的一些特征。第二，$∆W$没有重复$W$的顶级奇异方向，而只是放大了$W$中没有强调的方向。第三，放大系数相当大：21.5≈6.91/0.32，在$r=4$的情况下。关于为什么 $r = 64$ 的放大系数较小，见H.4节。我们还在第H.3节中提供了一个可视化的例子，说明当我们从$W$中包括更多的顶级奇异方向时，相关性是如何变化的。这表明，**低秩适应矩阵可能会放大特定下游任务的重要特征，而这些特征在一般的预训练模型中没有得到强调**。

## **八、结论和未来工作**

从所需的硬件和为不同任务托管独立实例的存储/切换成本来看，微调巨大的语言模型是非常昂贵的。我们提出了LoRA，一种高效的适应策略，既不引入推理延迟，也不减少输入序列的长度，同时保留了高的模型质量。重要的是，它允许在作为服务部署时通过共享绝大多数的模型参数来实现快速的任务切换。虽然我们专注于Transformer语言模型，但提出的原则一般适用于任何具有密集层的神经网络。

**未来的工作有很多方向:** 1) LoRA可以与其他有效的适应方法相结合，有可能提供不相关的改进。2) 微调或LoRA背后的机制还很不清楚--在预训练期间学到的特征是如何转化为在下游任务中表现良好的？我们相信LoRA比完全微调更容易回答这个问题。3) 我们主要依靠启发式方法来选择要应用LoRA的权重矩阵。是否有更多原则性的方法？4) 最后，$∆W$的秩亏表明，$W$也可能是秩亏的，这也可以成为未来工作的灵感来源。
# 4. 经典模型

## 4.1 



## 4.2 GPT 系列

> **论文**：**[GPT-1](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)  [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)  [GPT-3](https://arxiv.org/pdf/2005.14165)  [InstructGPT](https://arxiv.org/pdf/2203.02155)  [GPT-4](https://arxiv.org/pdf/2303.08774)**
>
> **代码**：**[GPT-2](https://github.com/openai/gpt-2)  [GPT-3](https://github.com/openai/gpt-3)**

**OpenAI** 是一家致力于推动人工智能前沿研究的公司，尤其在自然语言处理领域取得了重大突破。其开发的 **GPT**（**G**enerative **P**re-trained **T**ransformer）系列模型，包括 **GPT-1**、**GPT-2**、**GPT-3**、**ChatGPT** 和 **GPT-4**，逐步提升了语言模型的能力和应用范围。**GPT-1** 和 **GPT-2** 作为最早的版本，奠定了预训练模型的基础，而 **GPT-3** 则通过拥有 1750 亿个参数，使得语言生成能力达到了一个新的高度，能够执行少量示例学习任务。

![]()

**ChatGPT** 在 **GPT-3** 的基础上进行了优化，专注于对话生成，提升了多轮对话的流畅度和上下文一致性。**GPT-4** 则是目前最先进的版本，具备更强的推理能力、更高的精确度，且支持多模态输入。这些模型在文本生成、对话系统、编程辅助等多个领域得到了广泛应用，并且推动了 AI 技术的实际落地和发展。

**GPT&#x20;**&#x548C; **BERT&#x20;**&#x672C;质都是基于 **Transformer**，思想都是先通过在无标签的数据上学习一个预训练的语言模型，然后再根据特定热任务进行微调。**GPT&#x20;**&#x7528;的是 **Transformer** 中去掉中间 **Encoder-Decoder Attention** 层的 **Decoder**，其实也可以等价地说，用到的是 **Encoder** 层，只是将 **Multi-Head Attention** 换成了 **Masked Multi-Head Attention**，即 **Masked Self Attention**，是单向语言模型，给定前几个词预测下一个词，更适合自然语言生成的任务；而 **BERT** 使用的是 **Transformer&#x20;**&#x7684; **Encoder**，即 **Self Attention**，是双向的语言模型，给定周围上下文的词预测中间被 mask 的词，更适合自然语言理解的任务。

### 4.2.1 GPT-1

2018 年 6 月，OpenAI 发布了 **GPT-1**，其基于 Transformer 模型，利用 Transformer 的 Decoder 结构来进行单向语言模型的训练。**GPT-1** 的核心思想是先通过无标签的文本去训练生成语言模型，再根据具体的 NLP 任务，如文本蕴涵、QA、文本分类等，通过有标签的数据对模型进行微调。之所以采用预训练加微调的方式，是因为在计算机视觉中，这种方式流行了很长时间：先用海量有标注的数据集，通过有监督的训练生成一个预训练模型，然后通过下游任务，在这个模型上做微调。**GPT-1** 沿用了这个方式，具体的预训练与微调示意图如下：

![]()

* **预训练**

第一阶段的目标是预训练语言模型，使用 **BooksCorpus** 数据集，包含超过 7000 本来自各种类型的未出版的书籍，它包含了一长段连续的文本，这使得生成模型能够学习长距离信息。另一个数据集是 1B Word Benchmark，与 ELMo 使用方法大致相同，但是在句子层面被打乱了，破坏了长距离结构。在这个语料库中，语言模型达到了 18.4 的非常低的困惑度。

给定一个无监督的 token 语料：$$\mathcal{U} = \{u_1, \ldots, u_n\}$$，使用标准的语言建模目标来最大化以下似然值：

$$L_1(\mathcal{U}) = \sum_i \log P(u_i | u_{i-k}, \ldots, u_{i-1}; \Theta)$$

其中$$k$$是上下文窗口的大小，条件概率$$P$$通过具有参数$$\Theta$$的神经网络进行建模。这些参数使用随机梯度下降法进行训练。**GPT-1** 中使用多层 Transformer 解码器，即 **Multi-Head Self-Attention**，并在之后增加前馈网络层，最后输出一个分布：

$$h_0 = U W_e + W_p \\ 
h_l = \text{Transformer\_block}(h_{l-1}),  \forall l \in [1, n] \\
P(u) = \text{Softmax}(h_n W_e^\top)
$$

其中$$U = (u_{-k}, \ldots, u_{-1})$$是 token 的上下文向量，$$n$$是层数，$$W_e$$是 token 嵌入矩阵，$$W_p$$是位置嵌入矩阵。

训练时模型遵守 Transformer 的原始工作，训练了一个只有解码器的 12 层 Transformer，具有带掩码的自注意力，词编码的长度为 768 并且设置 12 个注意力头。对于前馈网络使用 3072 的隐藏层维度。训练时使用 Adam 优化器，最大学习率为 2.5e-4。学习率在最初的 2000 步中从零线性增长，并通过余弦退火调度算法逐渐减少到 0。训练的批量大小为 64，epoch 设置为 100，每个序列包含 512 个 token。在整个模型中使用了层归一化，权重初始化设置为$$N(0, 0.02)$$。使用字节对编码 **BPE**，共有 40000 个字节对，并且使用 drop 比例为 0.1 的 dropout 用于正则化。采用了 L2 正则化的修改版本，使用$$w=0.01$$的非偏置或增益权重。激活函数使用高斯误差线性单元 GELU。使用可学习的位置编码而不是 Transformer 中的三角式位置编码。使用 **ftfy&#x20;**&#x5E93;来清理 BooksCorpus 中的原始文本，标准化一些标点符号和空白，并使用 spaCy 分词器。

训练时通过准确率来评价训练何时停止：训练的时候生成的文本和原文本进行比对，得到准确率，通过准确率是否达到预期值或是准确率是否一直上下波动等来确定是否该停止训练。

* **微调**

有了预训练的语言模型之后，对于有标签的训练集$$\mathcal{C}$$ ，给定输入序列$$x^1, \cdots, x^m$$和标签$$y$$，可以通过模型得到$$h_l^m$$，经过输出层后对$$y$$进行预测：

$$P(y | x^1, \cdots, x^m) = \text{softmax}(h_l^m W_y)$$

则目标函数为：

$$L_2(\mathcal{C}) = \sum_{(x, y)} \log P(y | x^1, \ldots, x^m)$$

**GPT-1** 发现将语言模型去作为辅助对象来参与微调可以帮助提升监督模型的泛化性能及加速收敛。并且使用这个辅助目标函数可以提升性能，因此使用如下目标函数来进行优化：

$$L_3(\mathcal{C}) = L_2(\mathcal{C}) + \lambda * L_1(\mathcal{C})$$

对于不同 NLP 任务的微调过程：

> 1. **分类**：输入就是文本，最后一个词的向量直接作为微调的输入，得到最后的分类结果
>
> 2. **推理**：输入&#x662F;**`先验+分隔符+假设`**，最后一个词的向量直接作为微调的输入，得到最后的二分类结果，即：**是否成立**
>
> 3. **相似性**：输入是两个句子相互颠倒，得到的最后一个词的向量再相加，然后通过线性层，得到最后分类结果，即：**是否相似**
>
> 4. **问答**：输入是上下文和问题放在一起与多个回答，中间也是分隔符分隔，对于每个回答构成的句子的最后一个词的向量作为微调的输入，然后通过线性层，将多个结果进行 Softmax，得到最后概率最大的作为结果

微调时在分类器中加入了 dropout，drop 比率设为 0.1。对于大多数任务采用的学习率为 6.25e-5，批量大小为32，大多数情况下 3 个 epoch 的训练就可以。使用线性的学习率衰减调度，并在训练的前 0.2% 进行 warmup。参数$$λ=0.5$$。

通过训练，**GPT-1** 在多个 NLP 任务上展示了强大的性能。在有监督学习的 12 个基准任务中，**GPT-1** 在 9 个任务上的表现超过了当时的最佳模型。在 Zero-shot 任务中，**GPT-1** 也显示出较好的稳定性，并且随着训练次数的增加，性能逐渐提升。这表明 **GPT-1** 具有较强的泛化能力，可以应用于与训练任务不直接相关的其他 NLP 任务中。虽然 **GPT-1** 在未经微调的任务上虽然也有一定的效果，但其性能通常低于经过微调的有监督任务。这说明 **GPT-1** 在语言建模方面取得了显著进展，但仍需要在特定任务上进行微调才能充分发挥其潜力。因此，**GPT-1** 可以被视为一个强大的领域专家，但还不是一个通用的语言学家。

**代码实现**





**总结**

**GPT-1** 是 OpenAI 于 2018 年推出的首个生成式预训练模型，采用 **Transformer** 架构，通过在大规模未标注文本上进行无监督预训练，然后在特定任务上进行有监督微调，实现了对多种自然语言处理任务的有效应用。

**优点**

**预训练与微调相结合**：通过无监督预训练和有监督微调的两阶段训练方式，提升了模型在多种 NLP 任务上的表现。

**基于 Transformer 架构**：利用 Transformer 的自注意力机制，增强了模型对长距离依赖的捕捉能力。

**迁移学习能力**：预训练的模型可以通过微调适应不同的下游任务，体现了良好的迁移学习能力。

**缺点**

**单向语言模型**：仅考虑从左到右的上下文，限制了对双向上下文信息的理解。

**参数规模相对较小**：相比后续模型，参数量为 1.17 亿，模型容量有限，影响了对复杂语言现象的建模能力。

**对特定任务依赖微调**：需要针对每个下游任务进行微调，增加了应用的复杂性和成本。

### 4.2.2 GPT-2

**GPT-2** 的核心思想就是，当模型的容量非常大且数据量足够丰富时，仅仅靠语言模型的学习便可以完成其他有监督学习的任务，不需要在下游任务微调。**GPT-2** 依然沿用 **GPT-1** 单向 Transformer 的模式，只不过使用了更多的网络参数和更大的数据集，以此来训练一个泛化能力更强的词向量模型。

**GPT-2** 相比于 **GPT-1&#x20;**&#x6709;如下几点区别：

> 1. **使用 Zero-shot**，而 **GPT-1** 为预训练和微调
>
> 2. **字典更大**：同样使用了使用字节对编码构建字典，但字典大小从 40000 增加到 50257
>
> 3. **模型更大**：将 Transformer 堆叠层数增加到 48 层，隐层的维度增至 1600，参数量达到了 15 亿，而 **GPT-1** 只有 1 亿。
>
> 4. **数据集更大**：**GPT-2** 的文章取自于 Reddit 上高赞的文章，命名为 WebText。数据集共有约 800 万篇文章，累计体积约 40G。为了避免和测试集的冲突，WebText 移除了涉及 Wikipedia 的文章。而 **GPT-1** 只有 5GB。
>
> 5. **训练参数变化**：batch\_size 从 64 增加到 512，上文窗口大小从 512 增加到 1024

![]()

首先来看 Zero-shot，One-shot 和 Few-shot：

**Zero-shot**：只给出任务描述和任务提示 prompt

**One-shot**：给出任务描述，给出一个例子，给出任务提示

**Few-shot**：给出任务描述，给出若干个例子，给出任务提示

![]()

![]()

![]()

对于模型来说，**GPT-2** 去掉了 fine-tuning 层，因为不再针对不同任务分别进行微调建模，而是不定义这个模型应该做什么任务，模型会自动识别出来需要做什么任务。fine-tuning 去掉后，引入大量的训练文本，效果就非常好，这也说明只要训练文本够大，网络够大，模型是可以自己根据输入内容判断需要做的任务是什么的。当然 **GPT-2** 的输入也会加入提示词 prompt。并且将 Layer Normalization 放到每个 sub-block 之前，即为了降低训练难度将 post-norm 改成了 pre-norm，并在最后一个 Self Attention 后再增加一个 Layer Normalization。将残差层的初始化值用$$\frac{1}{\sqrt{N}}$$进行缩放，其中$$N$$是残差层的个数。

> **注**：**GPT-2** 认为，当一个语言模型的容量足够大时，它就足以覆盖所有的有监督任务，也就是说所有的有监督学习都是无监督语言模型的一个子集。例如当模型训练&#x5B8C;**`Micheal Jordan is the best basketball player in the history`**&#x8BED;料的语言模型之后，便也学会&#x4E86;**`question: "who is the best basketball player in the history ?"，answer: "Micheal Jordan"`**&#x7684; Q\&A 任务。**GPT-2** 的核心思想概括为：任何有监督任务都是语言模型的一个子集，当模型的容量非常大且数据量足够丰富时，仅仅靠训练语言模型的学习便可以完成其他有监督学习的任务。

![]()

![117M                      345M                     762M                     1542M]()

通过训练，**GPT-2** 达到了不错的效果：

> 1. 在 8 个语言模型任务中，仅仅通过 Zero-shot 学习，**GPT-2** 就有 7 个超过了当时的 SOTA 方法
>
> 2. &#x5728;**`Children's Book Test`**&#x6570;据集上的命名实体识别任务中，**GPT-2** 超过了当时的 SOTA 方法约7%
>
> 3. **`LAMBADA`**&#x662F;测试模型捕捉长期依赖的能力的数据集，**GPT-2** 将困惑度从 99.8 降到了 8.6
>
> 4. 在阅读理解数据中，**GPT-2** 超过了 4 个 baseline 模型中的三个
>
> 5. 在法译英任务中，**GPT-2** 在 Zero-shot 学习的基础上，超过了大多数的无监督方法，但是比有监督的 SOTA 模型要差
>
> 6. **GPT-2** 在文本总结的表现不理想，但是它的效果也和有监督的模型非常接近

**总结**

**GPT-2** 是 OpenAI 于 2019 年发布的基于 Transformer 架构的大型语言模型，拥有约 15 亿参数，在约 800 万网页的数据集上进行训练。该模型能够生成连贯的文本段落，在许多语言建模基准上取得了最先进的表现。从实用性的角度上，**GPT-2** 并没有带来很大的突破，很多情况下效果与 **BERT** 持平，但是 Zero-shot 的训练方式，有效证明了 NLP 领域训练出一个完全通用模型的可行性，这一刻开始，LLM 走 AIGC 的路初见萌芽，因为整个训练流程看起来就像是模型自主学习知识。

**优点**

**强大的生成能力**：**GPT-2** 在生成文本方面具有出色的能力，可以生成连贯、流畅的文章、故事甚至代码片段。&#x20;

**上下文理解**：该模型通过学习大量的文本数据，能够理解上下文并生成具有逻辑关联性的回复。&#x20;

**多领域应用**：**GPT-2** 对于多个领域的任务都具有良好的适用性，包括机器翻译、摘要生成、对话系统等。&#x20;

**缺点**

**可能生成不当内容**：由于训练数据来自互联网，**GPT-2** 可能生成包含偏见或不适当的内容。

**计算资源需求高**：模型规模较大，训练和推理过程需要大量的计算资源。

**缺乏常识推理**：在处理需要常识推理的任务时，**GPT-2** 的表现可能不尽如人意。

### 4.2.3 GPT-3

**Zero-shot** 的方式被 **GPT-2** 认证可行后，OpenAI 就不得不开始考虑模型是否能真正做到强大了，毕竟现在只是和 **BERT&#x20;**&#x6301;平而已。这一刻 OpenAI 开始思考将 LLM 一路走到底，既然模型之大避免不了，那不如来得更彻底一些。**GPT-3** 沿用了做通用模型的思路，把模型做大，达到 175B 参数，同时模型进行升级，使用 **Sparse Transformer**。

**GPT-3** 可以通过少量的样本进行学习，被称&#x4E3A;**`Few-Shot Learner`**。和人类一样，**GPT-3** 只需要看一小部分样例就能学会更多的知识。**GPT-3** 的体量非常庞大，因此在下游任务中进行 fine-tune 的成本很高。为了解决这个问题，**GPT-3** 使用了上下文学&#x4E60;**&#x20;In-context Learning** 的方式，在不进行梯度更新或 fine-tune 的情况下，直接在上下文中进行学习。

* **上下文学习**

**GPT-3** 的出色性能在很大程度上归功于其采用的 **In-context Learning** 方法。为了理解 **In-context Learning**，先来探讨一下元学习 Meta-learning 的概念。元学习的核心思想是通过学习如何学习，来找到一种有效的学习策略或初始化参数，使得模型能够在新的、未见过的任务上快速适应并取得良好的性能。

**In-context Learning** 是元学习思想的一种具体实现，它允许模型在给定一些示例的情况下，直接通过这些示例来学习并完成任务，而无需显式地更新模型参数。在 **GPT-3** 中，这种学习方式被应用于各种 NLP 任务中。具体来说，当给定一个新的任务时，可以向 **GPT-3** 提供少量的示例输入和对应的输出，&#x5373;**`上下文`**，然后让 **GPT-3** 根据这些示例来推断并生成针对新输入的输出。通过这种方式，**GPT-3** 能够在不依赖大量有标签训练数据的情况下，快速适应并完成各种 NLP 任务。

**GPT-3&#x20;**&#x7684; **In-context Learning** 能力得益于其巨大的参数量和训练数据集。通过在大规模无监督文本数据上进行预训练，**GPT-3&#x20;**&#x5DF2;经学习到了丰富的语言知识和模式。这使得它能够在给定少量示例的情况下，快速理解并应用这些知识来完成新任务。同时，**GPT-3** 的巨大参数量也使其具备了强大的表征能力，能够捕捉并表达复杂的语言现象和语义关系。

![]()

* **稀疏注意力**

> https://arxiv.org/pdf/1904.10509

模型结构上，**GPT-3&#x20;**&#x548C; **GPT-1，GPT-2&#x20;**&#x7684;区别主要在于使用了稀疏的自注意力：

**常规自注意力**

![]()

在上图中，左边是注意力矩阵，右边显示了关联性，即每个元素都跟序列内所有元素有关联。因为它要对序列中的任意两个向量都要计算相关度，得到一个$$n^2$$大小的相关度矩阵。从理论上来讲，Self Attention 的计算时间和显存占用量都是$$O(n^2)$$级别的，$$n$$是序列长度。

**膨胀自注意力**

![]()

启发于膨胀卷积，它对相关性进行了约束，要求每个元素只跟它相对距离为$$k,2k,3k,\cdots$$的元素关联，其中$$k>1$$是预先设定的超参数。因此每个元素只跟$$\frac{n}{k}$$个元素算相关性，这样理想情况下运行效率和显存占用都变成了$$O(\frac{n^2}{k})$$，即直接降低到原来的$$\frac{1}{k}$$。

**局部自注意力**

![]()

引入局部关联，约束每个元素只与前后$$k$$个元素以及自身有关联，保留了一个$$2k+1$$大小的窗口，然后在窗口内进行运算，即每个元素只跟$$2k+1$$个元素算相关性，这样一来理想情况下运行效率和显存占用都变成了$$O((2k+1)n)∼O(kn)$$了，也就是说随着$$n$$线性增长，这是很理想的性质，也直接牺牲了长程关联性。

**稀疏自注意力**

![]()

直接将膨胀自注意力和局部自注意力合并为一个，从注意力矩阵上看就是除了相对距离不超过$$k$$的、相对距离为$$k,2k,3k,\cdots$$的注意力都设为$$0$$，这样注意力就具&#x6709;**`局部紧密相关`**&#x548C;**`远程稀疏相关`**&#x7684;特性，这对很多任务来说可能是一个不错的先验，因为真正需要密集的长程关联的任务事实上是很少的。

使用稀疏自注意力的好处：减少注意力层的计算复杂度，节约显存和耗时，从而能够处理更长的输入序列；具有**局部紧密相关**和**远程稀疏相关**的特性，对于距离较近的上下文关注更多，对于距离较远的上下文关注较少。但是缺点是 NLP 中语言内容都是有上下文关系的，如此依赖必定会对长文本建模的效果变差。

* **预训练**

**GPT-3** 的预训练方法，包括模型、数据和训练过程，类似于 **GPT-2**，但是扩大了模型大小、数据集大小和多样性以及训练长度。即：

> 1. 采用了 **96** 层的多头自注意力，头的个数&#x4E3A;**&#x20;96**
>
> 2. 词向量的长度是 **12888**
>
> 3. 上下文划窗的窗口大小提升至 **2048** 个

总参数量达到了 1750亿，上下文学习方法也类似于 **GPT-2**，但系统地探索了不同的上下文学习设置。

**GPT-3** 的训练数据包括低质量的从 CommonCrawl 分片中下载的 **C4**，压缩后的纯文本大小为 45TB，过滤后为570GB，相当于大约 4000 亿个字节对编码的 token；高质量的 WebText2，两个基于互联网的书籍语料库Books1、Books2 和英文维基百科 Wikipedia。其中未经过滤的 Common Crawl 版本质量较低，因此采取了三个步骤提高数据集质量：1. 下载并根据与一系列高质量参考语料库的相似性筛选了一个版本的 Common Crawl；2. 在文档级别执行了模糊去重，既包括数据集内部也跨越不同数据集之间，以防止冗余并保持我们保留验证集的完整性，作为过度拟合的一个准确衡量标准；3. 添加了已知的高质量参考语料库到训练组合中，以增强Common Crawl并增加其多样性。

**GPT-3** 根据数据集的不同的质量赋予了不同的权值，权值越高的在训练的时候越容易抽样到。训练语料的 60% 来自于 C4，22% 来自于 WebText2，16% 来自于 Books，3%来自于 Wikipedia。

训练时，使用了 31 个分工明确的作者，超强算力的计算机（285,000 个 CPU， 10,000 个 GPU），1200 万的训练费用，45TB 的训练数据。通过实验发现：模型越大，测试案例数量越多，最终效果越好。当测试案例很多时，Prompt 变得不那么重要，因为从案例中也可以推断出任务类型。大型模型的更陡峭的上下文学习曲线表明从上下文信息中学习任务的能力有所提高，模型越大，上下文信息的使用效率就越高。

总体来看，在所有 Zero-shot，One-shot 和 Few-shot 三种情况下，对于大多数任务，作者发现模型容量与性能之间存在相对平滑的关系；一个值得注意的模式是，Zero-shot，One-shot 和 Few-shot 的性能差距通常随着模型容量的增加而增大，这可能表明更大的模型更擅长元学习。&#x20;

**总结**

**GPT-3** 是 OpenAI 于 2020 年发布的自回归语言模型，拥有1750亿个参数，是当时参数规模最大的非稀疏语言模型。该模型在大规模文本数据上进行预训练，能够执行多种自然语言处理任务，如语言翻译、问答和自动文本摘要。

**优点**

**强大的语言生成能力**：GPT-3能够生成连贯且富有创意的文本，适用于写作辅助、内容创作等领域。&#x20;

**少样本学习能力**：在提供少量示例的情况下，GPT-3可以执行特定任务，展示了出色的few-shot学习能力。&#x20;

**广泛的应用场景**：GPT-3可用于文本分类、情感分析、代码生成等多种自然语言处理任务。

**缺点**

**计算资源需求高**：由于模型参数庞大，训练和部署GPT-3需要大量的计算资源和存储空间。&#x20;

**可能生成不当内容**：GPT-3可能生成包含偏见或不适当的内容，反映了训练数据中的偏见。&#x20;

**缺乏常识推理能力**：在处理需要常识推理的任务时，GPT-3的表现可能不尽如人意。&#x20;

### 4.2.4 GPT-3.5

**GPT-3** 表现出了强大的零样本理解能力，但在对话方面表现不佳，主要存在以下问题：① 缺少正向价值观，回答不够圆滑；② 回答风格与人类预期有偏差；③ 编造事实。

以上缺点根本原因&#x662F;**&#x20;GPT** 是一个语言模型，它在大规模语料上熟练掌握了各种语法规则、知识网络，但它对人类的偏好不太了解，因此生成的答案不尽人意。这就需要继续训练，让它能够更偏向生成让人满意的答案。继续训练就需要人工标注大量的问答对，完全靠人工标注不现实，那么就需要借助强化学习的方式让它自我学习。

* **强化学习**

强化学习主要应用在游戏领域，一般是用机器控制一方，与对手博弈。以回合制游戏为例，每个轮次机器走一步，对手走一步，游戏进入下一状态，如此重复，直到系统判定游戏结束，其中一方获胜。我们一般把机器看成一个策略函数$$\pi$$，它通过当前状态$$s_t$$，预估下一步所有动作$$a$$的概率，然后以预估的概率随意抽取其中一个动作$$a_t$$执行，对手紧接着做出决策，游戏进入下一个状态$$s_{t+1}$$。

![]()

这个过程一般以两个指标评价这一步动作的价值：

1. $$r_t$$当前这一步的价值，有些游戏每一步后立刻会给出奖励，有的游戏需要玩完本局才能确定。

2. $$u_t$$当前这一步从全局看的价值，$$u_t=r_t+\lambda*r_{t+1}+\lambda^2*r_{t+2}+\cdots$$.

以策略函数$$\pi(a_t|s_t;\theta)$$为学习目标的学习方法被称为策略强化学习。学习过程如下：

> 1. 玩游戏，收集训练数据$$(s_t,a_t,u_t), a_t$$是由$$\pi$$采样得到，$$u_t$$是系统给出的反馈
>
> 2. 在收集的训练数据上，基于目标函数，使用梯度上升（或下降）更新$$\pi$$
>
> 3. 重复以上 **1**, **2** 步，直到达到结束条件

从上可以看出，强化学习是不需要人工标数据的，策略函数一边收集训练数据，一边迭代自己。之所以不需要人工标数据，是因为有一套程序可以自动对动作打分。比如围棋，下到最后，通过程序可以判断哪一方赢了。所以要想用强化学习来训练 **GPT**，得有一个评估答案好坏的程序，显然通过规则没法做到，因此需要先训练一个评估模型 **Reward Model**，然后再通过奖励模型得分来进行强化学习，分为三步：

通过人工标注问答数据，对 **GPT&#x20;**&#x8FDB;行微调，即监督微调 **SFT**（**S**upervised **F**ine-**T**uning）

通过微调后的模型及问题，生成多个答案，人工对答案排序，训练 **RM**（**R**eward **M**odel）

强化学习，以 **RM&#x20;**&#x4E3A;回报函数，使用 **PPO**（**P**roximal **P**olicy **O**ptimization）训练策略模型

![]()

* **监督微调 SFT**

SFT 数据集是用来训练第 1 步有监督的模型，即使用采集的新数据，按照 GPT-3 的训练方式对 GPT-3 进行微调。因为 GPT-3 是一个基于提示学习的生成模型，因此 SFT 数据集也是&#x7531;**`提示-答复`**&#x5BF9;组成的样本。SFT 数据一部分来自使用 OpenAI 的 PlayGround 的用户，另一部分来自 OpenAI 雇佣的 40 名标注工。并且他们对标注工进行了培训，在 SFT 阶段共标注 13k &#x7684;**`(prompt, completion)`**&#x5BF9;问答数据。在这个数据集中，标注工的工作是根据内容自己编写指示，并且要求编写的指示满足下面三点：

![]()

> 1. **简单任务**：标注工给出任意一个简单的任务，同时要确保任务的多样性
>
> 2. **Few-shot 任务**：标注工给出一个指示，以及该指示的多&#x4E2A;**`查询-响应`**&#x5BF9;
>
> 3. **用户相关的**：从接口中获取用例，然后让标注工根据这些用例编写指示

SFT 使用和 GPT 预训练完全一样的网络结构，训练目标也是似然函数最大化：

$$L(\theta)=\prod \limits_{i=1}^{N}\pi^{SFT}(y_i|x_i;\theta)$$

* **奖励模型 RM**

RM 数据集用来训练第 2 步的奖励模型，当然也需要为 **GPT-3.5** 的训练设置一个奖励目标，要尽可能全面且真实的对齐需要模型生成的内容。这可以通过人工标注的方式来提供这个奖励，通过人工可以给那些涉及偏见的生成内容更低的分从而鼓励模型不去生成这些人类不喜欢的内容。**GPT-3.5** 的做法是先让模型生成一批候选文本，然后通过标注工根据生成数据的质量对这些生成内容进行排序。

具体来说，随机抽取一批问题，SFT 后的模型对每个问题生成$$K$$个答案（$$4\le K\le9$$），人工对$$K$$个答案排序，然后两两回答一组，组成一条训练数据，例&#x5982;**`(prompt, A, B)`**，则一共有$$K \choose 2$$条训练数据。

![]()

在这一阶段，一共有 33k 的标注数据被用于训练。在训练时，对于提示$$x$$和所有回答$$y$$将组成一个 batch，通过构造并最小化 **Pairwise Ranking Loss** 的方法，来训练奖励模型。损失函数可以表示为：

$$L(\theta)=-\frac{1}{K \choose 2} E_{(x, y_w, y_l)\sim D} [log(\sigma(r_{\theta}(x, y_w)-r_{\theta}(x, y_l)))]$$

其中$$r_{\theta}(x, y)$$是奖励模型对提示$$x$$和回答$$y$$的标量输出，参数为$$\theta$$，$$y_w$$和$$y_l$$分别表示从$$K$$个答案中任取两个中的相对好、差答案，$$D$$表示prompt下人类标注排序的所有两两回答对，$$\sigma$$表示 sigmoid 函数。

对于训练后的奖励模型，希望当回答$$y$$的排序相对较高时，$$r_\theta(x, y)$$的得分也能越高。为了不让$$K$$的个数影响训练模型，需要在前面乘上$$\frac{1}{K \choose 2}$$，将 loss 平均到每一个答案对上。在训练时要将$$(x, y_w,y_l)\sim D$$当成一个 batch 同时送入模型，而不是将单条$$(x, y_w,y_l)$$数据分别送入模型，因为对于某一对$$(x, y_w,y_l)$$，用 batch 方式时，它只参与一次梯度计算，而用单条方式时，它需要参与$$K-1$$次梯度计算，这避免了过拟合的风险。除此之外也可以提升计算效率。在模型前向传播的过程中，最耗时的步骤是计算$$r_\theta(x, y)$$，用 batch 方式时，该计算只需执行$$K$$次，因为模型参数没有更新，相同的$$(x, y)$$可以重复使用，但采用单条方式时，需要计算$$K(K-1)$$次，因为一条计算更新一次模型，模型参数更新，相同的$$(x,y)$$需要重新计算。因此，$$K$$越大时，采用 batch 的方式越划算，因为它在保证相对排序信息丰富的同时，又节省了计算效率。

* **人类反馈强化学习 RLHF**

通过第 2 步得到的奖励模型来指导 SFT 模型的继续训练，让模型能够更好的对齐人类意图。这里 **GPT-3.5** 使用改良版本的 **PPO** 对 GPT 再次训练，但 **GPT-3.5** 的 **PPO** 数据没有进行标注，它均来自 GPT-3 的 API 的用户。其中有不同用户提供的不同种类的生成任务，占比最高的包括生成任务（45.6%），QA（12.4%），头脑风暴（11.2%），对话（8.4%）等。

> **注**：**PPO** 算法是一种新型的 Policy Gradient 算法，Policy Gradient 算法对步长十分敏感，但是又难以选择合适的步长，在训练过程中新旧策略的的变化差异如果过大则不利于学习。**PPO** 提出了新的目标函数可以在多个训练步骤实现小批量的更新，解决了 Policy Gradient 算法中步长难以确定的问题。

![]()

强化学习的三要素：**`策略`**、**`动作空间`**&#x548C;**`奖励函数`**。策略就是基于该语言模型，接收 prompt 作为输入，然后输出一系列文本的概率分布；动作空间就是词表所有 token 在所有输出位置的排列组合；奖励函数则是基于训好的 RM 模型计算得到 reward。具体的强化学习目标函数如下：

$$\text{objective}(\phi) = E_{(x, y) \sim D_{\pi_{\phi}^{\text{RL}}}} \left[ r_\theta(x, y) - \beta \log \left( \frac{\pi_{\phi}^{\text{RL}}(y \mid x)}{\pi^{\text{SFT}}(y \mid x)} \right) \right] + \gamma E_{x \sim D_{\text{pretrain}}} \left[ \log \left( \pi_{\phi}^{\text{RL}}(x) \right) \right]$$

其中$$\pi_{\phi}^{\mathrm{RL}}$$表示我们此刻要学的强化学习模型，又称&#x4E3A;**`策略`** policy，$$\pi^{\mathrm{SFT}}$$表示在第一步骤中，经过 SFT 的 GPT 模型，初始时，$$\pi_{\phi}^{\mathrm{RL}} = \pi^{\mathrm{SFT}}$$。$$r_\theta$$表示第二步骤中的奖励模型。

目的要最大化该损失函数，这里对公式每一项进行详细解释

> 1. $$E_{(x, y) \sim D_{\pi_{\phi}^{\mathrm{RL}}}}$$，$$x$$表示某个 prompt，$$y$$表示把$$x$$送入当前状态的强化学习模型中所产生的输出
>
> 2. $$r_\theta(x, y)$$表示当前强化模型下，将$$x$$和其所产生的$$y$$送入奖励模型进行打分，这个分数越高越好
>
> 3. $$\log (\frac{\pi_{\phi}^{\mathrm{RL}}(y \mid x)}{\pi^{\mathrm{SFT}}(y \mid x)})$$表示 KL 散度，用于比较两个模型的输出分布是否相似，KL 值越大，分布越不相似，分布相同时$$KL=0$$。在本阶段中希望强化学习后得到的 GPT，在能够理解人类意图的基础上，又不要和最原始的 GPT 的输出相差太远，即防止大模型训偏。参数$$\beta$$则表示对这种偏差的容忍程度，偏离越远，就要从奖励模型的基础上得到越多的惩罚。截止到这一步，称为 PPO
>
> 4. $$E_{x \sim D_{\text {prertain }}}$$中，$$D_{\text {prertain}}$$表示在 SFT 之前，最初始的 GPT3 模型，$$x$$表示来自初始模型产出的数据
>
> 5. $$\log \left(\pi_{\phi}^{\mathrm{RL}}(x)\right)$$表示将来自初始 GPT 中的数据送入当前强化模型下，同样是希望当前强化模型输出分布不要偏离太多。$$\gamma$$则是对这种偏离的惩罚程度。添加上这一项以后的优化策略，称为PPO-ptx

**总结**

GPT-3.5 是 OpenAI 在 GPT-3 基础上改进的语言模型，于 2022 年底推出。该模型通过在大规模互联网文本数据上进行预训练，并结合人工标注数据和强化学习，提升了模型的推理和生成能力。GPT-3.5 具备更强的上下文理解和生成连贯文本的能力，在对话系统、内容创作等领域表现出色。

**优点**

**增强的真实性和价值观对齐**：通过引入人工标注，GPT-3.5 的输出在真实性和符合人类价值观方面有显著提升。

**更自然的响应**：得益于 GPT-3 的强大泛化和生成能力，结合人工提示编写和结果排序，GPT-3.5 生成的响应更加自然。

**无害性有所提升**：虽然提升有限，但 GPT-3.5 在减少有害内容生成方面表现出一定改进。

**缺点**

**通用 NLP 任务性能下降**：在特定任务上的优化可能导致模型在其他通用自然语言处理任务上的性能有所下降。

**可能生成荒谬的输出**：尽管引入了人类反馈，GPT-3.5 有时仍会生成不真实或不合理的内容。

**对指令敏感且可能误解简单概念**：模型对输入指令的敏感性较高，可能会对简单概念进行过度解读，导致输出不符合预期。

### 4.2.5 GPT-4

**GPT-4** 是 OpenAI 发布的最新 GPT 系列模型。它是一个大规模多模态模型，相&#x6BD4;**&#x20;GPT-3.5**，**GPT-4** 可以接受图像和文本两种形式的输入，产生文本输出。输出依旧是一个自回归的单词预测任务。技术上，**GPT-4** 采用了专家混合技术，进一步增强模型的能力。整体来说，**GPT-4** 在各种专业和学术基准上表现出了人类的水平，对于生成式的幻觉、安全问题均有较大的改善。

* **模型结构**

**GPT-4** 的体系结构由 16 个不同的专家模型组成，每个模型都有 111B 个参数，总计约 1.76 万亿个参数。除了更大的参数规模外，另一个重要的细节是 GPT-4 使用了专家混合 **MoE**（**M**ixture **o**f **E**xperts）架构，这意味着模型中的不同组件协同工作，每个组件都有助于最终输出。在 **GPT-4** 中 Attention 的参数量有 55B，**MoE** 的参数量是 111B \* 16，一共 120 层 Transformer。每个 token 会通过一个路由算法选择两个 MLP 进行计算，参数 seq\_len 为 8k，每个 MLP 分到 1k 个 token。



![]()

**GPT-4** 的模型宽度，深度基本和 GPT-3 (175B) 差不多，区别在于 MLP 的数量要多 16 倍。

* **并行策略**

**GPT-4** 训练采用的并行策略是：

> * **张量并行**：8
>
> * **流水并行**：16
>
> * **数据并行**：196

总计使用约 3125 台机器（25000 张 A100）进行训练。其中 batch size 为 60M token，seq\_len 为 8k。

张量并行和流水并行包含了 **GPT-4** 完整的模型参数，其结构如右图所示。其中，张量并行通讯耗时占比小于 15%，PipeDream 流水线气泡占比 28% 左右，Interleaved 1F1B 流水线气泡占比 16% 左右。



![]()

**MoE** 模型是一种在模型增强的道路达到极限时采用的实用方法，由多个专家组成，每个专家都精通某个特定领域，然后利用他们的集体智慧，协作以提供增强的结果。如下图所示：

![]()

![]()

对于 **MoE** 模型进行并行训练，这里做一个初步的估计：按照目前最大的 8 路张量并行，16 路流水线并行计算，每个 GPU 上的参数量是 14B，假设用 FP32 的梯度，则参数和梯度需要占用 84 GB 的显存，目前市面上还没有能完整放下显卡。因此，**GPT-4** 大概率会使用专家并行（Expert Parallelism）技术来节省显存占用。

* **模型能力**

**GPT-4** 做的这些改进效果是显著的，相比前代 **GPT-3.5**，他具备了新的能力：

> 1. **具有图像理解能力**：突破纯文字的模态，增加了图像模态的输入
>
> 2. **支持更长的上下文窗口**：**GPT-4&#x20;**&#x7684; seq\_len 为 8k，&#x662F;**&#x20;GPT-3.5&#x20;**&#x4E0A;下文长度的 2 倍
>
> 3. **复杂任务处理能力大幅提升**：**GPT-4** 在更复杂、更细微的任务处理上，回答更可靠、更有创意
>
> 4. **改善幻觉、安全等局限性**：LLM 的生成模式不可避免会有一些幻觉问题。**GPT-4&#x20;**&#x5728;测试中发现其可以显著减轻在各类任务上幻觉问题，&#x6BD4;**&#x20;GPT-3.5&#x20;**&#x6A21;型提高 40% 左右。同样在安全能力的升级上，**GPT-4** 明显超出 **GPT3.5**

**总结**

**GPT-4** 是否会引发第四次工业革命仍需要时间验证，但其对人工智能领域的影响巨大。它挑战了传统人工智能的认知，展示出超越深度学习传统理解的零样本学习能力和更高阶的推理能力。**GPT-4&#x20;**&#x5DF2;成为日常工作中的得力助手，广泛应用于代码编写、文章修改以及解决非工作相关问题。此外，随着多种大模型的出现，深度学习领域重新焕发了活力。

未来，GPT-4 预预计将在多个方面产生深远影响：首先，互联网上将涌现大量 **GPT-4&#x20;**&#x751F;成的内容，需要警惕其对大众行为模式的潜在影响；其次，**GPT-4&#x20;**&#x5C06;在提高某些工作生产力的同时，也有可能替代一些工作岗位，为社会带来新的机会；最后，**GPT-4&#x20;**&#x5982;何影响个体将因人而异，但若它助推了 AGI 的发展，将为科技进步带来前所未有的机遇。

**优点**

**更强的推理能力**：**GPT-4** 相比于前代模型，在推理、理解复杂上下文、解决多轮对话等方面有显著提升。它能够处理更为复杂的问题，并生成更精确的回答。

**多模态支持**：**GPT-4** 支持多模态输入（文本和图像），使其能够理解和生成多种类型的数据，拓展了应用场景。例如，可以处理图像描述、图文结合的问答等任务。

**更好的知识整合与准确性**：**GPT-4** 在处理事实性问题时，比 **GPT-3** 和 **GPT-3.5** 更加准确，减少了虚假信息的生成。它能够更好地整合从多个来源获得的知识。

**缺点**

**计算资源消耗大**：**GPT-4** 的模型规模非常庞大，训练和部署所需的计算资源和存储空间远高于前代模型。这对研究和开发来说是一个挑战。

**仍存在偏见和有害输出**：虽然 **GPT-4** 在减少有害内容方面做出了改进，但它仍然可能生成带有偏见、不适当或有害的输出。这种问题在多模态生成中尤为复杂。

**推理仍有限**：虽然 **GPT-4** 提升了推理能力，但在面对复杂的推理问题时，它仍然可能出现错误，尤其是在缺乏足够上下文或知识的情况下。这使得它在某些情况下的可靠性较低。

### 4.2.6 GPT-4o

### 4.2.7 GPT-4.5

### 4.2.8 OpenAI o1

### 4.2.9 OpenAI o3 & o4

### 4.2.10 GPT-OSS

## 4.3 LLaMA 系列

> **论文**：**[LLaMA-1](https://arxiv.org/pdf/2302.13971)  [LLaMA-2](https://arxiv.org/pdf/2307.09288)  [LLaMA-3](https://arxiv.org/pdf/2407.21783)**
>
> **代码**：**[LLaMA-1](https://github.com/meta-llama/llama/tree/llama_v1)  [LLaMA-2](https://github.com/meta-llama/llama/tree/llama_v2)  [LLaMA-3](https://github.com/meta-llama/llama3)**

### 4.3.1 LLaMA-1

**LLaMA**（**L**arge **L**anguage **M**odel Meta **A**I）是 由 Meta AI 发布的一个开放且高效的大型基础语言模型，共有 **`7B`**、**`13B`**、**`33B`**、**`65B`**&#x56DB;种版本。

![]()

其数据集来源都是公开数据集，无任何定制数据集，保证了其工作与开源兼容和可复现，整个训练数据集在 token 化之后大约包含 **`1.4T`** 的 token。其中，**LLaMA-65B** 和 **LLaMA-33B** 是在 1.4万亿个 token 上训练的，而最小的模型 **LLaMA-7B** 是在 1万亿个 token 上训练的。具体的模型参数如上表。

最近有工作表明了，在给定的计算预算下，最佳性能不是由最大的模型实现的，而是基于更多数据上的训练较小模型实现的。因此 **LLaMA** 的重点是基于更多 token 的训练集，在各种推理预算下，训练出性能最佳的一系列语言模型，与现有最佳 LLM 相比，其性能是有竞争力的：具有 130 亿参数的 **LLaMA** 模型在大多数基准上可以胜过有 1750 亿参数的 **GPT-3**，而且**可以在单块 V100 GPU 上运行**；而最大的 650 亿参数的 **LLaMA** 模型可以媲美谷歌的 **Chinchilla-70B** 和 **PaLM-540B**。**LLaMA** 认为这有助于使 LLM 的使用和研究平民化，因为它可以在极少的 GPU 上运行。

**LLaMA** 优势在于其只使用公开可用的数据，这可以保证论文的工作与开源兼容和可复现。之前的大模型要么使用了不公开的数据集去训练从而达到了 SOTA，如 **Chinchilla**、**PaLM** 或 **GPT-3**；要么使用了公开数据集，但模型效果不是最佳无法和 **PaLM-62B** 或 **Chinchilla** 相竞争，如 **OPT**、**GPT-NeoX**、**BLOOM** 和 **GLM**。

* **模型结构**

和 **GPT** 系列一样，**LLaMA** 模型也是 **Decoder-only** 架构，但结合前人的工作做了一些改进：

> 1. **Pre-normalization**：借鉴 **GPT-3**，为了提高训练稳定性，**LLaMA** 对每个 Transformer 子层的输入进行归一化，使用 **`RMSNorm`** 归一化函数，好处是不用计算样本的均值，速度提升了 40%。
>
> 2. **FFN\_SWiGLU**：借鉴 **PaLM**，结构上使用门控线性单元，且为了保持 **FFN** 层参数量不变，将隐藏单元的数量调整为$$\frac{8}{3}d$$而不是 **PaLM** 论文中的$$4d$$，同时将 **`ReLU`** 替换为 **`SiLU`**&#x4EE5;提高性能。
>
> 3. **RoPE**：借鉴 **GPTNeo**，模型的输入不再使用 positional embeddings，而是在网络的每一层添加了 **`RoPE`**。

完整的模型结构图如右图所示:



![]()

**RMSNorm**

> https://arxiv.org/pdf/1910.07467

**RMSNorm**（**R**oot **M**ean **S**quare Layer **Norm**alization）假设 **LayerNorm** 中的重新中心化不再是必须的，即平移不变性不重要，并提出了一种新的归一化方法：**均方根层归一化 RMSNorm**。**RMSNorm** 通过**均方根 RMS** 对每一层神经元的输入进行归一化，使模型具备重新缩放不变性和隐式学习率调整的能力。相比 **LayerNorm**，**RMSNorm** 计算更为简洁，大约可以节省 7% 到 64% 的运算。

**RMSNorm** 对每个 token 的特征向量进行归一化计算。设某个 token 的特征向量为$$\textrm{x}\in \mathbb{R}$$，**RMSNorm** 的计算如下：

$$\text{RMSNorm}(x): \hat{x}_i = \gamma \odot \frac{x_i}{\text{RMS}(x)} \\ \text{RMS(x)} = \sqrt{\frac{1}{d} \sum_{x_i \in \textrm{x}} x_i^2 + \epsilon}$$

其中，$$\gamma$$是可学习的缩放参数，$$\epsilon$$的作用是为了保持数值稳定性。$$d$$为输入 token 的数量。

**代码实现**

```python
class RMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6):
        """
        初始化RMSNorm层。
        :param dim (int): 输入特征的维度大小。
        :param eps (float): 防止除零的小常数，默认为1e-6。
        """
        super().__init__()
        self.eps = eps  # 小的常数值，用于数值稳定性
        self.weight = nn.Parameter(torch.ones(dim))  # 可学习的权重参数，初始化为1

    def _norm(self, x):
        """
        应用RMS归一化。
        :param x (Tensor): 输入张量。
        :return (Tensor): 归一化后的张量。
        """
        # 计算最后一个维度上的平方平均值，加上eps以确保数值稳定性，然后取平方根的倒数，最后与x相乘
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        """
        定义前向传播过程。
        :param x (Tensor): 输入张量。
        :return (Tensor): 归一化并缩放后的输出张量。
        """
        # 对输入进行归一化，并转换回原始数据类型
        output = self._norm(x.float()).type_as(x)
        # 将归一化后的输出与可学习权重相乘，实现缩放
        return output * self.weight
```

**FFN\_SwiGLU**

> https://arxiv.org/pdf/2002.05202

**FFN** 计算过程用数学公式可表达为$$\text{FFN}(x, W_1, W_2, b_1, b_2) = \text{max}(0, xW_1 + b_1 )W_2 + b_2$$

在 **T5** 中，使用的是没有偏置的版本，数学公式表达为$$\text{FFN}(x, W_1, W_2) = \text{max}(0, xW_1)W_2$$

后续的研究提出了用其他非线性激活函数替换 ReLU，如高斯误差线性单元 **GELU**（**G**aussian **E**rror **L**inear **U**nits）：$$\text{GELU}(x) = x\Phi (x)$$和自门控激活函数$$\text{Swish}_{\beta}(x) = x\sigma(\beta x)$$，其中$$\sigma$$为 $$\text{Sigmoid}$$激活函数：

$$\text{FFN}_{\text{GELU}}(x, W_1, W_2) = \text{GELU}(xW_1)W_2 \\ \text{FFN}_{\text{Swish}}(x, W_1, W_2) = \text{Swish}_1(xW_1)W_2$$

其中激活函数$$\text{Swish}(x) = x⋅ \text{Sigmoid}(\beta x) = \frac{x}{1 + e^{-\beta x}}$$，Sigmoid 函数$$\sigma(x) = \frac{1}{1 + e^{-x}}$$。$$\beta$$可以是常数或可训练参数。下图展示了不同$$\beta$$值下的 Swish 曲线。&#x20;

> 1. 如果$$\beta = 1$$，Swish 等价于 Sigmoid 加权线性单&#x5143;**`SiLU`**&#x20;
>
> 2. 当$$\beta = 0$$时，Swish 变为缩放线性函数$$f(x) = \frac{x}{2}$$
>
> 3. 随着$$\beta$$趋近于无穷大，Swish 变得与 ReLU 函数相似。这表明 Swish 可以被看作是一个平滑的函数，在线性函数和 ReLU 之间进行非线性插值。如果将$$\beta$$设置为可训练参数，模型可以调控这种插值的程度

![]()

**GLU**（**G**ated **L**inear **U**nits）定义为输入的两个线性变换的逐元素乘积，其中一个经过了 Sigmoid 激活。另外还有省略激活函数版本，称之为双线性层：

$$\text{GLU}(x, W, V, b, c) = \sigma(xW+b)\otimes (xV+c) \\ \text{bilinear}(x, W, V, b, c) = (xW+b)\otimes (xV+c)$$

当然，也使用其他激活函数定义 GLU 变体，如:&#x20;

$$\text{ReGLU}(x, W, V,b, c) = \text{max}(0, xW+b)\otimes (xV+c) \\ \text{GEGLU}(x, W, V,b, c) = \text{GELU}(xW+b)\otimes (xV+c) \\ \text{SwiGLU}(x, W, V,b, c, \beta) = \text{Swish}_\beta(xW+b)\otimes (xV+c)$$

基于此，可以衍生出很多 Transformer FFN 层的变体，这些变体用 GLU 或其变体之一来替代原来的第一层线性变换和激活函数，和 FFN 一样，也省略了偏置项，这些 FFN 变体数学表达式如下所示:

$$\text{FFN}_{\text{GLU}}(x, W, V, W_2) = (\sigma(xW) \otimes xV)W_2 \\
\text{FFN}_{\text{Bilinear}}(x, W, V, W_2) = (xW \otimes xV)W_2 \\
\text{FFN}_{\text{ReGLU}}(x, W, V, W_2) = (\max(0, xW) \otimes xV)W_2 \\
\text{FFN}_{\text{GEGLU}}(x, W, V, W_2) = (\text{GELU}(xW) \otimes xV)W_2 \\
\text{FFN}_{\text{SwiGLU}}(x, W, V, W_2) = (\text{Swish}_1(xW) \otimes xV)W_2$$

**LLaMA** 对 FFN 的改进结构$$\text{FFN}_{\text{SwiGLU}}$$使用了$$\beta=1$$&#x7684;**`Swish`** 激活函数，&#x5373;**`SiLU`**，$$\text{SiLU}(x) = x⋅ \sigma(x)$$，**LLaMA** 官方提供的代码使用 **`F.silu()`** 激活函数，具体的数学表达式如下:

$$\text{FFN}_{\text{SwiGLU}}(x, W, V, W_2) = (\text{SiLU}(xW)\otimes xV)W_2$$

这其实就是最后一个 FFN 层的变体。

**代码实现**

```python
class FFN(nn.Module):
    """
    实现 FFN_SwiGLU，包含三个线性变换层。
    """
    def __init__(
        self,
        dim: int,                # 输入特征维度
        hidden_dim: int,         # 隐藏层初始维度
        multiple_of: int,        # 确保隐藏层维度是该数的倍数
    ):
        super().__init__()
        
        # 调整隐藏层维度：首先缩小到约2/3，然后调整为最接近的multiple_of的倍数
        hidden_dim = int(2 * hidden_dim / 3)
        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)

        # w1 和 w3 是线性层，用于将输入映射到隐藏空间
        self.w1 = nn.Linear(dim, hidden_dim, bias=False)
        self.w3 = nn.Linear(dim, hidden_dim, bias=False)

        # w2 是线性层，用于将隐藏表示转换回输出空间
        self.w2 = nn.Linear(hidden_dim, dim, bias=False)

    def forward(self, x):
        """
        定义前向传播过程。
        :param x (Tensor): 输入张量。
        :return (Tensor): 经过网络变换后的输出张量。
        """
        # 使用SiLU激活函数对w1的输出进行非线性变换，然后与w3的输出相乘，
        # 最后通过w2线性变换输出结果
        return self.w2(F.silu(self.w1(x)) * self.w3(x))
```

**旋转位置编码**

除此之外，**LLaMA** 还使用了旋转位置编码 **RoPE**，详见“位置编码”章节，这里进行完整的代码实现：

**代码实现**

首先定义一些模型配置

```python
@dataclass
class ModelArgs:
    dim: int = 512
    n_layers: int = 8
    n_heads: int = 8
    vocab_size: int = -1  # 之后由 tokenizer 定义
    multiple_of: int = 256  # 使 SwiGLU 隐藏层大小成为 2 的较大幂的倍数
    norm_eps: float = 1e-5
    max_batch_size: int = 32
    max_seq_len: int = 2048
```

实现注意力机制

```python
class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        
        # 初始化头的数量和每个头的维度
        self.n_local_heads = args.n_heads  # 因为去除了模型并行，这里直接使用总的头数
        self.head_dim = args.dim // args.n_heads

        # 使用基础的线性层替代并行线性层
        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)
        self.wk = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)
        self.wv = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)
        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)

        # 缓存用于存储过去的键和值，以实现高效的解码过程
        self.cache_k = torch.zeros(
            (args.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)
        ).cuda()
        self.cache_v = torch.zeros(
            (args.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)
        ).cuda()

    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):
        bsz, seqlen, _ = x.shape  # 获取批量大小、序列长度以及输入特征维度
        
        # 线性变换输入张量到查询、键和值空间
        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)

        # 调整形状以适应多头注意力计算
        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)
        xk = xk.view(bsz, seqlen, self.n_local_heads, self.head_dim)
        xv = xv.view(bsz, seqlen, self.n_local_heads, self.head_dim)

        # 应用旋转位置嵌入（rotary position embeddings）
        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)

        # 将缓存移动到正确的设备，并更新缓存
        self.cache_k = self.cache_k.to(xq)
        self.cache_v = self.cache_v.to(xq)

        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk
        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv

        # 获取当前和过去的键和值
        keys = self.cache_k[:bsz, : start_pos + seqlen]
        values = self.cache_v[:bsz, : start_pos + seqlen]

        # 调换维度以准备进行矩阵乘法运算
        xq = xq.transpose(1, 2)
        keys = keys.transpose(1, 2)
        values = values.transpose(1, 2)

        # 计算注意力分数
        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)
        
        # 如果有掩码，则应用掩码
        if mask is not None:
            scores = scores + mask  # 添加掩码到分数中

        # 应用softmax函数获取注意力权重
        scores = F.softmax(scores.float(), dim=-1).type_as(xq)
        
        # 加权求和得到输出
        output = torch.matmul(scores, values)  # 注意力加权的值向量
        
        # 调整输出形状，并通过最终线性层映射回原始特征空间
        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)

        return self.wo(output)  # 返回注意力机制处理后的结果
```

实现 Transformer Block

```python
class TransformerBlock(nn.Module):
    def __init__(self, layer_id: int, args: ModelArgs):
        super().__init__()
        
        # 初始化超参数
        self.n_heads = args.n_heads  # 注意力头的数量
        self.dim = args.dim  # 输入/输出特征维度
        self.head_dim = args.dim // args.n_heads  # 每个头的维度
        
        # 初始化子模块
        self.attention = Attention(args)  # 多头自注意力机制
        self.feed_forward = FFN(
            dim=args.dim, 
            hidden_dim=4 * args.dim,  # 前馈网络的隐藏层维度通常是输入维度的四倍
            multiple_of=args.multiple_of  # 这可能是为了确保某些维度是特定值的倍数
        )
        self.layer_id = layer_id  # 层的标识符，有助于在多层模型中识别每一层
        
        # 归一化层，用于在应用注意力机制和前馈网络之前对输入进行归一化
        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)
        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)

    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):
        """
        定义了Transformer块的前向传播过程。
        :param x (torch.Tensor): 输入张量，形状为(batch_size, sequence_length, dim)
        :param start_pos (int): 当前序列片段的起始位置，用于解码时更新缓存
        :param freqs_cis (torch.Tensor): 旋转位置嵌入，用于提升模型对位置信息的理解
        :param mask (Optional[torch.Tensor]): 可选的掩码张量，用于掩盖不必要的部分（例如，在自回归解码中）
        :return out (torch.Tensor): 经过一层Transformer处理后的输出张量
        """
        # 注意力层：先对输入x进行归一化，然后通过多头自注意力机制，最后加上原始输入x形成残差连接
        h = x + self.attention.forward(self.attention_norm(x), start_pos, freqs_cis, mask)
        
        # 前馈层：对注意力层的输出h进行归一化，然后通过前馈神经网络，再加上之前的残差h
        out = h + self.feed_forward.forward(self.ffn_norm(h))
        
        return out  # 返回最终的输出，准备传递给下一层或作为模型输出
```

实现 LLaMA

```python
class Transformer(nn.Module):
    def __init__(self, params: ModelArgs):
        super().__init__()
        
        # 保存模型参数
        self.params = params
        self.vocab_size = params.vocab_size  # 词汇表大小
        self.n_layers = params.n_layers  # 层数

        # 使用基础的nn.Embedding层代替ParallelEmbedding
        self.tok_embeddings = nn.Embedding(
            params.vocab_size, params.dim  # 词嵌入维度
        )

        # 创建一系列的Transformer块
        self.layers = torch.nn.ModuleList()
        for layer_id in range(params.n_layers):
            self.layers.append(TransformerBlock(layer_id, params))

        # 归一化层，用于最终输出前对隐藏状态进行归一化
        self.norm = RMSNorm(params.dim, eps=params.norm_eps)

        # 使用基础的nn.Linear层代替ColumnParallelLinear作为输出层
        self.output = nn.Linear(
            params.dim, params.vocab_size, bias=False  # 输出层没有偏置项
        )

        # 预计算旋转位置编码（rotary position embeddings）
        self.freqs_cis = precompute_freqs_cis(
            self.params.dim // self.params.n_heads, 
            self.params.max_seq_len * 2
        )

    @torch.inference_mode()  # 表明该方法仅用于推理阶段
    def forward(self, tokens: torch.Tensor, start_pos: int):
        """
        定义Transformer模型的前向传播过程。
        :param tokens (torch.Tensor): 输入的token序列，形状为(batch_size, sequence_length)
        :param start_pos (int): 当前序列片段的起始位置，主要用于解码时更新缓存
        :return output (torch.Tensor): 最终的输出张量，只计算最后一个token的logits
        """
        _bsz, seqlen = tokens.shape  # 获取批量大小和序列长度
        h = self.tok_embeddings(tokens)  # 将输入token转换为词嵌入表示
        
        # 确保旋转位置编码位于正确的设备上，并获取当前序列长度对应的编码部分
        self.freqs_cis = self.freqs_cis.to(h.device)
        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]

        mask = None
        if seqlen > 1:
            # 如果序列长度大于1，则创建一个掩码矩阵以应用于注意力机制中
            mask = torch.full((1, 1, seqlen, seqlen), float("-inf"), device=tokens.device)
            mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)

        # 依次通过每一层Transformer块
        for layer in self.layers:
            h = layer(h, start_pos, freqs_cis, mask)

        # 对最后一层的输出进行归一化
        h = self.norm(h)

        # 只计算序列中最后一个token的logits作为输出
        output = self.output(h[:, -1, :])
        
        return output.float()  # 返回浮点类型的输出
```

* **训练数据**

**LLaMA-1&#x20;**&#x5728;构建其训练数据集时，进行了多种来源的数据预处理和优化：

1. **CommonCrawl**：选取了 2017 至 2020 年的五个数据集，通过去重、语言识别和质量过滤等步骤，确保内容的高质量

2. **C4**：整合了公开的 C4 数据集，采用不同的启发式规则进行质量控制

3. **GitHub**：选择了特定许可下的项目，移除低质量文件和样板内容，并在文件级别去重

![]()

4. **维基百科**：添加了 20 种语言的 2022 年中期数据，清理了格式化内容

5. **书籍语料库**：包含 Gutenberg 项目和 Books3 部分，对书籍进行了去重

6. **ArXiv**：处理了科学论文的 Latex 文件，移除了不必要的部分，增强了文本的一致性

7. **Stack Exchange**：包含了多个领域的问答数据，整理并排序了答案

经过上述处理，**LLaMA-1&#x20;**&#x7684;整个训练数据集包含大约 1.4T token。对于 **LLaMA-1&#x20;**&#x7684;大部分训练数据，每个 token 在训练期间只使用一次，但维基百科和 Books 的数据进行了大约两个 epoch 的训练。

* **训练**

**LLaMA-1&#x20;**&#x6A21;型未进行特定任务的微调，专注于自监督学习。训练过程中采用了 AdamW 优化器，配置了$$β_1$$和$$β_2$$参数以影响收敛性和稳定性，并使用余弦学习率调度策略逐步降低学习率以改善收敛。模型实施了 0.1 的权重衰减和 1.0 的梯度裁剪来防止过拟合并确保数值稳定，同时引入预热步骤以稳定初期训练动态。根据模型大小调整学习率和批量大小，以优化资源分配与效率。

为了提高大规模语言模型训练的效率，**LLaMA-1&#x20;**&#x91C7;取了一系列优化措施。通过高效实现因果多头注意力机制，减少了内存占用和计算时间；手动实现反向传播函数替代自动微分系统，并利用检查点技术保存昂贵的激活计算，提升了训练速度并减少了资源消耗。此外，通过模型和序列并行性及优化 GPU 间通信，进一步增强了训练效率。这些优化对于 650 亿参数规模的模型尤为重要，能显著缩短训练时间并提升运算效率，展示了高性能计算中对资源管理和效率的高度关注。

### 4.3.2 LLaMA-2

**LLaMA-2** 相比于 **LLaMA-1** 训练数据提升了 40%，有 **`7B`**、**`13B`**、**`34B`**、**`70B`** 四个大小，其中 34B 的没有开放，另外三个都可下载。**LLaMA-2** 总共使用 **`2T`** 的 token 进行训练，上下文长度升级到了 4096，是 **LLaMA-1** 的两倍。从官网可知 **LLaMA-2** 的预训练是在 A100-80GB 上运行了 3.3M GPU hours。

![]()

![]()

**LLaMA-2** 的 Tokenizer 配置与 **LLaMA-1** 完全相同，分词使用 **`SentencePiece`** 库实现的 **`BPE`** 算法，字典大小为 **`32k`**。**LLaMA-2** 模型架构和 **LLaMA-1** 一模一样，但模型推理的解码阶段的 kv cache 优化上做了改变。具体来说，在 34B 和 70B 参数模型上使用了 **`GQA`** 优化技术，7B 和 13B 模型依然使用 **`MQA`**。具体的 GQA 与 MQA 优化技术详见“注意力”章节。

> **注**：kv cache 内存计算公式为：$$\text{kv-cache} = 22nhb(s+o) = 4nhb(s+o)$$

* **训练数据**

**LLaMA-2** 预训练使用了来自公开可用源的 2T 个数据 token。**LLaMA-2-Chat** 还在为此项目创建的 27540 &#x4E2A;**`提示-响应`**&#x5BF9;上进行了额外的微调，其表现优于更大但质量较低的第三方数据集。

![]()

为了实现对齐，使用了包含1,418,091个 Meta 示例和七个较小数据集的组合的人类反馈强化学习。在 Meta 示例中，平均对话深度为3.9，Anthropic Helpful 和 Anthropic Harmless 为 3.0，包括 OpenAI Summarize、StackExchange 等在内的其他五个集合的平均对话深度为1.0。微调数据包括公开可用的指令数据集以及超过一百万个新的人类标注示例。&#x20;

在预训练过程中，**LLaMA-2&#x20;**&#x5BF9;数据的安全性进行了全面考量。通过对预训练数据进行分析，能够增加透明度，并发现潜在的问题根源，如潜在的偏见。**LLaMA-2&#x20;**&#x91C7;取了一系列措施，包括遵循公司的隐私和法律审查流程，排除已知含有大量个人信息的网站的数据。此外，**LLaMA-2&#x20;**&#x672A;对数据集进行额外的过滤，以使模型在各种任务中更广泛可用，同时避免过度清洗可能导致的意外人口统计消除。对于语言的代表性和毒性的分析，**LLaMA-2&#x20;**&#x4F7F;用了相应的工具和数据集，以了解预训练数据的特征，为模型的安全调整提供指导。这一过程确保了我们的模型在安全性方面得到了充分的考虑，并促使我们在部署模型之前进行了重要的安全调整。&#x20;

**LLaMA-2&#x20;**&#x7684;预训练主要集中在英语数据上，尽管实验观察表明模型在其他语言方面已有一定的熟练度，但由于非英语语言的预训练数据量有限，其熟练度受到限制。因此该模型在非英语语言中的性能比较差，应谨慎使用。

* **训练**

**LLaMA-2&#x20;**&#x662F;在 **LLaMA-1&#x20;**&#x7684;基础上进一步发展的，而 **LLaMA-2**-**Chat&#x20;**&#x6A21;型则是基于 **LLaMA-2&#x20;**&#x8FDB;行微调的版本。这两个模型保持了固定的 4k 上下文长度。在 **LLaMA-2&#x20;**&#x548C; **LLaMA-2-Chat&#x20;**&#x7684;训练过程中，用户输入提示的token损失被清零，这意味着模型被训练以忽略这些特定的token，从而更专注于生成回复。

**LLaMA-2-Chat&#x20;**&#x7684;训练过程如下图所示。整个过程起始于预训练的 **LLaMA-2**。在此之后，通过有监督微调创建了 **LLaMA-2-Chat&#x20;**&#x7684;初始版本。随后，使用人类反馈的强化学习RLHF来迭代地改进模型，具体包括拒绝采样Rejection Sampling 和近端策略优化 PPO。在RLHF阶段，人类偏好数据也在并行迭代，以保持奖励模型的更新。

![]()

### 4.3.3 LLaMA-3

**LLaMA-3** 相比于 **LLaMA-2&#x20;**&#x4E3B;要有以下三点的改动：

> 1. **模型结构**：依然选择了相对标准的纯解码器架构，模型结构上和 **LLaMA-2** 相比几乎没变化。在 **LLaMA-2** 中只有 **`34B`**，**`70B`** 使用了分组查询注&#x610F;**`GQA`**，为了提高模型的推理效率，**LLaMA-3** 所有模型都采用&#x4E86;**`GQA`**
>
> 2. **分词器**：和 **LLaMA-2** 不同的是，**LLaMA-3** 将 tokenizer &#x7531;**`SentencePiece`**&#x6362;&#x6210;**`tiktoken`**, 词汇量&#x4ECE;**`32K`**&#x589E;加&#x5230;**`128K`**，增加了 4 倍。更大的词汇库能够更高效地编码文本，增加编码效率，可以实现更好的下游性能。不过这也会导致嵌入层的输入和输出矩阵尺寸增大，模型参数量也会增大
>
> 3. **序列长度**：模型输入上下文长度从 **LLaMA-1&#x20;**&#x7684;**`2048`**&#x548C; **LLaMA-2&#x20;**&#x7684;**`4096`**&#x589E;加到 **`8192`**，但相对于 **GPT-4** &#x7684;**`128K`**&#x6765;说还是相当小

**LLaMA-3.1**

首次发布&#x4E86;**`405B`**&#x6A21;型，和当下最强的 **GPT-4** / **Claude-3.5** 旗鼓相当。全新升级了 **LLaMA-3** 的 8B 和 70B 版本，升级版不仅支持多语言功能，而且其上下文长度延展到了 128K，具有最先进的工具使用能力，推理能力也显著提升。

> **注**：**LLaMA-3.1** 系列模型于 2024 年 7 月发布，有 3 个可用版本：**`8B`**、**`70B`**、**`405B`**。

**LLaMA-3.2**

2024 年 9 月又发布了 **LLaMA-3.2**，模型权重采&#x7528;**`BFloat16`**&#x6570;字格式，包括小型和中型视觉语言模&#x578B;**`11B`**&#x548C;**`90B`**，以及轻量级的文本模&#x578B;**`1B`**&#x548C; **`3B`**，这些模型可以在边缘设备和移动设备上运行，同时提供预训练和指令调优的版本。&#x20;

**LLaMA-3.2** 中&#x7684;**`1B`**&#x548C;**`3B`**&#x6A21;型支持 128K 的上下文长度，并且是同类中性能领先的，用于设备端的摘要生成、指令执行和文本重写任务。这些模型在发布时就已适配 Qualcomm 和 MediaTek 的硬件，并针对 ARM 处理器进行了优化。META 在 1B 和 3B 模型上采用了剪枝和知识蒸馏这两种技术，使得这些模型成为了首批适用于设备的高性能轻量级 **LLaMA** 模型。具体来说：

> 1. **剪枝**：使用了从 **LLaMA-3.1** 8B 进行一次性结构化剪枝的方法，即系统性地移除网络中的部分组件，并调整权重和梯度大小，最终生成一个更小巧且高效的模型，但仍保留了原始模型的性能表现
>
> 2. **知识蒸馏**：对于 **LLaMA-3.2** 的 1B 和 3B 模型，在预训练阶段引入了 **LLaMA-3.1** 8B 和 70B 模型的对数几率logits，并将这些较大模型的输出作为训练目标。在剪枝后，使用知识蒸馏技术进一步恢复模型的性能

在视觉任务上，**LLaMA-3.2** 的 11B 和 90B 模型在图像理解方面优于封闭模型如 Claude 3 Haiku，可以直接作为对应文本模型的替代品。这些模型既有预训练版本，也有对齐版本，可以使用 torchtune 微调，并通过 torchchat 部署到本地。

在后训练阶段，沿用了 **LLaMA-3.1** 的训练方案，通过多轮的对齐步骤生成最终的聊天模型。每一轮的对齐包括**监督微调（SFT）**、**拒绝采样（RS）**&#x548C;**直接偏好优化（DPO）**。在这个阶段，将模型的上下文长度扩展到了 128K，同时确保模型的质量与预训练模型保持一致。此外还使用合成数据进行训练，经过严格的数据处理和筛选，以确保数据质量。通过精心组合这些数据，优化了模型在摘要生成、文本重写、指令执行、语言推理以及工具使用等方面的能力。

**LLaMA-3.2&#x20;**&#x9996;次发布了 **LLaMA** Stack 分发版本，这将大大简化开发者在不同环境中使用 **LLaMA** 模型的流程，包括单节点部署、本地部署、云端部署，以及设备端部署，从而实现 **RAG** 和工具集成应用的快速部署。

* **训练数据**

**LLaMA-3&#x20;**&#x8BAD;练数据量大幅增加，从 **LLaMA-2&#x20;**&#x7684; 2T Token 扩展到了 15T Token，增长了约 8 倍。其中，代码数据扩充了 4 倍，显著提升了模型在代码能力和逻辑推理能力方面的表现。此外，**LLaMA-3&#x20;**&#x7684;训练数据包括超过 5% 的非英语 token，来源于 30 多种语言。这不仅使得模型在处理英语内容时更加高效，也显著提升了其多语言处理能力，这表明 **LLaMA-3&#x20;**&#x5728;全球多语言环境中的适应性和应用潜力。

为确保数据质量，Meta 开发了一系列数据过滤 pipeline，包括启发式过滤器、NSFW过滤器、语义重复数据删除技术及用于预测数据质量的文本分类器。这些工具的有效性得益于先前版本 **LLaMA** 的表现，特别是在识别高质量数据方面。此外，Meta 通过大量实验评估了在最终预训练数据集中混合不同来源数据的最佳策略，确保 **LLaMA-3&#x20;**&#x80FD;在多种场景下展现卓越性能。

* **训练**

**LLaMA-3&#x20;**&#x7CFB;列延续了其前代的架构，提供预训练模型 **LLaMA-3** 和微调后的版本 **LLaMA-3-Instruct**。为了支持最大规模的 **LLaMA-3&#x20;**&#x6A21;型训练，Meta 采用了数据并行、模型并行和流水线并行三种策略相结合的方法，在 16K GPU 集群上实现了超过 400 TFLOPS 的单 GPU 利用率，并最终在两个定制的 24K GPU 集群上完成了训练。为了确保高效稳定的训练过程，Meta 开发了一套先进的训练堆栈，具备自动错误检测与处理功能，增强了硬件可靠性，并引入了无声数据损坏检测机制。此外，还创建了新的可扩展存储系统，降低了检查点和回滚的开销，使有效训练时间达到 95% 以上。这些改进使得 **LLaMA-3** 的训练效率相比 **LLaMA-2** 提升了大约三倍。

在预训练阶段，**LLaMA-3&#x20;**&#x81F4;力于扩大训练规模，以更有效地利用数据。通过制定 scaling laws，能够在训练前预测模型性能，从而优化数据选择。新的观察显示，即使超出理论最优的数据量——如 8B 参数模型建议的 200B token——继续增加训练数据至 15T token，模型性能依然能对数线性提升。这表明更大的数据集对于提高模型能力的重要性。进入微调阶段，Meta 创新地结合了**有监督微调**、**拒绝采样**、**近似策略优化 PPO** 和**直接策略优化 DPO**。这种方法不仅强化了模型在复杂推理和编码任务中的表现，而且通过偏好排序训练，提高了 **LLaMA-3** 解决逻辑推理问题时的选择准确性，这对于增强 AI 的实际应用价值至关重要。

### 4.3.4 LLaMA-4

![]()

**Llama 4** 系列包&#x62EC;**`Llama 4 Scout`**、**`Llama 4 Maverick`**&#x548C;**`Llama 4 Behemoth`**。所有这些模型都经过了大量未标注的文本、图像和视频数据的训练。

> * **Llama 4 Scout** 拥有 170 亿激活参数和 16 个专家的模型，比前几代 Llama 模型更强大，且能适配单个 NVIDIA H100 GPU。此外，Llama 4 Scout 支持 10M 上下文窗口，在基准测试中表现优于 Gemma 3、Gemini 2.0 Flash-Lite 和 Mistral 3.1。
>
> * **Llama 4 Maverick** 拥有 128 位专家、 170 亿个激活参数模型，在基准测试中效果优于 GPT-4o 和 Gemini 2.0 Flash，同时在推理和编程方面取得了与新 DeepSeek v3 相当的结果，但激活参数不到一半。Llama 4 Maverick 的总排名第二，成为第四个突破 1400 分的大模型。其中开源模型排名第一，超越了 DeepSeek；在困难提示词、编程、数学、创意写作等任务中排名均为第一；大幅超越了自家 Llama 3 405B，得分从 1268 提升到了 1417。
>
> * 以上这两个模型是 Meta 目前最好的模型，他们从拥有 2880 亿激活参数和 16 个专家的 **Llama 4 Behemoth** 知识蒸馏而来。Llama 4 Behemoth 是 Meta 目前表现最好的模型，在多项科学、技术、工程和数学基准测试中，Llama 4 Behemoth 的表现优于 GPT-4.5、Claude 3.7 Sonnet 和 Gemini 2.0 Pro。

* **预训练**

Llama 4 是 Meta 首批使用 MoE 架构的模型。

![]()

Llama 4 Maverick 模型有 17B 个激活参数和 400B 个总参数。Meta 使用交替的 Dense 和 MoE 层来提高推理效率。MoE 层使用 128 位路由专家和一位共享专家。每个 token 都会发送给共享专家以及 128 位路由专家之一。这通过降低模型运行成本和延迟来提高推理效率，Llama 4 Maverick 可以在单个 NVIDIA H100 上运行，以便于部署，也可以通过分布式推理实现最高效率。

Llama 4 模型采用原生多模态设计，通过早期融合，将文本和视觉 token 无缝集成到统一的模型主干中。早期融合能够使用大量未标记的文本、图像和视频数据联合预训练模型。除此之外，Meta 改进了 Llama 4 中的视觉编码器，它基于 MetaCLIP，与冻结的 Llama 模型一起单独训练，以便更好地使编码器适应 LLM。

Meta 开发了一种新的训练技&#x672F;**`MetaP`**，能够可靠地设置关键模型超参数，例如每层的学习率和初始化尺度。Meta 发现所选的超参数在不同的批处理大小、模型宽度、深度和训练标记值之间具有良好的迁移性。Llama 4 对 200 种语言进行预训练，其中包括 100 多种语言，每种语言都有超过 10 亿个 token，多语言 token 比 Llama 3 多 10 倍。

此外，Meta 使&#x7528;**`FP8`**&#x8FDB;行高效的模型训练，不会牺牲质量并可以保证较高的模型 FLOP 利用率。在使用 FP8 和 32K GPU 预训练 Llama 4 Behemoth 时，实现了 390 TFLOPs/GPU。用于训练的整体数据组合由超过 30 万亿个 token 组成，是 Llama 3 预训练组合的两倍多，包括各种文本、图像和视频数据集。

最后，Meta 在所谓的“中期训练”阶段训练模型，以使用新的训练方案来提高核心功能，包括使用专门的数据集进行长上下文扩展。这能够提高模型质量，同时使 Llama 4 Scout 支持 10M 输入上下文长度。

* **后训练**

Llama 4 Maverick 在图像和文本理解方面有很好的性能，能够创建跨越语言障碍的复杂人工智能应用。作为通用助手和聊天用例的产品主力模型，Llama 4 Maverick 在精确图像理解和创意写作方面表现出色。

在对 Llama 4 Maverick 模型进行后训练时，最大的挑战是平衡多种输入模态、推理能力和对话能力。为了混合模态，Meta 设计了一种精心策划的课程策略，与单一模态专家模型相比，这种策略不会降低性能。在 Llama 4 中，Meta 通过采用不同的方法对后训练流程进行了全面改进：`轻量级监督微调（SFT）> 在线强化学习（RL）> 轻量级直接偏好优化（DPO）`。Meta 发现，SFT 和 DPO 可能会过度约束模型，限制在线 RL 阶段的探索能力，从而导致推理、编程和数学领域的精度下降。为了解决这一问题，Meta 使用 Llama 模型作为评判，移除了超过 50% 的标记为简单的数据，并在剩余较难的数据集上进行了轻量级监督微调（SFT）。在随后的多模态在线强化学习（RL）阶段，通过精心选择较难的提示，实现了性能的显著提升。此外，Meta 还实施了持续在线 RL 策略，交替训练模型并使用它持续过滤并保留中等至高难度的提示。这种策略在计算和准确性权衡方面非常有益。最后，Meta 还进行了轻量级直接偏好优化（DPO），以处理与模型响应质量相关的边缘情况，有效实现了模型智能与对话能力的良好平衡。这些改进促成了一个业界领先的通用聊天模型，具备最先进的智能和图像理解能力。

* **性能**

1. **Llama 4 Maverick**

Llama 4 Maverick 包含 170 亿激活参数、128 个专家和 4000 亿总参数，相比 Llama 3.3 70B，以更低的价格提供了更高的质量。

![]()

![]()

* **Llama 4 Scout**

较小模型 Llama 4 Scout 是一款通用型模型，拥有 170 亿激活参数、16 个专家和 1090 亿总参数。Llama 4 Scout 将支持的上下文长度从 Llama 3 的 128K 大幅提升至 1000 万 token。

Llama 4 Scout 在预训练和后训练中均使用 256K 上下文长度，使基础模型具备强大的长上下文泛化能力。在大海捞针检索等任务中，该模型表现优异。Llama 4 架构的关键创新之一是使用无位置嵌入的交错注意力层，并通过推理时的温度缩放来增强长上下文泛化能力。这被称&#x4E3A;**`iRoPE`**&#x67B6;构，其中 i 代表交错注意力层，强调其支持无限上下文长度的长期目标；RoPE 指大多数层中使用的旋转位置嵌入。

![]()

![]()

Meta 对这两款模型进行了广泛的图像和视频帧静止图像训练，以赋予它们广泛的视觉理解能力，包括对时序活动及相关图像的理解。这使得模型能够在多图像输入和文本提示下轻松进行视觉推理和理解任务。这些模型在预训练时最多支持 48 张图像，并且在后训练中可以支持 8 张图像，结果良好。

Llama 4 Scout 在图像定位方面表现卓越，能够将用户提示与相关视觉概念对齐，并将模型响应锚定到图像中的特定区域。这使得大型语言模型能够更精确地进行视觉问答，更好地理解用户意图并定位感兴趣的对象。

此外，Llama 4 Scout 在编码、推理、长上下文和图像基准测试中超越了类似模型，并且比所有之前的 Llama 模型表现更强。

![]()

* **Llama 4 Behemoth**

Llama 4 Behemoth 是一个教师模型，也是一个多模态专家混合模型，拥有 288B 个活动参数、16 位专家和近两万亿个总参数。它在数学、多语言和图像基准测试中表现优秀。

![]()

对一个拥有两万亿参数的模型进行后训练是一个巨大的挑战，这要求研究者从数据规模开始，彻底重新设计和改进训练方案。为了最大化性能，Meta 不得不对监督微调（SFT）数据进行 95% 的剪枝，而较小模型的剪枝比例为 50%。这一举措是为了在质量和效率上取得必要的平衡。Meta 还发现，先进行轻量级监督微调（SFT），再进行大规模强化学习（RL），能够显著提升模型的推理和编码能力。Meta 的强化学习（RL）方案专注于通过策略模型进行 pass@k 分析，采样难度较高的提示，并构建难度逐渐增加的训练课程。此外，在训练过程中动态过滤掉零优势的提示，并构建包含多种能力的混合提示训练批次，这些措施在数学、推理和编码方面为模型带来了显著的性能提升。最后，从多种系统指令中采样对于确保模型在推理和编码任务中保持指令遵循能力至关重要，这使得模型能够在多种任务中表现出色。

为两万亿参数的模型扩展强化学习（RL）也是一项巨大的挑战，这迫使 Meta 不得不重新设计并改进底层的强化学习基础设施，以应对前所未有的规模。Meta 对 MoE 并行化的设计进行了优化，以提升速度，从而加快迭代过程。此外，他们还开发了一个完全异步的在线强化学习训练框架，增强了灵活性。与现有的分布式训练框架相比，后者为了将所有模型加载到内存中而牺牲了计算内存，Meta 的新基础设施能够灵活地将不同模型分配到不同的 GPU 上，并根据计算速度在多个模型之间平衡资源。这一创新使得训练效率相比上一代提升了约 10 倍。

## 4.4 Qwen 系列

> **论文**：[**Qwen1**](https://arxiv.org/pdf/2309.16609)  **[Qwen2](https://arxiv.org/pdf/2407.10671)  [Qwen2.5](https://arxiv.org/pdf/2412.15115)  [YaRN](https://arxiv.org/pdf/2309.00071)  [DCA](https://arxiv.org/pdf/2402.17463)  [ABF](https://arxiv.org/pdf/2309.16039)**
>
> **代码**：**[Qwen1](https://github.com/QwenLM/Qwen)  [Qwen2.5](https://github.com/QwenLM/Qwen2)**

### 4.4.1 Qwen1

**Qwen** 是由阿里提出的聊天和预训练大型语言模型。目前开源了 **Qwen** 的各系列模型，包括基础语言模&#x578B;**`Qwen-1.8B`**、**`Qwen-7B`**、**`Qwen-14B`**&#x548C;**`Qwen-72B`**，以及相应的聊天模&#x578B;**`Qwen-1.8B-Chat`**、**`Qwen-7B-Chat`**、**`Qwen-14B-Chat`**&#x548C;**`Qwen-72B-Chat`**。各个规模模型参数如右表。

![]()

此外，阿里还发布了专门的编码和数学模型，&#x5373;**`Code-Qwen`**, **`Code-Qwen-Chat`**&#x548C;基于 **Qwen** 的数学模&#x578B;**`Math-Qwen-Chat`。**

![]()

除此之外，还有多模态大模&#x578B;**`Qwen-VL`**&#x548C;**`Qwen-VL-Chat`**。模型总览图如上图。

* **模型结构**

**Qwen** 使用经过修改的 Transformer 架构，并采用了最近流行的大型语言模型 **LLaMA** 的训练方法，具体有以下几点：

**分词**

**Qwen** 使&#x7528;**`BPE`**&#x4F5C;为分词方法，并从开源 fast BPE tokenizer 中选&#x62E9;**`tiktoken`**&#x4F5C;为起点。为了提高模型在多语言下游任务上的性能，特别是中文任务，**Qwen**增加了常用汉字、词语和其他语言中的词汇量。同时，**Qwen** 也将数字拆分为单个数字。最终的词汇表大小约&#x4E3A;**`152K`**。实验结果表明 **Qwen** 相对于其他分词器具有更高的压缩效率，在大多数语言中都能实现更高的信息传递效率。此外还证明扩展 **Qwen** 的词汇表大小不会对下游任务的表现产生负面影响。具体的分词步骤如下：

> 1. 基于开源分词器 tiktoken &#x7684;**`cl100k`**&#x57FA;础词表进行初始化
>
> 2. 针对中文场景，向词表中增添常用的中文字和词，扩充词表规模
>
> 3. 参考 GPT-3.5 和 LLaMA 的实现，将数字切分成单个数字，如：&#x5C06;**`123`**&#x5206;词&#x4E3A;**`1`**、**`2`**、**`3`**

> **注**：将数字分成各个数字的原因主要与模型的训练效率和下游任务的表现有关
>
> 1. **增强模型的泛化能力**：将数字拆分成单个数字，模型可以更好地理解和处理不同的数字组合，而不仅仅是记住特定的数字。这种方法提高了模型在处理新数字和未见过的数字组合时的泛化能力。
>
> 2. **减少词汇表的大小**：数字有无数种组合形式，若每个数字都作为一个单独的词汇存在于词汇表中，会显著增加词汇表的大小。而拆分数字可以减少词汇表的大小，从而提高模型的训练效率和推理速度。

**输入嵌入和输出投影**

**Qwen** 不共享输入 Embedding 和输出 Projection 的权重，因为输入嵌入和输出投影在功能上有不同的需求，使用独立的权重允许有更多的灵活性，以便更好地满足需求，从而提高模型的整体表现。但代价是增加了内存消耗。

**位置编码&#x20;**

**Qwen** 选择旋转位置编&#x7801;**`RoPE`**&#x4F5C;为首选选项，以将位置信息纳入 **Qwen** 模型中，并且 **Qwen** 选择使&#x7528;**`FP32`**&#x7CBE;度来计算逆频率矩阵，而不是 **BF16** 或 **FP16**，以便优先考虑模型性能并实现更高的准确性。

> **注**：使用 FP32 的原因
>
> 1. **逆频率矩阵的需要**：在RoPE中，位置编码依赖于逆频率矩阵来计算位置向量的旋转。位置向量的旋转通过旋转矩阵实现，而这个旋转矩阵又依赖于位置和频率之间的精确关系。

![Qwen 模型结构]()

> 2. **高精度计算**：如果逆频率矩阵的精度不够高，会导致位置编码不准确。这会直接影响模型对序列中相对位置关系的理解，从而影响模型在处理位置敏感任务时的性能。这种高精度有助于减少数值计算中的误差，提高模型的稳定性和准确性。

**线性偏置项&#x20;**

对于大多数层，**Qwen** 删除偏差，但对于注意力中的 **QKV** 层添加了偏差，以增强模型的外推能力。

**优点**

移除偏置项可以减少参数数量，从而降低模型的复杂度和过拟合的风险，有助于提高模型的稳定性和性能，而且在训练过程中可以更好地泛化，从而在不同的数据集上表现得更为一致。

**缺点**

在 QKV 层中添加偏差项可以增强模型的外推能力，更好地捕捉输入数据的特征，从而提升模型在处理未知或未见数据时的表现。这种增强外推能力的做法对于处理复杂任务和应对多样化的数据输入非常重要。

**归一化&#x20;**

**Qwen** 使用 **Pre-Norm&#x20;**&#x5F52;一化方法，因为在 Transformer 结构的模型中，**Pre-Norm** 是最常用的方法，并且已被证明比 **Post-Norm** 更能提高训练稳定性。此外，**Qwen** &#x7528;**`RMSNorm`**&#x53D6;代了传统层归一化技术。这一改变带来了相同的表现水平，同时也提高了效率。

**Pre-Norm&#x20;**

1. **稳定性**：在训练早期可以稳定梯度，缓解梯度消失和梯度爆炸的问题。

2. **收敛性**：收敛速度较快，因为归一化使得每个子层的输入具有较好的分布，促进了模型的学习。

   **缺点**

1) **深层网络训练**：在非常深的网络中会导致网络中的信息传递不够充分，因为归一化会削弱信息的传递。

**Post-Norm&#x20;**

1. **信息传递**：处理深层网络时表现更好，因为可以保留和传递子层信息，捕捉复杂的特征和模式。

2. **性能提升**：可以提高模型的最终性能，特别是对于需要捕捉长程依赖关系的任务。

   **缺点**

1) **训练难度**：训练初期可能会面临梯度消失或梯度爆炸的问题，导致梯度不够稳定。

2) **收敛速度**：收敛速度较慢，需要更多的训练步骤和调整超参数来稳定训练过程。

> **注**：选择 **Pre-Norm** 还是 **Post-Norm** 取决于具体的应用场景和模型需求：
>
> 1. **收敛速度和训练稳定性**：如果优先考虑训练的收敛速度和稳定性，Pre-Norm 可能是更好的选择。
>
> 2. **深层模型和信息传递**：对于需要训练非常深的模型或需要捕捉复杂依赖关系的任务，Post-Norm 可能更合适。

**激活函数**

**Qwen** 选择&#x4E86;**`SwiGLU`**&#x4F5C;为激活函数，它是 Swish 和门控线性单元的组合。**Qwen** 实验证明基于 GLU 的激活函数通常优于其他基准选项，如 GeLU。与以往研究常见的做法一样，**Qwen** 将 FFN 的维度从隐藏大小的四倍减少到三分之八倍的隐藏大小。

**LayerNorm**

1. **稳定性**：对每一层的输入进行归一化，可以缓解梯度消失和梯度爆炸的问题，提高模型的稳定性。

2. **适用性广泛**：在各种神经网络结构中表现良好，尤其适用于 RNN 和 Transformers。

3. **无批量依赖**：不依赖于批量数据的大小，在小批量甚至单样本情况下依然有效。

**缺点**

1. **计算复杂度**：需要计算均值和标准差，这在计算上相对复杂。

2. **对特征缩放敏感**：有时会对特征缩放较为敏感，需要额外调整超参数。

**RMSNorm**

1. **计算效率高**：计算比LayerNorm更简单，只需计算均方根值，不需要计算均值和标准差。&#x20;

2. **稳定性好**：能缓解梯度消失和梯度爆炸的问题，提升模型的训练稳定性。

3. **适用性**：特别适用于需要简化计算的模型，如某些大规模预训练模型。

   **缺点**

1) **信息损失**：只考虑了均方根值，可能会丢失一些与输入分布相关的信息。



> **注**：选择 **SwiGLU** 的好处
>
> 1. Swish 对于负值的响应相对较小，克服了 ReLU 某些神经元上输出始终为零的缺点。
>
> 2. GLU 的门控特性，意味着它可以根据输入的情况决定哪些信息应该通过、哪些信息应该被过滤。这种机制可以使网络更有效地学习到有用的表示，有助于提高模型的泛化能力。在大语言模型中，这对于处理长序列、长距离依赖的文本特别有用。
>
> 3. **SwiGLU** 中的参&#x6570;**`W1`**,**`W2`**,**`W3`**,**`b1`**,**`b2`**,**`b3`**&#x53EF;以通过训练学习，使得模型可以根据不同任务和数据集动态调整这些参数，增强了模型的灵活性和适应性。
>
> 4. 计算效率相比某些较复杂的激活函数如 GELU 等更高，同时仍能保持较好的性能。这对于大规模语言模型的训练和推理是很重要的考量因素。

* **预训练**

预训练阶段包括学习大量数据，以获得对世界及其各种复杂性的全面理解。这不仅包括基本的语言能力，还包括算术、编码和逻辑推理等高级技能。

**数据**

**Qwen** 使用了高达3万亿个 token 的数据进行预训练，数据主要涉及公共网络文档、百科全书、书籍、代码等，数据涉及多语言，但以中文和英文为主。为了保证预训练数据的质量，研究团队制定了一套全面的数据预处理程序：

> 1. **文本数据抓取**：对于 Web 数据，从 HTML 中提取文本内容
>
> 2. **语言识别**：采用语言识别工具确定语种
>
> 3. **去重**：为了增加数据的多样性，采用了重复数据删除技术，包括语言规范化后的精确匹配重复数据删除和使&#x7528;**`MinHash`**&#x548C;**`LSH`**&#x7B97;法的模糊重复数据删除
>
> 4. **质量过滤**：结合规则和机器学习的方法过滤低质量数据，即通过多个模型对内容进行评分，包括语言模型、文本质量评分模型
>
> 5. **安全控制**：使用模型识别并过滤涉及暴力、偏见、色情等潜在冒犯性内容
>
> 6. **上采样**：针对某些高质量源的数据进行上采样，以确保多样化的高质量内容
>
> 7. **引入指令数据**：引入高质量的指令数据
>
> 8. **防止数据泄露**：为了消除了与评估中使用的测试集中存在的任何数据重叠的问题，删除了任何与测试集中指令样本存在超过 13-gram 的文本内容

**训练**

具体的训练细节如下：

> 1. 采用标准的自回归语言模型训练目标
>
> 2. 训练时上下文长度&#x4E3A;**`2048`**
>
> 3. 注意力模块采&#x7528;**`Flash Attention`**&#x6280;术，以提高计算效率并减少内存使用
>
> 4. 采&#x7528;**`AdamW`**&#x4F18;化器，设置$$β_1=0.9, β2=0.95, \epsilon=1e-8$$
>
> 5. 使用余弦学习率，为每种模型设定一个峰值学习率，学习率会衰减到峰值的10%
>
> 6. 使&#x7528;**`BFloat16`**&#x6DF7;合精度加速训练

**外推能力扩展**

Transformer 架构的模型在注意机制的上下文长度方面有很大的限制。随着上下文长度的增加，二次复杂度计算导致计算和内存成本的急剧增加。为了解决这个问题，**Qwen&#x20;**&#x9009;择在推理阶段实现上下文长度的扩展 ，而没有在训练阶段扩展，因为在训练阶段扩展上下文会导致训练成本的急速增长。

由于 **Qwen** 使用的位置编码是 **RoPE**，虽然 **RoPE** 相较于绝对位置编码具有可外推的优点，但是在直接使用外推的过程中还是存在一些问题，因此 **Qwen** 针对这些问题，来进行了改进。

**RoPE** 使用正弦和余弦函数将位置信息嵌入到词汇向量的旋转矩阵中。然而 **RoPE** 直接外推会导致 Attention Score 显著增加：

**正弦和余弦函数的周期性**

1. 正弦和余弦函数是周期性的，在训练数据中，位置通常在一个相对较小的范围内，&#x5982;**`0`**&#x5230;**`2048`**，这些位置的编码值会保持在周期的某一部分。

2. 当位置超出这个范围时，如位置变&#x4E3A;**`3000`**，编码值会进入正弦和余弦函数的另一个周期。由于这些函数的周期性，这些位置的编码值可能与训练数据中的编码值非常不同，导致模型在计算注意力分数时出现剧烈变化。

**高频成分的影响**

1. 在 **RoPE** 编码中，较高维度的编码，即频率较高的正弦和余弦成分，会对较大的位置变化更加敏感。这意味着，随着位置数值的增加，这些高频成分会迅速变化。

2. 对于较大的位置值，正弦和余弦函数的值可能会经历快速变化。这种快速变化会导致 Attention 机制中的 Attention Score 出现显著波动。

为了解决 **RoPE** 直接外推产生的 Attention Score 过大，以及 **RoPE** 嵌入插值时丢失高频信息的问题，**Qwen** 利用&#x4E86;**`动态NTK感知插值`**&#x6765;实现推理阶段的上下文长度扩展。

**NTK-aware 插值**

**`NTK-aware插值`**&#x6838;心思想是：**高频外推，低频内插**。

与 **RoPE** 将每个维度平均缩放一个因子$$S$$不同，**NTK-aware** 插值通过减少高频的缩放和增加低频的缩放来将插值压力分散到多个维度；虽然可以通过许多方法获得这样的变换，但最简单的方法是对$$θ$$的值进行基础更改。

一般来说，内插方法可以写成如下表达式：

$$f'(x_m, m, \theta) = f(x_m, g(m), h(\theta))$$

其中第$$i$$个维度有$$\theta_i=b^{\frac{−2(i−1)}{d}}, b=10000, i=1,2,\cdots, \frac{d}{2}$$，这里从位置和旋转角度两个方面对所有内插方案进行了总结，即所有内插方案都是建立在对二者的变换上的。此时可以将位置插值 **PI** 改写为：

$$f_{PI}(x_m, m, \theta) = f(x_m, g(m)=\frac{m}{S}, h(\theta_i)=\theta_i)$$

即 **PI** 中没有对旋转角度做任何改变，只是将位置索引除以扩展比。基于上式，NTK-aware 的做法可以被表述为：

$$f_{NTK}(x_m, m, \theta) = f(x_m, g(m)=m, h(\theta_i)=(b\cdot S^{\frac{d}{d-2}})^{\frac{−2(i−1)}{d}})$$

即 **NTK-aware** 插值本质上就是将原始 **RoPE** 中的$$\theta_i=b^{\frac{−2(i−1)}{d}}$$改为$$h(\theta_i)=(b\cdot S^{\frac{d}{d-2}})^{\frac{−2(i−1)}{d}}$$，更本质的区别是将基数 base 乘以了一个和扩展比$$S$$有关的常量$$S^{\frac{d}{d-2}}$$，与位置插值 **PI** 相比，这种方法在扩展非微调模型的上下文大小方面表现得更好。然而这种方法的一个主要缺点是，它不仅仅是一种插值方案，一些维度会被轻微外推到超出边界的值，因此使用 **NTK-aware** 插值进行微调的结果不如 **PI**；此外，由于存在越界值，理论尺度因子$$S$$并不能准确描述真实的上下文扩展尺度。在实践中，对于给定的上下文长度扩展，尺度值$$S$$必须设置得高于预期尺度。

**Dynamic NTK-aware 插值**

在多数情况下，以从 1 到最大上下文大小不等的序列长度进行多次前向传递。一个典型的例子是自回归生成，其中序列长度在每一步之后递增 1，有两种方法可以应用使用比例因子$$S$$的插值方法：

> 1. 整个推理周期中嵌入层固定，包括缩放因子$$S=\frac{L'}{L}$$，$$L'$$是固定数量的扩展上下文大小
>
> 2. 每次前向传递时更新缩放因子：$$S=\max(1,l'/L)$$，$$l'$$是当前序列的序列长度

上述方法中，方法 1 的问题在于模型在长度小于 L 时可能出现性能折扣，当序列长度大于 L′ 时可能出现突然退化，因此提出了方法 2，这种推理方法为动态缩放方法，当与 NTK-aware 插值相结合时，称之为 Dynamic NTK-aware 插值。

**LogN-Scaling**

根据熵不变性以及一些合理的假设，可以得到一个新的缩放因子，从而得到一种 **Scaled Dot-Product Attention**：

$$\text{Attention}(Q,K,V)=\text{softmax}({\frac{\kappa\log n}{d} QK^\top})V$$

这里的$$\kappa$$是一个跟$$𝑛,𝑑$$都无关的超参数。LogN-Scaling 可以根据上下文长度与训练长度的比值，对$$Q$$和$$V$$的点积进行重新缩放，确保注意力值的熵随着上下文长度的增长而保持稳定。

**分层窗口 Self-Attention**

分层窗口将注意力限制在一个上下文窗口内，防止模型关注到太远的内容。**Qwen** 团队观察到模型在处理长上下文时在不同层次上的建模能力存在差异，较低的层次相对于较高的层次更加敏感于上下文长度的扩展；为此，为每个层分配了不同的窗口大小，对较低的层使用较短的窗口，对较高的层使用较长的窗口。

综合这些技术，**Qwen&#x20;**&#x6A21;型在推理阶段可以处&#x7406;**`8192`**（相较于训练所使用&#x7684;**`2048`**，扩展了4倍）个 token 的长序列，外推能力优异。同时，实验结果表明，应用以上所述的关键技术可以使模型在上下文长度增加时始终保持低困惑度，这表明这些技术在增强模型理解和生成长文本的能力方面发挥了重要作用。

* **对齐**

**Qwen&#x20;**&#x9884;训练的模型在实际应用中表现与人类行为不一致，这里展示 **Qwen** 模型如何进行 **SFT** 和 **RLHF** 提高语言模型进行自然对话的能力。

**SFT 数据**

在 SFT 数据处理过程中，**Qwen** 主要采用了以下手段：

1. **使用 ChatML 格式**：**Qwen** 采用 **ChatML** 格式来进行模型训练。它利用特殊符号表示不同类型信息，如系统设置、用户输入、助手输出等，这有助于模型有效区分信息

![]()

2. **对话流式风格**：采用会话流式对话风格，而不是简单的问答形式，使模型学会真实的人机交互

3. **任务的多样性**：通过专注于不同任务的自然语言生成来提高模型的有用性

4. **去除格式化数据**：为了确保模型能够泛化到广泛的场景，特别排除了在提示模板中格式化的数据，因为这些数据可能会限制模型功能

5. **安全性**：通过注释与安全问题相关的数据（如暴力、偏见和色情）来优先考虑语言模型的安全性

**SFT 训练**

在SFT的训练过程中，**Qwen** 采用了以下手段：

> * **训练目标**：与预训练一致，使用 Next Token Prediction
>
> * **损失函数**：在训练过程中对系统和用户输入应用 loss mask，将用户、系统的输入 mask 掉，不计算loss，仅对 **Qwen** 的回复部分计算 loss
>
> * **优化器**：使&#x7528;**`AdamW`**&#x4F18;化器，超参数$$β_1, β_2, ϵ$$为别为 0.9、0.95 和 1e−8，学习率先增后恒定
>
> * **批大小**：序列长度限制在 2048，训练 batch size=128
>
> * **学习率**：训练 4000 步,在前 1430 步中，学习率逐渐增加，达到 2e−6 的峰值
>
> * **防过拟合**：为了防止过拟合，权重衰减的值设置为 0.1，dropout 设置为 0.1，梯度裁剪的限制为 1.0

**Reward Model 训练**

在构建偏好模型时，分成了两步：偏好模型预训练、偏好模型微调。

预训练偏好模型 **PMP**（**P**reference **M**odel **P**retraining）：

> 1. **比较数据集**：使用了一个庞大的比较数据集，该数据集由样本对组成，每个样本对包含对单个查询的两个不同的响应及其相应的人类偏好
>
> 2. **池化层**：奖励模型由同等大小 **Qwen** 模型+池化层得来，用特殊的句子结束标记经过池化层的映射值作为模型奖励值

微调奖励模型：

> 1. **分类系统**：为确保 Prompt 具备一定的多样性和复杂性，创建了一个包含约 6600 详细标签的分类系统
>
> 2. **平衡采样**：采用了一种平衡采样算法，以在选择提示时兼顾多样性和复杂性
>
> 3. **多样采样**：为了生成多样的回复，实验过程使用了不同规模和采样策略的 **Qwen** 模型，因为多样化的回复有助于降低标注难度并提高奖励模型的性能

模型在训练过程中，学习率恒为 3e−6，批次大小为 64，最大长度为 2048，训练一个 epoch。最终得到的 **PMP** 模型在分布外数据上表现出较高的泛化能力，奖励模型在 **Qwen** 奖励数据集上表现出显著的改进。

**PPO 强化学习**

**PPO** 阶段共包含四个模型：**`policy模型`**、**`value模型`**、**`reference模型`**、**`reward模型`**。在开始 PPO 流程之前，暂停 policy 模型的更新，先对 value 模型训练 50 步预热，从而确保 value 模型能够有效地适应不同的 reward 模型。在 **PPO** 过程中，policy 模型和 value 模型的学习率分别为 1e−6 和 5e−6。为了增强训练的稳定性，裁剪值设置为 0.15。

在进行推理时，生成策略的 top-p 值设置为 0.9。研究结果表明，虽然熵值略低于 top-p=1.0 时的值，但奖励的增加速度更快，最终在类似条件下始终能够获得较高的评估奖励。此外，**Qwen** 还采用了预训练梯度来减缓所谓的对齐税。研究表明，在这种特定的奖励模型下，KL 惩罚在非严格代码或数学性质的基准测试中足以弥补对齐税。与 **PPO** 数据相比，预训练梯度必须使用更大量的预训练数据，以确保预训练梯度的有效性。此外，实证研究表明，过大的系数值会大大阻碍与奖励模型的匹配，最终影响匹配，而过小的系数值对减缓对齐税的效果微不足道。

### 4.4.2 Qwen1.5

**Qwen1.5** 是在 2024 年 2 月份推出的，开源了包&#x62EC;**`0.5B`**、**`1.8B`**、**`4B`**、**`7B`**、**`14B`**、**`32B`**&#x548C;**`72B`**&#x5171;计 7 个不同规模的 Base 和 Chat 模型，, 以及 **Qwen&#x20;**&#x7CFB;列的首个 **MoE** 模&#x578B;**`Qwen1.5-MoE-A2.7B`**。为了提升模型的推理效率，**Qwen1.5** 提供了多种量化方案，支&#x6301;**`Int4`**、**`Int8`**、**`AWQ`**、**`GGUF`**&#x7B49;量化模型。量化技术的应用，特别是&#x5728;**`GPTQ`**&#x548C;**`AWQ`**&#x91CF;化的引入，使得模型可以在不牺牲性能的前提下，显著减少存储和计算资源消耗。

本次更新在对齐最新的 **Qwen1.5** 系列时有效地采用了直接策略优化 **DPO** 和近端策略优化 **PPO** 等技术，着重提升了 Chat 模型与人类偏好的对齐程度，并且显著增强了模型的多语言处理能力。在序列长度方面，所有规模模型均已实&#x73B0;**`32768`**&#x4E2A; tokens 的上下文长度范围支持。同时，预训练 Base 模型的质量也有关键优化。**Qwen1.5** 的性能在多个标准基准测试中得到了验证，包括基础能力（如语言理解、代码生成、推理等）、多语言能力、对齐能力、智能体 **Agent** 能力以及检索增强生成 **RAG** 能力：

> 1. **通用能力**：在不同模型尺寸下，**Qwen1.5** 都在评估基准中表现出强劲的性能。**Qwen1.5-72B** 在所有基准测试中都远远超越&#x4E86;**&#x20;**&#x4C;LaMA-2-70B，展示了其在语言理解、推理和数学方面的卓越能力。参数规模低于 70 亿的 **Qwen1.5** Base 模型，与业界领先的小型模型相比具有很强的竞争力。
>
> 2. **对齐能力**：尽管落后于 GPT-4-Turbo，但最大的 **Qwen1.5** 模型 **Qwen1.5-72B-Chat** &#x5728;**`MT-Bench`**&#x548C;**`Alpaca-Eval v2`**&#x4E0A;都表现出不俗的效果，超过了 Claude-2.1、GPT-3.5-Turbo-0613、Mixtral-8x7b-instruct 和 TULU 2 DPO 70B，与 Mistral Medium 不相上下。
>
> 3. **多语言能力**：**Qwen1.5** Base 模型在 12 种不同语言的多语言能力方面表现出色，在考试、理解、翻译和数学等各个维度的评估中，均展现优异结果。不论阿拉伯语、西班牙语、法语、日语，还是韩语、泰语，**Qwen1.5&#x20;**&#x5747;展示了在不同语言环境中理解和生成高质量内容的能力。
>
> 4. **长序列能力**：即使像 **Qwen1.5-7B-Chat** 这样的小规模模型，在多个任务中表现出与 GPT3.5-turbo-16k 类似的性能。最好的模型 **Qwen1.5-72B-Chat**，仅略微落后于 GPT4-32k。
>
> 5. **外部能力**：较大的 **Qwen1.5-Chat** 模型通常优于较小的模型，接近 GPT-4 的工具使用性能。不过，在数学解题和可视化等代码解释器任务中，即使是最大的 **Qwen1.5-72B-Chat** 模型，也会因编码能力而明显落后于 GPT-4。

**Qwen1.5 MoE**

**Qwen1.5-MoE-A2.7B**，仅拥有 27 亿个激活参数，但性能却能与当时最先进的 70 亿参数模型，如 Mistral 7B 和 Qwen1.5-7B 相媲美。相较于包含 65 亿个 Non-Embedding 参数的 **Qwen1.5-7B**，**Qwen1.5-MoE-A2.7B** 只有 20 亿个 Non-Embedding 参数，约为原模型大小的三分之一。此外，相比 **Qwen1.5-7B**，**Qwen1.5-MoE-A2.7B** 的训练成本降低&#x4E86;**`75%`**，推理速度则提升&#x81F3;**`1.74`**&#x500D;。

**Qwen1.5-MoE** 采用了特别设计的MoE架构。通常情况下，如 Mixtral 等，每个 Transformer block 中的 MoE 层会配备 8 个 expert，并采用 top-2 门控策略进行 Routing，但这种配置还存在很大的优化空间。**Qwen1.5-MoE** 对这一架构进行了多项改进：

> 1. **Finegrained Experts**：DeepSeek-MoE 和 DBRX 已经证明了 Finegrained Experts 的有效性。从 FFN 层过渡到 MoE 层时，一般只是简单地复制多次 FFN 来实现多个 Expert。而 Finegrained Experts 的目标是在不增加参数数量的前提下生成更多 Expert，将单个 FFN 分割成几个部分，每个部分作为一个独立的 Expert。**Qwen1.5-MoE** 设计了具有总共 64 个 Expert 的 MoE，这个实现能达到效果和效率的最优。
>
> 2. **初始化**：实验表明从零开始训练 MoE 模型可能效率低下，且难以提升至预期的最优性能水平。因此首先利用已有的 Qwen-1.8B，将其改造为 **Qwen1.5-MoE-A2.7B**。此外在初始化阶段引入随机性可以显著加快收敛速度，并在整个预训练过程中带来更好的整体性能表现。
>
> 3. **新的 Routing 机制**：目前一个明显的趋势是在 MoE 中实现共享 Expert 与 Routing Expert。从宏观角度看这是一种广义的 Routing 方法，因为在没有共享 Expert 的情况下，实际上就退化为传统的 MoE 路由设置。**Qwen1.5-MoE-A2.7B** 在其中整合&#x4E86;**`4`**&#x4E2A;总是被激活的共享 Expert 和每次只激活其&#x4E2D;**`4`**&#x4E2A;&#x7684;**`60`**&#x4E2A; Routing Expert，这种方式非常灵活，并且效率最佳。







### 4.4.3 Qwen2

2024 年 6 月 7 日，阿里巴巴发布了最新的大模型 **Qwen2**，迎来了 **Qwen** 系列模型从 **Qwen1.5** 到 **Qwen2** 的重大升级。相比 **Qwen1.5**，**Qwen2** 在大规模模型实现了非常大幅度的效果提升。**Qwen2-7B** 的数学能力，甚至可以比肩 **Qwen1.5-110B**。

![]()

**Qwen2** 系列具备以下特点：

> 1. 5 个尺寸的预训练和指令微调模型, 包&#x62EC;**`Qwen2-0.5B`**、**`Qwen2-1.5B`**、**`Qwen2-7B`**、**`Qwen2-57B-A14B`** 以&#x53CA;**`Qwen2-72B`**
>
> 2. 在中文英语的基础上，训练数据中增加了 27 种语言相关的高质量数据
>
> 3. 多个评测基准上的领先表现
>
> 4. 代码和数学能力显著提升
>
> 5. 增大了上下文长度支持，最高达到 128K tokens（**`Qwen2-72B-Instruct`**）

5 个模型的参数规模如下表所示：

| **模型**            | **0.5B** | **1.5B** | **7B** | **57B-A14B** | **72B** |
| ----------------- | -------- | -------- | ------ | ------------ | ------- |
| 参数量               | 0.49B    | 1.54B    | 7.07B  | 57.41B       | 72.71B  |
| 非Embedding参数      | 0.35B    | 1.31B    | 5.98B  | 56.32B       | 70.21B  |
| Tie Embedding     | True     | True     | False  | False        | False   |
| 上下文长度             | 32K      | 32K      | 128K   | 64K          | 128K    |
| Hidden Size       | 896      | 1,536    | 3,584  | 3,584        | 8,192   |
| Layers            | 24       | 28       | 28     | 28           | 80      |
| Query Heads       | 14       | 12       | 28     | 28           | 64      |
| KV Heads          | 2        | 2        | 4      | 4            | 8       |
| Head Size         | 64       | 128      | 128    | 128          | 128     |
| Intermediate Size | 4,864    | 8,960    | 18,944 | 2,560        | 29,568  |
| Embedding Tying   | True     | True     | False  | False        | False   |
| Trained Tokens    | 12T      | 7T       | 7T     | 4.5T         | 7T      |

其中 **Qwen2-57B-A14B&#x20;**&#x4E2D;的专家参数如下：Routed Experts = 64，Activated Experts = 8，Shared Experts = 8。

* **模型结构**

在 **Qwen1.5** 系列中，只有 32B 和 110B 的模型使用了 **GQA**。这一次，所有尺寸的模型都使用了 **GQA**，带来了推理加速和显存占用降低的优势。针对小模型，由于 embedding 参数量较大，**Qwen2** 使用了 tie embedding 的方法让输入和输出层共享参数，增加非 embedding 参数的占比。

上下文长度方面，所有的预训练模型均在 32K tokens 的数据上进行训练，并且其在 128K tokens 时依然能在 PPL 评测中取得不错的表现。而在使&#x7528;**`YARN`**&#x8FD9;类方法时，Qwen2-7B-Instruct 和 Qwen2-72B-Instruct 均实现了长达 128K tokens 上下文长度的支持。

**tokenizer**

**Qwen2** 使用 BPE 分词器，这种分词器在 **Qwen1.5** 中已经使用过，BPE 具有高编码效率，这得益于它比其他替代方案有更好的压缩比率。这种高效的编码方式对于处理大型语言模型尤其重要，它可以减少模型训练和推理时的内存和计算需求。

tokenizer 的设计支持多语言能力，这对于 **Qwen2** 模型来说是一个关键特性，因为它需要能够处理包括英语、中文、西班牙语、法语、德语、阿拉伯语、俄语、韩语、日语、泰语和越南语等在内的约 30 种语言。所有模型使用了一个共有的词汇表，包&#x542B;**`151,643`**&#x4E2A;常规 tokens &#x548C;**`3`**&#x4E2A; control tokens：**`<|endoftext|>`**, **`<|im_start|>`**, **`<|im_end|>`**。这种统一的词汇表有助于保持不同模型规模之间的一致性。

**Qwen2 Dense**

**Qwen2 Dense** 基于 Transformer 架构， 与前一代 **Qwen** 模型相比，**Qwen2 Dense** 在多个方面进行了改进，以提高模型的性能和效率：

> 1. **分组查询注意力 GQA**：**Qwen2** 采用&#x4E86;**`GQA`**&#x66FF;代了 MHA。GQA 通过优化 KV Cache 使用，在推理过程中显著提高了吞吐量
>
> 2. **DCA** **与 YARN**：为了扩大模型的上下文窗口，**Qwen2** 实现了双块注意力机&#x5236;**`DCA`**，将长序列分割成可管理的长度块，此&#x5916;**`YARN`**&#x7528;于重新调整注意力权重，以更好地处理不同长度的序列
>
> 3. **其他技术**：**Qwen2** 模型还使用&#x4E86;**`SwiGLU`**&#x4F5C;为激活函数，**`RoPE`**&#x4F5C;为位置编码，**`QKV bias`**&#x6765;改善注意力机制，以&#x53CA;**`RMSNorm`**&#x548C;**`Pre-Norm`**&#x6280;术来提高训练稳定性

**双块注意力** **DCA**（**D**ual **C**hunk **A**ttention）

**DCA** 是一种用于自然语言处理任务的注意力机制，旨在高效处理长序列数据。**DCA** 通过将输入序列分块来减少计算复杂度，同时保留全局信息和局部信息。**DCA** 通常分为两个阶段：局部注意力和全局注意力。

**局部注意力阶段**：输入序列被分成若干个块，每个块内的元素之间进行注意力计算。这种方式可以有效减少计算复杂度，因为注意力计算只在较小的块内进行。

**全局注意力阶段**：每个块的表示被用来计算全局注意力，这个表示可以是该块内所有元素的平均值或最大值。然后，这些块表示之间进行注意力计算，从而捕捉全局信息。

**例**：假设有一个长度为 12 的输入序列：

$$\text{Input} = [x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_{10}, x_{11}, x_{12}]\\$$

将这个序列分成3个块，每个块包含4个元素：

$$\text{Chunk}_1 = [x_1, x_2, x_3, x_4] , \text{Chunk}_2 = [x_5, x_6, x_7, x_8], \text{Chunk}_3 = [x_9, x_{10}, x_{11}, x_{12}]$$

**局部注意力**：在每个块内计算注意力。对于第一个块：

$$\text{Attention}(x_i, x_j) = \frac{\exp(x_i \cdot x_j)}{\sum_{k=1}^{4} \exp(x_i \cdot x_k)}\\$$

这里 $$\cdot$$表示点积操作。然后对每个块进行类似的操作，得到每个块内的注意力表示。

**全局注意力**：对于每个块，计算一个全局表示。假设使用平均值作为块的表示：

$$R_1 = \frac{1}{4} \sum_{i=1}^{4} x_i, R_2 = \frac{1}{4} \sum_{i=5}^{8} x_i, R_3 = \frac{1}{4} \sum_{i=9}^{12} x_i \\$$

然后在这些块表示之间计算全局注意力：

$$\text{Global Attention}(C_i, C_j) = \frac{\exp(C_i \cdot C_j)}{\sum_{k=1}^{3} \exp(C_i \cdot C_k)} \\$$

这里$$C_i$$表示第$$i$$个块的表示。

**DCA** 通过分块的方式，将长序列的注意力计算分解为局部注意力和全局注意力两部分，从而有效减少计算复杂度，同时保留全局和局部信息。这种方法特别适用于处理长序列数据的任务，如长文本的自然语言处理。

**YARN**（**Y**et **A**nother **R**escaling **M**ethod）

**YARN** 用于重新调整注意力权重，以改善模型对不同长度序列的处理能力。它通过重新调整注意力权重，使得模型能够更好地处理长序列和短序列，避免在处理长序列时注意力权重过于分散，从而提升模型的整体表现。

**YARN** 的核心思想是对注意力权重进行重新缩放，以便在处理不同长度的序列时，注意力机制能够更有效地聚焦于重要的信息。具体来说，**YARN** 通过以下步骤实现这一目标：

> 1. **计算注意力权重**：计算原始的注意力权重矩阵，一般通过点积注意力机制或其他注意力机制实现
>
> 2. **计算缩放因子**：根据序列的长度计算缩放因子，这个因子通常是与序列长度相关的函数，用于调整注意力权重的分布
>
> 3. **应用缩放因子**：将计算得到的缩放因子应用到原始的注意力权重矩阵上，重新调整权重的分布
>
> 4. **归一化**：对调整后的注意力权重进行归一化处理，确保权重的总和为 1

**例**：假设有一个序列长度为 5 的输入序列，模型计算得到的原始注意力权重矩阵为：

$$A = \begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.2 & 0.2 \\ 0.2 & 0.1 & 0.2 & 0.3 & 0.2 \\ 0.3 & 0.2 & 0.1 & 0.2 & 0.2 \\ 0.2 & 0.3 & 0.2 & 0.1 & 0.2 \\ 0.2 & 0.2 & 0.2 & 0.2 & 0.2 \\ \end{bmatrix}$$

假设使用一个简单的缩放因子$$\alpha = \frac{1}{\sqrt{L}}$$，其中$$L$$是序列的长度。在这个例子中，序列长度$$L=5$$，因此缩放因子$$\alpha = \frac{1}{\sqrt{5}} \approx 0.447$$。

将缩放因子应用到原始注意力权重矩阵上：

$$A' = \alpha A = 0.447 \cdot \begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.2 & 0.2 \\ 0.2 & 0.1 & 0.2 & 0.3 & 0.2 \\ 0.3 & 0.2 & 0.1 & 0.2 & 0.2 \\ 0.2 & 0.3 & 0.2 & 0.1 & 0.2 \\ 0.2 & 0.2 & 0.2 & 0.2 & 0.2 \\ \end{bmatrix} = \begin{bmatrix} 0.0447 & 0.0894 & 0.1341 & 0.0894 & 0.0894 \\ 0.0894 & 0.0447 & 0.0894 & 0.1341 & 0.0894 \\ 0.1341 & 0.0894 & 0.0447 & 0.0894 & 0.0894 \\ 0.0894 & 0.1341 & 0.0894 & 0.0447 & 0.0894 \\ 0.0894 & 0.0894 & 0.0894 & 0.0894 & 0.0894 \\ \end{bmatrix}$$

最后，对调整后的注意力权重矩阵进行归一化处理，使得每一行的权重总和为 1：

$$A'' = \text{softmax}(A')$$

通过这个过程，**YARN** 机制能够有效地调整注意力权重，使得模型在处理不同长度的序列时，能够更好地聚焦于重要的信息，从而提升模型的性能。

**Qwen2 MoE**

**Qwen2 MoE** 模型的架构与 **Qwen1.5-MoE-A2.7B** 非常相似。作为原始 **FFN** 的替代，**MoE FFN** 由$$n$$个单独的 **FFN** 组成，每个 **FFN** 充当一个专家。根据门控网络$$G$$分配的概率，每个 token 被定向到特定的 expert $$E_i$$进行计算：

$$p=\text{softmax}\left( G\left( x \right) \right)$$

$$y=\sum_{i\in\text{top}_k\left( p \right)}E_i\left( x \right)$$

下面是 **Qwen2 MoE** 和其它 **MoE** 模型的主要区别：

**专家粒度**： **MoE** 与 **Dense** 之间的一个关键结构差异在于，**MoE** 层包含多个 **FFN**，每个 **FFN** 作为一个独立的专家。因此从 **Dense** 架构过渡到 **MoE** 架构的一个直接策略是将每个专家的参数设置为原始 **Dense** 模型中单个 **FFN** 的参数。与这个策略不同地是 **Qwen2** 的模型采用了细粒度专家，同时创建规模更小的专家并激活更多的专家。在专家参数总数和激活参数数相等的情况下，细粒度专家提供了更丰富的专家组合。通过利用这些细粒度专家，**Qwen2 MoE** 促进了更多样化和动态的专家利用，从而提高了整体性能和适应性。

**专家路由**：设计专家路由机制对于提高 **MoE** 模型的性能至关重要。最近的工作出现了一个明显的趋势，即在 **MoE** 层中整合共享专家和特定路由专家。**Qwen2** 采用了这种方法，因为它便于在各种任务中应用共享专家，同时保留其他专家用于特定的路由场景选择性使用。引入共享专家和专用专家提供了一种更适应性强、效率更高的方法来开发 **MoE** 路由机制。

**专家初始化**：给定指定专家的中间维度大小$$h_E$$、专家数量$$n$$以及原始 **FFN** 中间维度大小$$h_{FFN}$$，**FFN** 将复制&#x20;

$$\lceil n \times h_E / h_{FFN} \rceil$$次。这种复制确保了与指定专家数量的兼容性，同时适应任何任意的专家中间维度大小。为了促进每个 **FFN** 副本内部的多样性，参数会沿着中间维度进行洗牌。这保证了每个细粒度专家即使在不同的 **FFN** 副本中也展现出独特的特征。随后，从 **FFN&#x20;**&#x526F;本中提取这些专家，并丢弃剩余的维度。对于每个细粒度专家，其 50% 的参数将被随机重新初始化。这个过程在专家初始化中引入了额外的随机性，可能增强了模型在训练期间的探索能力。

* **预训练**

在 **Qwen2** 的预训练中，主要重点放在完善数据集和研究有效处理扩展上下文长度的方法上：

**预训练数据**

**Qwen2** 模型的预训练涉及到开发一个新的、大规模的、高质量的多语言数据集。这个数据集相比之前 **Qwen** 和**Qwen1.5** 使用的数据集有了改进，增强了预训练数据的规模、质量和多样性，主要在以下几个关键领域：

**质量增强：**&#x8FC7;滤算法已经通过额外的启发式和基于模型的方法进行了改进，包括使用 **Qwen** 来筛选出低质量的数据。此外，这些模型还被用来合成高质量的预训练数据。

**数据扩展：Qwen2** 收集了更大量的高质量代码、数学和多语言数据，增强了模型在这些领域的能力。这个新数据集支持大约 30 种语言，如英语、中文、法语、德语等。

**分布改进**：为了确保模型学习到类似于人类学习的分布，**Qwen2** 在小模型上进行实验，以优化来自不同来源和领域的数据混合。基于这些增强，预训练数据从 **Qwen1.5** 的 3 万亿个 token 扩展到了 7 万亿个 token。尝试进一步放宽质量阈值，产生了一个 12 万亿个 token 的数据集。但在这个数据集上训练的模型并没有显示出比 7 万亿个 token 更好的效果。除了 **Qwen2-0.5B** 之外，所有的 **Qwen2 Dense** 模型都是在这个超过 7 万亿个 token 的大规模数据集上进行预训练的。**Qwen2-0.5B** 是在使用 12 万亿个 token 的数据集上进行预训练的。**MoE** 模型根据提升再利用的原则，另外接收了 4.5 万亿个 token 的预训练。与之前的 **Qwen** 类似，高质量的多任务指令数据被整合到 **Qwen2** 的预训练过程中，以增强上下文学习和指令跟随能力。

**长文本训练**

为了增强 **Qwen2** 处理长文本的能力，在预训练的最后阶段，**Qwen2** 将上下文长度从 4,096 个 token 增加到了32,768 个token。这一扩展通过引入大量高质量的长文本数据得到了补充。为了配合这些增强，**Qwen2** 将 **RoPE** 的频率从 10,000 调整到了 1,000,000，以优化长上下文场景下的性能。为了充分利用模型的外推潜力，**Qwen2** 采用了 **YARN** 机制和双块注意力机制 **DCA**。这些策略使模型能够处理长达 131,072 个 token 的序列，同时保持高性能。

* **后训练**

在进行了广泛的大规模预训练之后，**Qwen2** 进行了后训练阶段。这一过程对于提高其在包括编程、数学、逻辑推理、指令遵循和多语言理解在内的效果至关重要。此外，它确保了模型的生成与人类价值观相符合。与严重依赖大量人类监督的传统方法不同，**Qwen2** 的方法侧重于可扩展的对齐，最小化人类注释。具体来说，**Qwen2** 研究了获取高质量指令和偏好数据的方法，用于 **SFT** 和 **RLHF**，旨在在最大化数据质量和可靠性的同时，最小化对人类标记的需求。

**后训练数据**

后训练数据主要由两个组成部分构成：问题-答案数据集$$D = \left\{ (x_i, y_i) \right\}$$和偏好数据集$$P = \{(x_i, y_i^{+}, y_i^{-})\}$$，其中$$x_i$$代表指令，$$y_i$$代表回答，$$y_i^+$$和$$y_i^-$$是针对$$x_i$$的两个响应，前者表示回答更好。数据集$$D$$用于 SFT，而数据集 $$P$$用于RLHF。

训练数据的构建涉及两步过程：协作数据注释和自动化数据合成。首先，从大规模指令语料库中提取数据，从而得到广泛和多样化的高质量指令集合。这些指令被系统性地增强，以包含更大的复杂性。通过人工注释，获得目标响应$$y_i$$及其正面和负面的回答$$(y^+_i, y^−_i)$$。随后，采用多种自动化对齐策略来合成大量人工标注的数据，涵盖代码、数学、指令遵循、创作、角色扮演和安全等领域。

**协作数据注释**

**自动提取**：首先应&#x7528;**`InsTag`**，一个开放式细粒度标记器，从小规模指令数据集中提取底层本体

**指令选择**：每个带有标签的指令都根据标签多样性、语义丰富性、复杂性和意图完整性进行评估，基于这些标准，选择一组代表性的指令

**指令演化**：为了丰富指令数据集，采用自我演化策略，促使 **Qwen** 模型向现有指令添加约束或要求，从而增加它们的复杂性，并确保数据集中不同难度级别的多样性

**人工注释**：使用不同的生成策略和不同规模的 **Qwen** 模型获得对指令的多种响应，注释者根据他们的偏好对这些响应进行排名，确保最佳响应满足既定标准，产生示范和偏好数据

**自动化数据合成**

**拒绝采样**：对于具有明确答案的数学类任务，应用拒绝采样来提高数据质量，大模型为每个指令生成多个响应，即推理路径，那些得出正确回答并且被模型认为是合理的路径将被保留，作为示范数据。通过对比正确和错误的路径来生成偏好数据。

**执行反馈**：对于编程任务，LLM 被用来生成解决方案和相关的测试用例。通过编译和执行这些解决方案来对抗测试用例，从而评估这些解决方案的有效性，由此创建示范数据和偏好数据。这种方法也适用于评估指令遵循。对于每个带有约束的指令，例如`长度限制`，LLM 被指派生成一个 Python 验证函数，以确保响应符合指令要求。

**数据再利用**：在文学写作任务中创造好的回答对于没有专门训练的注释者来说是一个挑战。为了解决这个问题，**Qwen2** 收集了来自公共领域的高质量文学作品，并利用大模型来制定不同详细程度的指令。这些指令与原始作品配对，作为示范数据。例如，`为了生成生动有趣的角色扮演数据，从维基百科这样的知识库中获取详细的人物档案，并指导大模型生成相应的指令和响应`。这个过程类似于阅读理解任务，确保了人物档案的完整性得以保持。

**SFT**

**Qwen2** 收集了一个包含超过 50 万个示例的广泛指令数据集，涵盖了指令遵循、编程、数学、逻辑推理、角色扮演、多语言能力和安全等技能。对模型进行两轮 **SFT**，序列长度为 32,768。为了优化学习过程，学习率从 7e-6 逐渐降低到 7e-7。为了解决过拟合问题，应用了 0.1 的权重衰减，并将梯度裁剪在最大值 1.0。

**RLHF**

RLHF 训练方案包括两个连续的阶段：离线训练和在线训练。在离线训练阶段，使用预编译的偏好数据集$$P$$，通过**DPO** 来最大化$$y_i^+$$和$$y_i^-$$之间的差异。在在线训练阶段，模型利用 reward model 进行实时反馈，迭代地优化其性能。具体来说，从当前的策略模型中采样多个响应，Reward Model 选择最优和最差的响应，形成 **DPO** 的偏 pair 对。此外，**Qwen2** 采用了 [**Online Merging Optimizer**](https://arxiv.org/pdf/2405.17931) 来减轻对齐成本。

### 4.4.4 Qwen2.5

与之前的版本相比，**Qwen2.5&#x20;**&#x5728;预训练和后训练阶段都有显著改进。预训练数据从 7T tokens 扩展&#x4E3A;**`18T`&#x20;**&#x74;okens，为常识、专业知识、推理能力提供了坚实的基础。后训练阶段包括在超&#x8FC7;**`1M`**&#x6570;据上进行的 SFT 和多阶段 RL：**`offline DPO`**&#x548C;**`online GRPO`**。后训练显著增强了人类偏好，改善了长文本生成、结构数据分析和指令跟踪能力。

**Qwen2.5** 开源了各个规模的 Base 和 Instruct 模型，包括 **`0.5B`**、**`1.5B`**、**`3B`**、**`7B`**、**`14B`**、**`32B`**、**`72B`**，以及量化版本。闭源模型目前包括两个 **MoE** 变体：**`Qwen2.5-Turbo`** 和 **`Qwen2.5-Plus`**。**Qwen2.5** 在语言理解、推理、数学、代码、人类偏好对齐等广泛 benchmark 上展现了顶级性能。**Qwen2.5-72B-Instruct** 优于许多其他开源和闭源模型。**Qwen2.5-Turbo** 和 **Qwen2.5-Plus** 的成本也与 **GPT-4o-mini** 和 **GPT-4o** 相比具有竞争力。

**Qwen2.5** 在支持训练领域专家模型的训练中发挥了重要作用，包&#x62EC;**`Qwen2.5-Math`**、**`Qwen2.5-Coder`**、**`QwQ`** 和多模态模&#x578B;**`Qwen2.5-VL`**。总的来说，**Qwen2.5** 有几个关键特征：

> 1. **模型规模**：相比上一代 **Qwen2**，新增了 3B、14B、32B 模型。Qwen2.5-Turbo 和 Qwen2.5-Plus 在准确率、延迟、成本之间提供了很好的平衡
>
> 2. **数据规模**：预训练数据增加到了 18T tokens，关注知识、代码、数学。分阶段进行预训练以允许不同数据混合之间的过渡。后训练数据超过 1M 样本，涵盖 SFT、DPO、GRPO
>
> 3. **模型使用**：相比上一代，生成长度从 2K 提升至 8K。更好地支持结构化输入和输出，更易于使用工具。此外，Qwen2.5-Turbo 支持 1M 的上下文长度

对于 Dense 模型，保持 Transformer Decoder 架构。其他关键组件也与前几代相同：用于高效 KV cache &#x7684;**`GQA`**、**`SwiGLU`**&#x6FC0;活函数、**`RoPE`**&#x4F4D;置编码、注意力机制&#x4E2D;**`QKV bias`**、**`RMSNorm`**&#x5F52;一化。各个参数规模的参数如右表：

![]()

在 Dense 模型架构的基础上，通过用 **MoE** 层替换标准 **FFN** 层来扩展到 **MoE** 架构。与 **Qwen1.5-MoE** 相同，进行细粒度专家划分 [**DeepSeekMoE**](https://arxiv.org/pdf/2401.06066) 和共享专家路由 [**DeepSpeed-MoE**](https://proceedings.mlr.press/v162/rajbhandari22a/rajbhandari22a.pdf)。提供了下游任务中模型性能的实质提升。

Tokenizer &#x4E3A;**`BBPE`**，词表大&#x5C0F;**`151,643`**。与之前版本相比，control tokens 的数量从 3 增加&#x5230;**`22`**，加入&#x4E86;**`2`**&#x4E2A;用于工具调用的新 token。

* **预训练**

预训练阶段由多个关键部分组成：

> 1. 通过复杂的过滤和打分机制，结合数据混合策略，仔细收集高质量预训练数据
>
> 2. 对超参数进行了大量研究，以有效进行各个规模模型的训练
>
> 3. 加入专门的长上下文预训练来增强模型对长文的能力

**数据**

与上一代相比，**Qwen2.5** 预训练数据质量的提升：

> 1. **数据过滤**：数据质量评估和过滤是 pipeline 的关键部分。使用 **Qwen2-Instruct** 作为过滤器，对数据进行全面、多维度地评估和打分。由于 **Qwen2** 在更大的多语言语料库上进行了训练，因此过滤相比 **Qwen2** 有了显著提升，提高了高质量数据的保留，有效过滤了跨多个语言的低质数据。
>
> 2. **数学和代码数据**：结合了 **Qwen2.5-Math** 和 **Qwen2.5-Coder** 的训练数据。这种数据集成策略被证明非常有效。
>
> 3. **合成数据**：同时使用 **Qwen2-72B-Instruct** 和 **Qwen2-Math-72B-Instruct** 进行高质量合成数据生成。通过专有的通用 **RM** 和 **Qwen2-Math-RM-72B** 进行严格过滤。
>
> 4. **数据混合**：使用 **Qwen2-Instruct** 对不同领域的内容进行分类和平衡。分析显示，电商、社交媒体、娱乐等领域数据过多，这些数据通常包含重复、基于模版、机器生成等内容。相反，科技、学术研究等领域数据虽然包含更高质量信息，但是占比较低。通过下采样和上采样，进行更优的数据混合。

通过这些方法，得到了更大更高质量的 18T 数据集。

**超参数**

基于预训练数据进行超参数 **scaling law** 研究。在先前的给定计算资源下的最优模型规模的 **scaling law** 的基础上，进行跨模型架构的最优超参数识别。对于不同大小的 **Dense** 模型和 **MoE** 模型来确定 batch size 和学习率等关键超参数。

通过大量实验系统研究了模型架构与最优超参数之间的关系。分析了最优**学习率**$$μ_\text{opt}$$和 **batch size** $$B_\text{opt}$$如何随着模型规模 N 和预训练数据规模 D 的变化而变化。实验包&#x62EC;**`44M`**&#x5230;**`14B`**&#x89C4;模的 **Dense** 模型以&#x53CA;**`44M`**&#x5230;**`1B`**&#x89C4;模的 **MoE** 模型，&#x4ECE;**`0.8B`**&#x5230;**`600B`** tokens 数据集上训练。利用最优超参数预测，将最终的 loss 建模为模型架构和训练数据规模的函数。

此外，利用 **scaling law** 来预测和比较不同规模 **MoE** 模型及其 **Dense** 模型的表现，指导 **MoE** 模型的超参数配置，使得能够通过调整激活参数和总参数来实现与特定 **Dense** 模型变体的性能等价。

**长上下文预训练**

两阶段训练方法：

初始采&#x7528;**`4,096`**&#x4E0A;下文长度，之后在预训练的最后阶段扩展&#x5230;**`32,768`**&#x4E0A;下文长度（除了 **Qwen2.5-Turbo**）。同时，利&#x7528;**`ABF`**&#x6280;术将 **RoPE** 的基频从 10,000 提高到 1,000,000。

对于 **Qwen2.5-Turbo**，进行 4 个阶段的渐进上下文长度扩展策略：**`32,768`**、**`65,536`**、**`131,072`**、**`262,144`**。RoPE 基频为 1,000,000。在每个阶段，仔细调整训练数据来涵盖 40% 序列满足当前最大长度，60% 为更短的序列。

采&#x7528;**`YARN`**&#x548C;**`DCA`**&#x6765;增强模型在推理过程中处理长序列的能力。通过这些创新，实现了模序列长度容量的 4 倍增长，**Qwen2.5-Turbo** 能够处理 1M tokens，其他模型能够处理 131,072 tokens。

* **后训练**

与 **Qwen2** 相比，**Qwen2.5** 在后训练设计上有两个改进：

1. **扩展 SFT 数据覆盖**

利用了百万级别的高质量 **SFT** 数据，数据扩展专门针对先前模型的不足领域，如长序列生成、数学问题、代码、指令跟随、结构化数据理解、逻辑推理、跨语言迁移、鲁棒的系统指令。

* **两阶段强化学习**

**Qwen2.5** 的 RL 过程分为两个阶段：**Offline RL** 和 **Online RL**：

**Offline RL**

关注 **RM** 较难评估的能力的开发，如推理、事实、指令跟随。通过精心构造和验证训练数据，确保 Offline RL 信号既是可学习的又是可靠的。

**Online RL**

利用 **RM** 检测输出质量细微差别的能力，包括 truthfulness、helpfulness、简明性、相关性、无害性、无偏性。

**SFT**

**Qwen2.5** 在 **SFT** 阶段主要进行了 9 个领域的关键改进：

**长序列生成**

**Qwen2.5** 上下文长度可达 8,192 个token，显著超过了通常的后训练响应长度。为弥补这一差距，开发了长响应数据集。通过回译技术从预训练语料库生成长文本数据的查询，施加输出长度限制，并使用**Qwen2** 过滤掉低质量的配对数据。

**数学**

引入了 **Qwen2.5-Math** 的链式思考数据，涵盖了多种查询来源，包括公共数据集、K-12 问题集合和合成问题。为确保高质量的推理，采用了拒绝采样方法，并结合奖励建模和标注答案进行指导，生成逐步推理过程。这种方法保证了推理过程的准确性和详细性。

**代码**

整合了 **Qwen2.5-Coder** 的指令调优数据，利用多语言代理生成近 40 种编程语言的高质量指令对。通过合成代码相关问答网站的新示例和收集 **GitHub** 上的算法代码片段来扩展数据集。采用多语言沙箱进行静态代码检查，并通过自动化单元测试确保代码质量与正确性。

**指令跟随**

实施了一个严格的基于代码的验证框架。大型语言模型生成指令及其对应的验证代码，并附带全面的单元测试以进行交叉验证。通过基于执行反馈的拒绝采样，精心筛选用于监督微调的训练数据，从而保证模型忠实遵循预期指令。

**结构数据理解**

开发了一个全面的结构化理解数据集，涵盖表格问答、事实验证、错误纠正和结构理解的传统任务和涉及结构化和半结构化数据的复杂任务。通过在模型响应中引入推理链，显著增强了从结构化数据中推断信息的能力，从而提升在这些多样化任务中的表现。

**逻辑推理**

引入了涵盖多个领域的 70,000 个新的查询，包括选择题、判断题和开放式问题。模型经过训练，能够系统地解决问题，采用多种推理方法，如演绎推理、归纳概括、类比推理、因果推理和统计推理。通过迭代精炼，系统地过滤掉包含错误答案或有缺陷推理过程的数据。

**翻译**

将高资源语言指令转换为多种低资源语言，生成候选回复。为确保回复的准确性和一致性，评估每个多语言响应与其原始版本之间的语义对齐。这保留了原始响应的逻辑结构和风格细节，从而在不同语言中保持其完整性和一致性。

**系统指令**

构建了数百个通用系统提示，以提高后训练中系统提示的多样性，并确保系统提示与对话之间的一致性。通过不同系统提示进行的评估表明，模型保持了良好的性能并减少了方差，显示出更强的鲁棒性。

**回复过滤**

采用了多种自动标注方法，包括专用的批评模型和多代理协作评分系统。响应经过严格评估，只有在所有评分系统中都被认为无瑕疵的响应才会被保留。这种方法确保输出内容维持最高质量标准。

最终，构建了一个包含超&#x8FC7;**`1M`**&#x6570;据的 **SFT** 数据集。&#x5728;**`32,768`**&#x5E8F;列长度下微调了 2 个 epochs。学习率从 7e−6 逐渐降低到 7e−7 。为了解决过拟合问题，应用了 0.1 的 weight decay 并进行最大值为 1.0 的梯度裁剪。

**Offline DPO**

与 **Online RL** 相比，**Offline RL** 能够预先准备训练数据，适用于存在标准答案单但 **RM** 难以评估的任务。

这里主要关注客观 query 领域，比如数学、代码、指令遵循、逻辑推理，在这些领域获得准确的 evaluation 可能很复杂。在 **SFT** 中，大规模应用执行反馈和答案匹配等策略来确保回复的质量。在 **Offline RL** 阶段重用这一 pipeline，使用 **SFT** 模型对一组新的 query 进行重新采样。

在 **DPO** 中，通过质量检查的回复为正例，未通过的为负例。通过人工和自动检查过程来进一步提高训练数据的可靠性和准确性，同时确保数据不仅是可学习的，也符合人类的期望。最终，构建了规模约&#x4E3A;**`150,000`** pairs 的训练数据。&#x7528;**`Online Merging Optimizer`**&#x8BAD;练了 1 个 epoch，学习率为 7e−7 。

**Online GRPO**

为了开发一个用于 **Online RL** 的强大 **RM**，采用了一套精心制定的标注标准。这套标准确保模型生成的回复不仅是高质量的，而且符合道德并且以用户为中心。数据标注具体标准如下：

> 1. **Truthfulness**：回复必须以事实准确性为基础，真实的反应提供的背景和指令。模型应避免生成错误或给出数据不支持的信息。
>
> 2. **Helpfulness**：模型回复应该是真正有用的，有效地解决用户的查询，同时提供积极、吸引人、有教育意义和相关的内容。它应该精确地遵循给定的说明，为用户提供价值。
>
> 3. **Conciseness**：回复应简明扼要，避免不必要的冗长。目标是清晰有效地传达信息，而不会用过多的细节压倒用户。
>
> 4. **Relevance**：回复的所有部分都应该与用户的查询、对话历史和助手的上下文直接相关。模型应调整其输出，以确保其完全符合用户的需求和期望。
>
> 5. **Harmlessness**：模型必须优先考虑用户安全，避免任何可能导致非法、不道德或有害行为的内容。它应该始终促进道德行为和负责任的沟通。
>
> 6. **Debiasing**：模型应产生无偏见的回复，包括但不限于性别、种族、国籍和政治。它应该平等、公平地对待所有话题，遵守广泛接受的道德和伦理标准。

用于训练 **RM** 的 query 来自两个不同的数据集：开源数据和具有更高复杂性的专有数据。

回复是由 **Qwen** 模型生成的，**Qwen** 模型来自 **SFT**、**DPO**、**RL** 等不同阶段的训练。为了引入多样性，用不同温度进行采样。偏好对是由人工和自动化标注过程构建的，**DPO** 训练数据也囊括&#x5230;**&#x20;RM&#x20;**&#x6570;据中。

**Online RL** 框架采用了 **GRPO**。用于训练 **RM** 的提问集和 **RL&#x20;**&#x8BAD;练的提问集是相同的。训练过程中提问的处理顺序由其回复评分的方差决定，该方差是由奖励模型评估得出的，回复方差较大的提问先被训练。对每个提问采&#x6837;**`8`**&#x4E2A;回复。

所有模型都使&#x7528;**`2048`**&#x7684;全局批量大小和每个阶&#x6BB5;**`2048`**&#x4E2A;样本进行训练，其中将一对提问和回复视为一个样本。

**长上下文微调**

为了进一步扩展 **Qwen2.5-Turbo** 的上下文长度，在后训练期间引入了更长的 **SFT** 数据，使得模型在长提问下与人类偏好保持一致。这里使用了两阶段的 SFT：

> 1. **第一阶段**：短指令微调，每个指令最&#x591A;**`32,768`** tokens
>
> 2. **第二阶段**：结合短指令（最&#x591A;**`32,768`** tokens）和长指令（最&#x591A;**`262,144`** tokens）

在 **RL** 阶段，使用类似其他 **Qwen2.5** 模型的训练策略，只关注短指令。这种设计的主要原因有：

1. **RL** 训练对于长下文来说成本很高

2. 目前缺乏能为长下文任务提供合适奖励信号的 **RM**

3. 仅在短指令上进行 **RL** 仍然可以显著增强模型与人类在长下文任务上的偏好一致性

### 4.4.5 Qwen3

* **模型结构**

Qwen3 系列包含：

> * 6 个 **Dense** 模型：**`Qwen3 - 0.6B`**、**`Qwen3 - 1.7B`**、**`Qwen3 - 4B`**、**`Qwen3 - 8B`**、**`Qwen3 - 14B`**&#x548C; **`Qwen3 - 32B`**
>
> * 2 个 **MoE** 模型：**`Qwen3 - 30B - A3B`**&#x548C;**`Qwen3 - 235B - A22B`**，旗舰模型 Qwen3 - 235B - A22B 共有 2350 亿个参数，其中激活参数为 220 亿个

**Qwen3 Dense 模型**的架构与 Qwen2.5 类似，包括使用分组查询注意力机制 GQA、门控线性单元 SwiGLU、旋转位置嵌入 RoPE，以及采用预归一化的均方根归一化 RMSNorm。

![]()

![]()

除此之外，Qwen3 还去除了 Qwen2 中使用的 QKV 偏差，并在注意力机制中引入了 QK 归一化 QK-Norm，以确保 Qwen3 的稳定训练。模型架构的关键信息见上表。

**Qwen3 MoE 模型**与 Qwen3 密集模型具有相同的基本架构。模型架构信息如右表：

![]()

Qwen3 MoE 沿用了 Qwen2.5 - MoE 的设计，并实施了细粒度的专家划分。Qwen3 混合专家模型共有 128 个专家，每个 token 会激活 8 个专家。但是与 Qwen2.5 - MoE 不同的一点是，Qwen3 - MoE 的设计中没有共享专家。除此之外，Qwen Team 还采用了全局批处理负载均衡损失来促进专家的专业化，提升了模型在下游任务中的性能。

![]()

* **预训练**

Qwen3 的数据集相比 Qwen2.5 有了显著扩展。Qwen2.5是&#x5728;**`18T`**&#x4E2A; token 上进行预训练的，而 Qwen3 使用的数据量几乎翻倍，达到&#x4E86;**`36T`**&#x4E2A; token，为了构建这个数据集，Qwen Team 不仅从网络上收集数据，还从 PDF 文档中提取信息。并且使用 Qwen2.5-VL 从这些文档中提取文本，并用 Qwen2.5 改进提取内容的质量。为了增加数学和代码数据的数量，利用 Qwen2.5-Math 和 Qwen2.5-Coder 这两个数学和代码领域的专家模型合成数据，合成了包括教科书、问答对以及代码片段等多种形式的数据。

| 语系    | 语种&方言                                                                                                                                                                                                                                                                                                                                                                 |
| ----- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 印欧语系  | 英语、法语、葡萄牙语、德语、罗马尼亚语、瑞典语、丹麦语、保加利亚语、俄语、捷克语、希腊语、乌克兰语、西班牙语、荷兰语、斯洛伐克语、克罗地亚语、波兰语、立陶宛语、挪威语（博克马尔语）、挪威尼诺斯克语、波斯语、斯洛文尼亚语、古吉拉特语、拉脱维亚语、意大利语、奥克语、尼泊尔语、马拉地语、白俄罗斯语、塞尔维亚语、卢森堡语、威尼斯语、阿萨姆语、威尔士语、西里西亚语、阿斯图里亚语、恰蒂斯加尔语、阿瓦德语、迈蒂利语、博杰普尔语、信德语、爱尔兰语、法罗语、印地语、旁遮普语、孟加拉语、奥里雅语、塔吉克语、东意第绪语、伦巴第语、利古里亚语、西西里语、弗留利语、撒丁岛语、加利西亚语、加泰罗尼亚语、冰岛语、托斯克语、阿尔巴尼亚语、林堡语、罗马尼亚语、达里语、南非荷兰语、马其顿语僧伽罗语、乌尔都语、马加希语、波斯尼亚语、亚美尼亚语 |
| 汉藏语系  | 中文（简体中文、繁体中文、粤语）、缅甸语                                                                                                                                                                                                                                                                                                                                                  |
| 亚非语系  | 阿拉伯语（标准语、内志语、黎凡特语、埃及语、摩洛哥语、美索不达米亚语、塔伊兹-阿德尼语、突尼斯语）、希伯来语、马耳他语                                                                                                                                                                                                                                                                                                           |
| 南岛语系  | 印度尼西亚语、马来语、他加禄语、宿务语、爪哇语、巽他语、米南加保语、巴厘岛语、班加语、邦阿西楠语、伊洛科语、瓦雷语（菲律宾）                                                                                                                                                                                                                                                                                                        |
| 德拉威语  | 泰米尔语、泰卢固语、卡纳达语、马拉雅拉姆语                                                                                                                                                                                                                                                                                                                                                 |
| 突厥语系  | 土耳其语、北阿塞拜疆语、北乌兹别克语、哈萨克语、巴什基尔语、鞑靼语                                                                                                                                                                                                                                                                                                                                     |
| 壮侗语系  | 泰语、老挝语                                                                                                                                                                                                                                                                                                                                                                |
| 乌拉尔语系 | 芬兰语、爱沙尼亚语、匈牙利语                                                                                                                                                                                                                                                                                                                                                        |
| 南亚语系  | 越南语、高棉语                                                                                                                                                                                                                                                                                                                                                               |
| 其他    | 日语、韩语、格鲁吉亚语、巴斯克语、海地语、帕皮阿门托语、卡布维尔迪亚努语、托克皮辛语、斯瓦希里语                                                                                                                                                                                                                                                                                                                      |

Qwen3 通过三阶段流程进行预训练：

1. **通用阶段 General Stage, S1**：所有 Qwen3 模型均使&#x7528;**`4096`**&#x7684;序列长度，&#x5728;**`30T`** token 上进行训练。在此阶段，模型已完成语言能力和通用世界知识的全面预训练，训练数据覆&#x76D6;**`119`**&#x79CD;语言和方言。

2. **推理阶段 Reasoning Stage, S2**：为进一步提升推理能力，通过增加STEM、编码、推理和合成数据的比例来优化此阶段的预训练语料库。模型使&#x7528;**`4096`**&#x7684;序列长度，在&#x7EA6;**`5T`**&#x9AD8;质量token上进一步预训练，同时在此阶段加速学习率衰减。

3. **长上下文阶段 Long Context Stage**：作者团队收集高质量长上下文语料库以扩展 Qwen3 模型的上下文长度。所有模型使&#x7528;**`32,768`**&#x7684;序列长度，在数百亿 token 上进行预训练。长上下文语料库中，**`75%`**&#x7684;文本长度&#x5728;**`16,384`**&#x81F3;**`32,768`** token之间，**`25%`**&#x7684;文本长度&#x5728;**`4,096`**&#x81F3;**`16,384`** token之间。继续延续 Qwen2.5 的做法，使&#x7528;**`ABF`**&#x5C06; **RoPE** 的基础频率&#x4ECE;**`10,000`**&#x63D0;高&#x81F3;**`1,000,000`**。同时引&#x5165;**`YARN`**&#x548C;双块注意&#x529B;**`DCA`**，以在推理时将序列长度处理能力提升四倍。

> 与 Qwen2.5 类似，Qwen3 基于上述三个预训练阶段开发了用于预测最优超参数的缩放定律，研究了模型架构、训练数据、训练阶段与最优训练超参数之间的关系，为每个模型设定了预测的最优学习率和批量大小策略

由于模型架构的改进、训练数据的增加以及更有效的训练方法，Qwen3 Dense 基础模型的整体性能与参数更多的 Qwen2.5 基础模型相当。例如，Qwen3-1.7B/4B/8B/14B/32B-Base 分别与 Qwen2.5-3B/7B/14B/32B/72B-Base 表现相当。特别是在 STEM、编码和推理等领域，Qwen3 Dense 基础模型的表现甚至超过了更大规模的 Qwen2.5 模型。而 Qwen3 MoE 在仅使用 10% 激活参数的情况下达到了与 Qwen2.5 Dense 基础模型相似的性能。

* **后训练**

Qwen3 的后训练包含两个核心目标：

> 1. **思维控制 Thinking Control**：涉及两种不同模式的集成，即 Non-Thinking 模式和 Thinking 模式，user 能够灵活选择模型是否进行推理，并通过为 Thinking 过程指定 token 来控制 Thinking 深度。（后续又进行了拆分）
>
> 2. **强到弱蒸馏 Strong-to-Weak Distillation**：精简和优化轻量级模型的后训练流程，通过利用大规模模型的知识，大幅降低了构建较小规模模型所需的计算成本和开发工作量

> **混合推理模型**，这个概念最早&#x662F;**`Claude3.7`**&#x63D0;出来的，**`Gemini2.5 Flash`**&#x6700;新也支持了。其实就是一个模型既可以推理，也可以不推理。主要就是解决，在简单问题，或者是对实效性要求较高的情况下，可以通过控制，不生成思考过程，在不怎么影响效果的情况下，更快生成回复。
>
> 之前基本上是没有太好的办法直接让推理模型不生成思考过程，只能训练，提示词基本上控制不了。而这次 Qwen3 使用的控制办法是，硬切换设&#x7F6E;**`enable_thinking`**&#x4E3A;**`True`**&#x6216;&#x8005;**`False`**，当为 True 时，还可以二次软切换，通过文本后面&#x52A0;**`/no_think`**&#x6216;&#x8005;**`/think`**&#x6765;控制。
>
> 这里 Qwen Team 也给出了建议的参数配置，即：
>
> 1. **Thinking 模式**：**`Temperature=0.6, TopP=0.95, TopK=20, MinP=0`**
>
> 2. **Non-Thinking 模式**：**`Temperature=0.7, TopP=0.8, TopK=20, MinP=0`**

![]()

Qwen3 系列的旗舰模型包含四阶段训练流程。前两个阶段培养模型的思考能力，后两个阶段则将强大的非思考功能集成到模型中。

> 将教师模型的 logits 输出直接蒸馏到轻量级学生模型中，能够在保持对推理过程细粒度控制的同时有效提升其性能。这种方法无需为每个小规模模型单独执行详尽的四阶段训练流程，不仅能通过更高的Pass@1分数，体现出更优的即时性能，还能通过改善的长序列探索能力增强模型的推理扩展性。另一方面，可以以更高的训练效率实现了这些提升，仅需四阶段训练方法$$\frac{1}{10}$$的 GPU 时长。

**Stage 1：长思维链冷启动**

在这个阶段，Qwen Team 整理了一个涵盖广泛类别的综合数据集，包括数学、代码、逻辑推理和通用 STEM 问题。数据集中的每个问题都配有经过验证的参考解答或基于代码的测试用例。这个数据也是长思维链训练冷启动阶段的基础。

数据集构建涉及严格的两阶段过滤流程：Query 过滤和 Response 过滤。在 Query 过滤阶段，使用 Qwen2.5-72B-Instruct 模型识别并移除不易验证的 Query，包括包含多个子问题或请求通用文本生成的 Query。而且还排除了Qwen2.5-72B-Instruct 不使用思维链推理即可正确回答的 Query，这可以防止模型依赖表面猜测，确保数据集中仅包含需要深度推理的复杂问题。同时使用 Qwen2.5-72B-Instruct 为每个 Query 标注领域标签，以保持数据集中各领域的均衡分布。

在保留验证集后，使用 QwQ-32B 模型为每个剩余 Query 生成 N 个候选 Response。当 QwQ-32B 持续无法生成正确解决方案时，人工标注员会手动评估 Response 的准确性。对于通过率（Pass@N）为正的 Query ，使用更严格的过滤标准移除 Response。随后从数据集中挑选子集，用于推理模式的初始冷启动训练，目标是在模型中灌输基础推理模式，而不过度强调即时推理性能。这种方法确保模型的潜力不受限制，从而在后续强化学习阶段具备更大的灵活性和提升空间。为有效实现这一目标，在准备阶段最好尽量减少训练样本数量和训练步骤。

**Stage 2：推理强化学习**

推理强化学习阶段使用的 Query - Verifier 对必须满足四个标准：

`未在冷启动阶段使用过`、`冷启动模型可从中学习`、`尽可能具有挑战性`、`覆盖广泛的子领域`

最终收集了总&#x5171;**`3,995`**&#x4E2A; Query - Verifier 对，并采&#x7528;**`GRPO`**&#x66F4;新模型参数。

> 使用大批次大小、每个 Query 进行高次数的展开模拟，以及通过离策略训练提高样本效率，均对训练过程有益。
>
> 通过控制模型的熵值稳步增加或保持稳定，对于维持训练的稳定性至关重要。因此在单次强化学习运行过程中，无需对超参数进行任何人工干预，即可实现训练奖励和验证性能的持续提升。

**Stage 3：思维模式融合**

思维模式融合阶段的目标是将 Non-Thinking 能力集成到先前训练的 Thinking 模型中。这种方法使开发人员能够管理和控制推理行为，同时降低为 Thinking 和 Non-Thinking 任务部署独立模型的成本和复杂性。这一阶段对推理强化学习模型进行持续监督微调，并设计聊天模板融合两种模式：

![]()

SFT数据融合了 Thinking 和 Non-Thinking 两类数据。

> 为确保第二阶段模型的性能不会因额外的 SFT 而受损，Thinking 数据通过第二阶段模型自身对第一阶段 Query 进行拒绝采样生成
>
> Non-Thinking 覆盖编码、数学、指令遵循、多语言任务、创意写作、问答和角色扮演等多样化任务

此外采用自动生成的数据来评估 Non-Thinking 数据的 Response 质量。为了提升低资源语言任务的性能，特别增加了翻译任务的比例。

针对 Thinking 和 Non-Thinking 的样本，分别在用户 Query 或系统消息中引&#x5165;**`/think`**&#x548C;**`/no_think`** token。这使模型能够根据用户输入选择相应的思考模式：对于 Non-Thinking 模式样本，Response 中会保留空的思考块。这确保了模型内部格式的一致性，可通过在聊天模板中拼接空思考块来阻止模型执行推理行为。默认情况下模型处于 Thinking 模式，因此添加部分未包&#x542B;**`/think`&#x20;**&#x74;oken 的用户 Query 作为训练样本。对于更复杂的多轮对话，在用户 Query 中随机插入多&#x4E2A;**`/think`**&#x548C;**`/no_think`** token，模型 Response 遵循最后一个遇到的 token。

思考模式融合的另一个优势在于，一旦模型学会在 Thinking 和 Non-Thinking 两种模式下的 Response，它自然会形成处理中间情况的能力——基于不完整的思考过程生成 Response。

> 当模型的思考内容长度达到用户定义的阈值时，会手动终止思考过程，并插入停止思考指令：**`Considering the limited time by the user, I have to give the solution based on the thinking directly now.\n.\n\n`**。插入该指令后，模型会基于截至该时刻积累的推理内容继续生成最终 Response。这种能力并非通过显式训练获得，而是应用思考模式融合后自然涌现的结果。

**Stage 4：通用强化学习**

用来提升模型在多样化场景中的能力和稳定性。Qwen Team 建立了一个复杂的奖励系统，覆盖20多个不同的任务，每个任务都有定制的评分标准。这些任务专门针对这些核心能力提升：

> * **指令遵循**：确保模型准确理解并遵循用户指令，包括与内容、格式、长度及结构化输出使用相关的要求，从而生成符合用户期望的 Response
>
> * **格式遵循**：除了显式指令外，还期望模型遵循特定的格式规范。例如，模型应通过切换思考模式与非思考模式，&#x5BF9;**`/think`**&#x548C;**`/no_think`** token 做出恰当 Response，并在最终输出中始终使用指定 token，&#x5982;**`<thinking>`**&#x548C;**`</thinking>`**&#x6765;分隔思考内容与回答内容
>
> * **偏好对齐**：对于开放式 Query ，偏好对齐侧重于提升模型的实用性、吸引力和风格适配性，最终提供更自然且令人满意的用户体验
>
> * **Agent 能力**：通过指定接口正确调用工具。在强化学习过程中，允许模型与真实环境执行反馈进行完整的多轮交互循环，从而提升其在长程决策任务中的性能和稳定性
>
> * **专业场景能力**：在更专业的场景中，针对特定上下文设计任务。例如在 RAG 中引入奖励信号引导模型生成准确且符合上下文的 Response，从而最大限度降低幻觉风险

> Qwen3 在工具调用上做了专门训练，并且 Qwen-Agent 支持 MCP。
>
> ![]()
>
> ![]()

为上述任务提供反馈时，我们使用了三种不同类型的奖励：

**基于规则的奖励**

基于规则的奖励对指令遵循和格式遵守等通用任务很有用，能够高精度评估模型输出的正确性，避免奖励破解等问题

**带参考回答的基于模型的奖励**

为每个 Query 提供一个参考回答，并提示 Qwen2.5-72B-Instruct 模型根据此参考答案对模型的响应进行评分。这种方法可以更灵活地处理各种任务，不需要严格的格式，从而避免了纯粹基于规则的奖励可能出现的误报

**不带参考回答的基于模型的奖励**

利用人类偏好数据，训练一个奖励模型来为模型的响应分配标量分数。这种方法不依赖于参考答案，可以处理更广泛的 Query，同时有效增强模型的参与度和实用性

**强到弱的蒸馏**

强弱蒸馏流程是为优化轻量级模型设计的，包含5个 Dense 模&#x578B;**`Qwen3-0.6B`**、**`Qwen3-1.7B`**、**`Qwen3-4B`**、**`Qwen3-8B`**、**`Qwen3-14B`**&#x548C;一个 MoE 模&#x578B;**`Qwen3-30B-A3B`**。在提升模型性能的同时，赋予了模型稳健的模式切换能力。蒸馏过程主要分为两个阶段：

> 1. **Off-policy Distillation**：初始将教师模型通&#x8FC7;**`/think`**&#x548C;**`/no_think`**&#x4E24;种模式生成的输出结果结合起来进行 Response 蒸馏。这可以让轻量级学生模型培养基础推理能力和不同 Thinking 模式的切换能力，为下一阶段的 On-policy 策略训练奠定基础
>
> 2. **On-policy Distillation**：学生模型生成用于微调的 On-policy 策略序列。首先对 Prompt 进行采样，然后学生模型&#x4EE5;**`/think`**&#x548C;**`/no_think`**&#x6A21;式生成 Response 。随后，通过将学生模型的 logits 与教师模&#x578B;**`Qwen3-32B`**&#x6216;**`Qwen3-235B-A22B`**&#x5BF9;齐，以最小化KL散度对学生模型进行微调

### 4.4.6 QwQ

## 4.5 DeepSeek 系列

> **论文**：[**DeepSeek-V1**](https://arxiv.org/pdf/2401.02954)  **[DeepSeek-V2](https://arxiv.org/pdf/2405.04434)  [DeepSeek-V3](https://arxiv.org/pdf/2412.19437)  [DeepSeekMoE](https://arxiv.org/pdf/2401.06066)**
>
> **代码**：[**DeepSeek-V1**](https://github.com/deepseek-ai/DeepSeek-LLM)  **[DeepSeek-V2](https://github.com/deepseek-ai/DeepSeek-V2)  [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3)**

### 4.5.1 DeepSeek-V1

**DeepSeek&#x20;**&#x6DF1;入研究了规模定律，并提出了自己独特的发现，这些发现有助于在两种流行的开源配&#x7F6E;**`7B`**&#x548C;**`67B`**&#x4E2D;扩展大型模型，并从长远的角度推进了开源语言模型的发展。

为了支持预训练阶段，**DeepSeek&#x20;**&#x5F00;发了一个包&#x542B;**`2T`**&#x4E2A; token 的数据集，并且还在不断扩大。作者还对**DeepSeek LLM Base&#x20;**&#x6A21;型进行了 **SFT** 和直接偏好优化 **DPO**，从而创建了 **DeepSeek** **Chat** 模型。评估结果表明，**`DeepSeek LLM 67B`**&#x5728;各种基准测试中超过了 LLaMA-2 70B，特别是在代码、数学和推理领域。此外，与 GPT-3.5 相比，**`DeepSeek LLM 67B Chat`**&#x8868;现出更优越的性能。

* **模型结构**

**微观设计**：大体上遵循了 LLaMA 的设计，采用了带&#x6709;**`RMSNorm`**&#x51FD;数&#x7684;**`Pre-Norm`**&#x7ED3;构，并使&#x7528;**`SwiGLU`**&#x4F5C;为前馈网络 FFN 的激活函数，其中间层维度为$$\frac{8}{3}d_\text{model}$$。使用旋转嵌&#x5165;**`RoPE`**&#x7528;于位置编码。为了优化推理成本，**`67B`**&#x6A21;型使用了分组查询注意&#x529B;**`GQA`**，而不是传统的多头注意力 MHA。与大多数使用 GQA 的工作不同，**DeepSeek LLM** **67B&#x20;**&#x6A21;型的参数扩展在网络深度上，而不是通常做法中拓宽 FFN 层的中间宽度，旨在获得更好的性能。

**宏观设计**：**DeepSeek LLM&#x20;**&#x4E0E;其他不同：

**`DeepSeek LLM 7B`**&#x662F;**`30`**&#x5C42;的网络

**`DeepSeek LLM 67B`**&#x662F;**`95`**&#x5C42;的网络

![]()

这些层次调整不仅保持了与其他开源模型的参数一致性，也有助于模型流水线划分，以优化训练和推理过程。这种设计策略在保证计算效率的同时，也提高了模型的表现力和灵活性，使其在处理复杂任务时具有优势。

* **预训练**

**数据**

**DeepSeek&#x20;**&#x4E3B;要目标是全面增强数据集的丰富性和多样性。为了实现这些目标，**DeepSeek&#x20;**&#x5C06;方法分为三个基本阶段：**重复数据删除**、**过滤**和**重新混合**。重复数据删除和重新混合阶段通过对唯一实例进行采样来确保数据的多样化表示。过滤阶段提高了信息密度，从而实现了更高效、更有效的模型训练。

**去重阶段**：**DeepSeek&#x20;**&#x91C7;用了一种激进的去重策略，扩大了去重的范围。

![]()

**DeepSeek&#x20;**&#x53D1;现对整个 Common Crawl 数据集进行去重处理比仅在一部分数据内进行去重要去除更多的重复内容。右表展示了通过对 91 个不同批次的数据进行全面去重，所删除的重复文档数量是只对一个批次数据进行去重时的四倍。

**过滤阶段**：专注于制定稳健的文档质量评估标准。这结合语言和语义评估的详细分析，从个体和全局角度提供数据质量视图。

**混合阶段**：调整方法以解决数据不平衡问题，重点是增加代表性不足的领域的存在。这一调整旨在实现更加平衡和包容的数据集，确保充分代表不同的观点和信息。

在分词器中，**DeepSeek&#x20;**&#x4F7F;用了基于 Huggingface 团队的 tokenizers 库&#x7684;**`BBPE`**&#x7B97;法。预分词被用于防止来自不同字符类别的标记的合并，如新行、标点符号和中文-日语-韩语（**`CJK`**）符号，类似于 GPT-2。并且将数字拆分为单个数字。将词汇表中的传统标记数量设置&#x4E3A;**`100000`**。分词器在大&#x7EA6;**`24GB`**&#x7684;多语言语料库上进行训练，并且在最终词汇表中增加&#x4E86;**`15`**&#x4E2A;特殊标记，使总大小达&#x5230;**`100015`**。为了确保训练期间的计算效率并为将来可能需要的任何其他特殊标记预留空间，将模型词汇表的大小配置&#x4E3A;**`102400`**&#x7528;于训练。

**基建**

**DeepSeek&#x20;**&#x4F7F;用一个高效且轻量级的训练框架，名&#x4E3A;**`HAI-LLM`**（萤火平台，高效且轻量的大模型训练工具），来训练和评估大型语言模型。**数据并行**、**张量并行**、**序列并行**和**1F1B流水线并行**等并行策略都集成到了这个框架中。还利&#x7528;**`Flash Attention`**&#x6280;术来提高硬件利用率。**`ZeRO-1`**&#x88AB;用于分区状态的优化，以减少数据并行级别的通信开销。努力使计算和通信重叠，以最小化额外的等待开销，包括最后一个微批的反向传播过程和 ZeRO-1 中&#x7684;**`reduce-scatter`**&#x64CD;作，以及序列并行中&#x7684;**`GEMM`**&#x8BA1;算&#x548C;**`all-gather`**/**`reduce-scatter`**。

一些层操作被融合在一起以加速训练，包括 LayerNorm、GEMM 和 Adam 更新。为了提高模型训练的稳定性，使&#x7528;**`BF16`**&#x7CBE;度训练模型，但在计算梯度时使&#x7528;**`FP32`**&#x7CBE;度。还执行就地交叉熵以减少 GPU 内存消耗，即：在交叉熵 CUDA 内核中实时将 BF16 logits 转换为 FP32 精度，而不是在 HBM 中转换，计算相应的 BF16 梯度，并用其梯度覆盖 logits。

模型权重和优化器状态&#x6BCF;**`5`**&#x5206;钟异步保存一次，即使在偶尔出现硬件或网络故障的最坏情况下，也只会丢失不超过 5 分钟的训练进度。这些临时的模型检查点会定期清理，以避免占用过多的存储空间。还支持从不同的 **3D** 并行配置中恢复训练，以应对计算集群负载的动态变化。对于评估，在生成任务中采&#x7528;**`vLLM`**，在非生成任务中采用连续 batch 处理，以避免手动调整 batch 大小和减少 tokens 填充。

* **后训练**

收集了&#x7EA6;**`1.5M`**&#x6761;英语和中文指令数据实例，涵盖了广泛的帮助和无害主题。有用数据包&#x542B;**`1.2M`**&#x4E2A;实例，其中一般语言任务的分布&#x4E3A;**`31.2%`**，数学问题的分布&#x4E3A;**`46.6%`**，编程练习的分布&#x4E3A;**`22.2%`**。安全数据包&#x542B;**`300K`**&#x4E2A;实例，涵盖各种敏感主题。对齐流程包含两个阶段：

**监督微调**

对 **7B** 模型进行了 4 个 epoch 的微调，但对 **67B** 模型只进行了 2 个 epoch 的微调，因为 **67B** 模型存在严重的过拟合问题。**`GSM8K`**&#x548C;**`HumanEval`**&#x5728; **7B** 模型上得到了持续改进，而 **67B** 模型很快达到了上限。**7B&#x20;**&#x548C; **67B** 模型的学习率分别&#x4E3A;**`1e-5`**&#x548C;**`5e-6`**。除了监控基准准确度外，还评估了微调过程中聊天模型的重复率。总共收集&#x4E86;**`3868`**&#x4E2A;中文和英文提示，并确定了生成响应中无法终止并无限重复文本序列的比例。观察到重复率随着数学 **SFT** 数据的数量增加而上升，这可以归因于数学 **SFT** 数据偶尔包含类似的推理模式。因此，较弱的模型难以掌握这种推理模式，导致重复性响应。为了解决这个问题，尝试了两阶段微调和 **DPO**，这两种方法都可以保持基准分数并显著减少重复。

**直接偏好优化 DPO**

为了进一步提高模型的能力，**DeepSeek&#x20;**&#x4F7F;用了直接偏好优化算法，该算法被证明是一种简单而有效的 LLM 对齐方法。根据有用性和无害性构建了 **DPO** 训练所需的偏好数据。对于有用性数据，收集了多语言提示，涵盖了创意写作、问答、指令跟随等类别。然后，使用 **DeepSeek Chat** 模型作为候选响应生成响应。类似的操作也应用于无害性偏好数据构建。**DeepSeek** 进行了一个 DPO epoch 的训练，学习率&#x4E3A;**`5e-6`**，batch\_size &#x4E3A;**`512`**，使用了学习率预热和余弦学习率调度器。**DeepSeek** 发现 DPO 可以增强模型的开放式生成技能，同时在标准基准测试中的性能差异很小。

* **缩放定律**

缩放定律表明，随着计算预算$$C$$、模型规模$$N$$和数据规模$$D$$的增加，模型性能可以得到可预测的改善。当模型规模由模型参数表示，数据规模由 token 数表示时，计算预算$$C$$可以近似为$$C = 6ND$$。

为了降低实验成本和拟合难度，**DeepSeek** 采用&#x4E86;**`Chinchilla`**&#x4E2D;&#x7684;**`IsoFLOP profile`**&#x65B9;法来拟合缩放曲线。为了更准确地表示模型规模，使用一种新的模型规模表示方法，即非嵌入 FLOPs / token **`M`**，替换了先前使用的模型参&#x6570;**`N`**，并将近似计算预算公式$$C = 6ND$$替换为更精确的$$C = MD$$。实验结果提供了关于最佳模型/数据扩展分配策略和性能预测的见解，并准确地预测了 **DeepSeek LLM 7B&#x20;**&#x548C; **67B** 模型的预期性能。

**DeepSeek** 在缩放定律方面的贡献和发现可以总结如下：

> 1. 建立了超参数的缩放定律，为确定最佳超参数提供了一个经验框架
>
> 2. 采用非嵌入 FLOPs / token M 来表示模型规模，以及更精确的最佳模型/数据扩展分配策略和大规模模型的泛化损失的更好预测
>
> 3. 提出了一个基于计算预算和数据规模的模型扩展策略，该策略在计算预算有限的情况下提高了模型的泛化能力
>
> 4. 证明了缩放定律的普遍适用性，并预测了未来更大规模模型的性能
>
> 5. 持续关注数据质量和其对缩放定律的影响，并进行更深入的分析

**超参数的缩放定律**

**DeepSeek** 最初在计算预算&#x4E3A;**`1e17`**&#x7684;小规模实验上进行了 batch 大小和学习率的网格搜索，并使&#x7528;**`177M FLOPs/token`**&#x7684;特定模型大小，结果如右图(a)所示。从图中可以看出，泛化误差在 batch 大小和学习率的各种选择范围内保持稳定。这表明在相对较宽的参数空间内可以实现接近最优的性能。

![图1  批大小和学习率在1e17和1e20的FLOP上进行实验]()

然后 **DeepSeek** 利用上述多步学习率调度器，通过重用第一阶段，有效地训练了不同 batch 大小、学习率和计算预算&#x4ECE;**`1e17`**&#x5230;**`2e19`**&#x7684;多个模型。考虑到参数空间中的冗余，将泛化误差不超过最小&#x503C;**`0.25%`**&#x7684;模型参数视为近最优超参数。然后我们拟合batch大小$$B$$和学习率$$\eta$$与计算预算$$C$$之间的关系。拟合结果如下图所示，表明**最优 batch 大小$$B$$随着计算预算$$C$$的增加而逐渐增加，而最优学习率$$\eta$$则随着计算预算$$C$$的增加而逐渐减小**。这与直观的模型扩展时 batch 大小和学习率的经验设置相一致。此外，所有近最优超参数都落在一个宽的带宽范围内，表明在此区间内选择近最优参数相对容易。拟合的最终学习率和 batch 大小公式如下：

![图2  Batch size 和学习率的缩放曲线]()





$$\eta_\text{opt} = 0.3118\cdot C^{−0.1250}$$

$$B_\text{opt} = 0.2920\cdot C^{0.3271}$$

然后在具&#x6709;**`1e20`**&#x8BA1;算预算的一系列模型上验证了上述公式，&#x5728;**`2.94B FLOPs/token`**&#x6A21;型大小上实验的结果上图(b)所示，这表明拟合的参数集中在最优参数空间中，并且 **DeepSeek LLM 7B** 和 **67B** 模型拟合的参数同样取得了良好的性能。

**模型和数据缩放评估**

在先前的工作中，模型规模通常由模型参数表示，其中非嵌入参数为$$N_1$$，完整参数为$$N_2$$。计算预算$$C$$与模型/数据规模之间的关系可以近似描述为$$C=6ND$$，这意味着可以使用$$6N_1$$或$$6N_2$$来近似模型规模。然而，由于$$6N_1$$和$$6N_2$$都没有考虑到注意力操作的计算开销，并且$$6N_2$$还包括词汇计算，这对模型的容量贡献较小，因此在某些设置下，两者都有显著的近似误差。

为了减小这些误差，**DeepSeek&#x20;**&#x5F15;入了一种新的模型规模表示：非嵌入 **FLOPs / token** **`M`**。$$M$$包括注意力操作的计算开销，但不考虑词汇计算。使用$$M$$表示模型规模时，计算预算$$C$$可以简化为$$C=MD$$。$$6N_1, 6N_2$$和$$M$$之间的具体差异如下：

$$6N_1 = 72 n_{\text{layer}} d_{\text{model}}^2$$

$$6N_2 = 72 n_{\text{layer}} d_{\text{model}}^2 + 6 n_{\text{vocab}} d_{\text{model}}$$

$$M = 72 n_{\text{layer}} d_{\text{model}}^2 + 12 n_{\text{layer}} d_{\text{model}} l_{\text{seq}}$$

![]()

其中，$$n_{\text{layer}}$$表示层数，$$d_{\text{model}}$$表示模型宽度，$$n_{\text{vocab}}$$是词汇量大小，而$$l_{\text{seq}}$$是序列长度。&#x20;

在评估不同规模模型之间的差异时，**DeepSeek&#x20;**&#x53D1;现$$6N_1$$和$$6N_2$$要么高估要么低估了不同规模模型的计算成本。这种差异在小型模型中尤为明显，差异率高&#x8FBE;**`50%`**。这种不准确性在拟合缩放曲线时会导致较大的统计误差。模型规模表示间的差异以及$$6N_1$$和$$6N_2$$与$$M$$之间的差异如上表所示。

在采用$$M$$表示模型规模后，目标可以更清晰地描述为：**给定计算预算$$C=MD$$，找到最优模型规模$$M_\text{opt}$$和数据规模$$D_\text{opt}$$，以最小化模型的泛化误差**。这个目标可以形式化为：

$$M_{\text{opt}}(C), D_{\text{opt}}(C) = \underset{M, D \text{ s.t. } C = MD}{\arg\min} L(N, D)$$

为了降低实验成本和拟合难度，**DeepSeek** 采用 **IsoFLOP profile** 方法来拟合缩放曲线，并选择&#x4E86;**`8`**&#x4E2A;不同的计算预算，范围&#x4ECE;**`1e17`**&#x5230;**`3e20`**，并为每个预算设计了大&#x7EA6;**`10`**&#x79CD;不同的模型/数据规模分配。每个预算的超参数$$B$$和$$\eta$$由之前的讨论确定，泛化误差在类似的独立验证集上计算，该验证集的分布与训练集类似，包&#x542B;**`100M`**&#x4E2A; token。

右图展示了 IsoFLOP 曲线和模型 / 数据缩放曲线，这些曲线是通过使用每个计算预算的最优模型 / 数据分配来拟合的。最优非嵌入 FLOPs / token $$M_{\text{opt}}$$和最优令牌数 $$D_{\text{opt}}$$的具体公式如下：

$$M_{\text{opt}} = M_{\text{base}}C^a$$

$$D_{\text{opt}} = D_{\text{base}}C^b$$

![图3  IsoFLOP 曲线和最佳模型 / 数据分配]()

其中，$$M_{\text{base}} = 0.1715, \quad a = 0.5243, \quad D_{\text{base}} = 5.8316, \quad b = 0.4757 $$

**不同数据的缩放定律**

在 **DeepSeek LLM** 的开发过程中，数据集经过多次迭代优化，调整了不同数据源的比例，同时提高了整体质量。这进一步分析了不同数据集对缩放定律的影响。

这里使用三种不同的数据集研究了缩放定律：**早期的内部数据**、**当前的内部数据**和 **OpenWebText2**（先前用于缩放定律研究的文本数据集）。内部数据评估表明，当前内部数据的数据质量高于早期内部数据。此外，由于其规模较小，OpenWebText2 的数据质量甚至超过了当前内部数据，这使其能够进行更加细致的处理。

分析得出这三个数据集之间的最优模型/数据扩展分配策略与数据质量一致。如右表所示，随着数据质量的提高，模型扩展指数$$a$$逐渐增加，而数据扩展指数$$b$$逐渐减少，这表明**计算预算的增加应更多地分配给模型而不是数据**。这一发现解释了早期规模定律研究中观察到的最优模型/数据扩展分配的显著差异。

![]()

对于这一发现的一个直观猜测是，高质量的数据通常意味着逻辑清晰和经过充分训练后预测难度较低。因此，在增加计算预算时，扩展模型大小更有优势。

**总结**

**DeepSeek LLM** 是一系列基于2T token 的英中大数据集训练的开源模型。工作深入探讨了超参数的选择、扩展规律及微调尝试，并校准了扩展定律，提出了新的资源分配策略以优化模型/数据扩展。此外，还提出了一种方法来预测给定计算预算下的 Batch Size 和 Learning Rate，指出扩展定律与数据质量相关，影响不同研究工作的扩展行为。通过最佳超参数进行预训练并全面评估。

**DeepSeek Chat&#x20;**&#x5B58;在一些局限性，如知识更新滞后、可能生成非事实信息及幻觉倾向。特别是初版中文数据不详尽，导致在特定中文主题上的表现不佳，且对非中英文的语言支持较弱。未来，**DeepSeek** 计划发布关于代码和 **MoE** 的技术报告，并致力于创建更高质量的数据集以增强下一版本在推理、中文知识、数学和编码方面的能力。**DeepSeek** 对齐团队正探索如何提供更加有用、诚实和安全的模型，初步实验显示强化学习可提升复杂推理能力。

### 4.5.2 DeepSeek-V2

**Deepseek-V2** 采用了包括多头潜在注意力 **MLA** 和 **DeepSeekMoE** 在内的创新架构。**MLA** 通过将 KV 缓存大幅压缩成一个潜在向量来保证高效的推理过程，而 **DeepSeekMoE** 则采用大量的小参数专家进行建模，同时在训练和推理上加入了更多的优化。沿袭了一贯的作风，**DeepSeek** 对模型进行了完全的 MIT 协议开源，可以商用。对于算力不是那么充足的开发者，官方提供了 API 调用的方案，费用更是达到了全场最低。

**Deepseek-V2** 模型参数量方面达&#x5230;**`236B`**，每次处理 token 时激活其中&#x7684;**`21B`**&#x4E2A;，同时由于模型小专家混合的特性，模型在推理时的激活参数很少，可以实现高推理速度。在通用能力的表现上，模型&#x5728;**`MMLU`**&#x591A;选题 benchmark 上取得了第二名，**Deepseek-V2** 在众多开源模型中表现仅次于 70B 的 LLaMA-3，超过了此前发布的 **Deepseek-V1 67B** 的非 MoE 模型。在成本效率方面，相比 **V1** 的稠密模型，**V2** 模型节约&#x4E86;**`42.5%`**&#x7684;训练成本，减少了推理&#x65F6;**`93.3%`**&#x7684; KV 缓存占用，将生成的吞吐量也提升到了原来&#x7684;**`5.76`**&#x500D;。借&#x52A9;**`YaRN`**&#x4F18;化的长度外推训练方法，模型的上下文能力得以扩展到&#x4E86;**`128k`**&#x5927;小。

**Deepseek-V2** 在一个包&#x542B;**`8.1T`** token 的高质量多源语料库上进行了预训练，并进一步进行了监督微调 SFT 和强化学习 RL，以充分挖掘其潜力。评估结果显示，即使仅激活了 **21B** 个参数，**DeepSeek-V2** 及其 **Chat** 版本在开源模型中依然达到了顶级性能。

* **模型结构**

对于 **Deepseek-V2&#x20;**&#x7684;模型结构来说，主要有多头潜在注意力 **MLA** 和 **DeepSeekMoE** 两个方面，如右图，这在本书“**模型架构**”章节的“**注意力改进**”及“**FFN 改进**”部分有详细的介绍，这里不再赘述。

**MHA** 中 kv cache 会成为推理瓶颈。**MQA** 和 **GQA** 可以一定程度减少 kv cache，但效果上不如 **MHA**。**DeepSeek-V2** 设计了 **MLA**，通过低秩 key-value 联合压缩，实现了比 **MHA** 更好的效果并且需要的 kv cache 要小很多。

![]()

**DeepSeekMoE** 引入了两个主要策略：

> 1. **细粒度专家分割**：通过将每个 FFN 专家进一步细分，这允许模型在保持参数总数不变的情况下，激活更多的、更细粒度的专家。这种策略使得各个专家能够专注于更细致的知识领域，提高了专家的专业化程度。
>
> 2. **共享专家隔离**：设置一部分专家作为“共享专家”，这些专家总是被激活，用于捕捉和整合常见的跨上下文知识。这样可以减少路由专家之间的知识冗余，每个路由专家可以更专注于独特的知识领域。

这两个策略是对缓解传统 **MoE** 的缺陷进行的改进：

> 1. **知识杂糅**：传统的 **MoE** 模型中，每个专家往往需要处理多种类型的知识，这使得专家难以形成专门化的知识结构
>
> 2. **知识冗余**：不同的专家在处理不同的输入时可能需要相同的知识，导致多个专家中存在重复的知识，浪费了模型参数

**Deepseek-V2 中，**&#x54;ransformer 层的数量&#x4E3A;**`60`**，隐藏维度设置&#x4E3A;**`5120`**。所有可学习的参数均以标准差&#x4E3A;**`0.006`**&#x8FDB;行随机初始化。在 **MLA** 中，注意力头的数量$$n_h=128$$，每个头的维度$$d_h=128$$。KV 压缩维度$$d_c=512$$，Query 压缩维度$$d_c'=1536$$。对于解耦的 Query 和 Key，每个头的维度$$d^R_h=64$$。除第一层外，用 **MoE** 层替换了所有的 **FFN** 层。每个 **MoE&#x20;**&#x5C42;包&#x542B;**`2`**&#x4E2A;共享专家&#x548C;**`160`**&#x4E2A;路由专家，每个专家的中间隐藏维度&#x4E3A;**`1536`**。在路由专家中，每个 token 将激&#x6D3B;**`6`**&#x4E2A;专家。此外，低秩压缩和细粒度专家分割会影响一层的输出规模，因此在压缩的潜在向量后使用额外&#x7684;**`RMSNorm`**&#x5C42;，并在宽度瓶颈处（即压缩的潜在向量和路由专家的中间隐藏状态）乘以额外的比例因子，以确保训练的稳定性。

* **预训练**

**数据**

**Deepseek-V2&#x20;**&#x5728;保持与 **DeepSeek-V1 67B** 相同的数据处理阶段的同时，增加了数据量并提升了数据质量。为了扩大预训练语料库，**DeepSeek&#x20;**&#x63A2;索了互联网数据的潜力，并优化了清理流程，恢复了大量被误删的数据。此外还加入了更多的中文数据，以更好地利用中文互联网上的语料资源。除了增加数据量外，也注重数据质量，通过多种来源的高质量数据丰富预训练语料库，并改进基于质量的过滤算法，确保去除大量非有益数据，同时保留有价值的数据。同时过滤掉了有争议的内容，以减少特定区域文化带来的数据偏差。

**Deepseek-V2&#x20;**&#x91C7;用了与 **DeepSeek-V1 67B** 相同的基于字节级字节对编&#x7801;**`BBPE`**&#x7B97;法构建的分词器，其词汇量&#x4E3A;**`100K`**。经过分词处理的预训练语料库包&#x542B;**`8.1T`**&#x4E2A;标记，其中中文标记比英文标记多大&#x7EA6;**`12%`**。

**超参数**

**Deepseek-V2&#x20;**&#x4F7F;&#x7528;**`AdamW`**&#x4F18;化器进行训练，其超参数设置为$$\beta_1 = 0.9$$，$$\beta_2 = 0.95$$，权重衰减设置&#x4E3A;**`0.1`**。学习率采用预热和阶梯衰减策略进行调度。初始阶段，在&#x524D;**`2K`**&#x6B65;中，学习率线性增加从0到最大值。随后，在训练大&#x7EA6;**`60%`**&#x7684;标记后，学习率乘&#x4EE5;**`0.316`**；在训练大&#x7EA6;**`90%`**&#x7684;标记后再乘&#x4EE5;**`0.316`**。最大学习率设置为$$2.4 \times 10^{-4}$$，梯度裁剪范数设置&#x4E3A;**`1.0`**。同时使用了批量大小调度策略，在&#x524D;**`225B`**&#x4E2A; token 的训练过程中，批量大小逐渐&#x4ECE;**`2304`**&#x589E;加&#x5230;**`9216`**，之后保持在 9216 不变。我们将最大序列长度设置&#x4E3A;**`4K`**，并&#x5728;**`8.1T`**&#x4E2A;标记上训练 **DeepSeek-V2**。

为了部署模型的不同层，采用流水线并行策略，将不同层分布在不同的设备上，对于每一层，路由专家均匀分布&#x5728;**`8`**&#x4E2A;设备上，即$$D = 8$$。至于设备限制的路由，每个标记最多发送&#x5230;**`3`**&#x4E2A;设备，即$$M = 3$$。对于损失函数的平衡，设置$$\alpha_1=0.003$$，$$\alpha_2=0.05$$，$$\alpha_3=0.02$$。在训练期间，采用了丢弃 token 的策略以加速训练，但在评估时不丢弃任何 token。

**基建平台**

**DeepSeek-V2&#x20;**&#x57FA;&#x4E8E;**`HAI-LLM`**&#x6846;架进行训练，这是 **DeepSeek&#x20;**&#x5185;部开发的高效轻量级训练框架。它采用&#x4E86;**`16`**&#x8DEF;零气泡流水线并行、**`8`**&#x8DEF;专家并行&#x548C;**`ZeRO-1`**&#x6570;据并行。鉴于 **DeepSeek-V2** 激活的参数相对较少，并且部分操作符通过重新计算以节省激活内存，因此可以在不需要张量并行的情况下进行训练，从而减少了通信开销。此外，为了进一步提高训练效率，将共享专家的计算与专家并行的全对全通信重叠。并且为不同专家之间的通信、路由算法和融合线性计算定制了更快的 CUDA 内核。此外，**MLA&#x20;**&#x4E5F;基于改进版的 **FlashAttention-2&#x20;**&#x8FDB;行了优化。所有的实验都在配备 **NVIDIA H800 GPU** 的集群上进行。

**长上下文**

在 **DeepSeek-V2** 的初步预训练之后，使&#x7528;**`YaRN`**&#x5C06;默认的上下文窗口长度&#x4ECE;**`4K`**&#x6269;展&#x5230;**`128K`**。**YaRN** 特别应用于解耦的共享键$$k^R_t$$，因为它负责携带 **RoPE**。对于 **YaRN**，尺度$$s=40$$，$$\alpha=1$$，$$\beta=32$$，目标最大上下文长度设置&#x4E3A;**`160K`**。在这种设置下，可以预期模型&#x5728;**`128K`**&#x7684;上下文长度下表现良好。由于注意力机制与原&#x59CB;**`YaRN`**&#x6709;所不同，为了调节注意力熵，对长度缩放因子进行了调整。计算因子$$\sqrt{t} = 0.0707 \ln s + 1$$，旨在最小化困惑度。

此外，在序列长度&#x4E3A;**`32K`**&#x548C;批量大小&#x4E3A;**`576`**&#x4E2A;序列的情况下，对模型额外进行&#x4E86;**`1000`**&#x6B65;的训练。尽管训练仅在 32K 的序列长度上进行，但当在 128K 的上下文长度下评估时，模型仍然表现出强大的性能。实验结果表明，**DeepSeek-V2** 在所有上下文窗口长度上的表现都非常好。这表明即使在较长的上下文长度下，**DeepSeek-V2&#x20;**&#x4E5F;能保持良好的性能和稳定性。

* **后训练**

**监督微调 SFT**

**DeepSeek&#x20;**&#x6574;理了指令调优数据集，包&#x542B;**`1.5M`**&#x4E2A;实例，其&#x4E2D;**`1.2M`**&#x4E2A;实例用于提升帮助性，**`0.3M`**&#x4E2A;实例用于提升安全性。与初始版本相比，提高了数据质量，以减少幻觉响应并增强写作能力。使&#x7528;**`2`**&#x4E2A; epoch 对 **DeepSeek-V2&#x20;**&#x8FDB;行微调，学习率设置为$$5 \times 10^{-6}$$。

在评估 **DeepSeek-V2 Chat (SFT)** 时，主要采用了基于生成的基准测试，除了几个具有代表性的选择题任&#x52A1;**`MMLU`**&#x548C;**`ARC`**，还使用指令跟随评&#x4F30;**`IFEval`**&#x5BF9; **DeepSeek-V2 Chat (SFT)** 进行了评估，采用提示级别的宽松准确率作为评估指标。此外还使&#x7528;**`LiveCodeBench`**&#x7684;问题来评估聊天模型。除了标准基准测试，**DeepSeek&#x20;**&#x8FD8;在开放式对话基准上进一步评估了模型，包&#x62EC;**`MT-Bench`**、**`AlpacaEval 2.0`**&#x548C;**`AlignBench`**。为了进行比较，在相同的评估框架和设置下评估了 Qwen1.5 72B Chat、LLaMA-3-70B Instruct 和 Mistral-8x22B Instruct。**DeepSeek 67B Chat** 则直接引用先前发布的评估结果。这样全面的评估方法确保了 **DeepSeek-V2** 在多种任务上的表现得到充分验证。

**强化学习**

为了节省RL的训练成本，**DeepSeek-V2&#x20;**&#x91C7;用了组相对策略优化 **GRPO**（**G**roup **R**elative **P**olicy **O**ptimization），这种方法放弃了通常与策略模型大小相同的评价模型，而是通过组评分来估计基线。具体来说，对于每个问题$$q$$，**GRPO** 从旧策略$$\pi_{\theta_{old}}$$中采样一组输出$$\{o_1, o_2, \cdots, o_G\}$$，然后通过最大化以下目标函数来优化策略模型$$\pi_{\theta}$$：

$$\mathcal{J}_{GRPO}(\theta) = \mathbb{E}[q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{old}}(O|q)]$$

$$\frac{1}{G} \sum_{i=1}^G \left( \min \left( \frac{\pi_\theta(o_i|q)}{\pi_{\theta_{old}}(o_i|q)} A_i, \text{clip} \left( \frac{\pi_\theta(o_i|q)}{\pi_{\theta_{old}}(o_i|q)}, 1-\varepsilon, 1+\varepsilon \right) A_i \right) - \beta \mathbb{D}_{KL}(\pi_\theta || \pi_{ref}) \right)$$

$$\mathbb{D}_{KL}(\pi_\theta || \pi_{ref}) = \frac{\pi_{ref}(o_i|q)}{\pi_\theta(o_i|q)} - \log \frac{\pi_{ref}(o_i|q)}{\pi_\theta(o_i|q)} - 1$$

其中，$$\varepsilon$$和$$\beta$$是超参数；$$A_i$$是优势值，使用每个组内输出对应的奖励组 $$\{r_1, r_2, \ldots, r_G\}$$ 进行计算：

$$A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \cdots, r_G\})}{\text{std}(\{r_1, r_2, \cdots, r_G\})}$$

**GRPO** 和 **PPO** 的区别如右图所示：

![]()

在初步实验中，**DeepSeek&#x20;**&#x53D1;现针对推理数据，如代码和数学提示，进行的强化学习训练表现出与一般数据训练不同的独特特征。例如在数学和编程能力方面可以在较长的训练步骤中持续改进。因此**采用了一个两阶段的强化学习训练策略，首先进行推理对齐，然后进行人类偏好对齐**。在第一阶段的推理对齐中，**DeepSeek&#x20;**&#x8BAD;练了一个专门用于代码和数学推理任务的奖励模型$$\text{RM}_\text{reasoning}$$，并使用该奖励模型的反馈来优化策略模型$$\pi_\theta$$：

$$r_i = \text{RM}_\text{reasoning}(o_i)$$

在第二阶段的人类偏好对齐中，采用了一个多奖励框架，该框架从一个有用性奖励模型$$\text{RM}_\text{helpful}$$、一个安全性奖励模型$$\text{RM}_\text{safety}$$和一个基于规则的奖励模型$$\text{RM}_\text{rule}$$中获取奖励。具体来说，每个奖励模型分别评估响应的不同方面：$$\text{RM}_\text{helpful}$$评估响应的帮助性和相关性；$$\text{RM}_\text{safety}$$评估响应的安全性和适当性；$$\text{RM}_\text{rule}$$基于预定义的规则集评估响应的合规性。最终响应$$o_i$$的奖励是通过结合这些不同来源的奖励来确定的：

$$r_i = c_1\cdot \text{RM}_\text{helpful}(o_i) + c_2\cdot \text{RM}_\text{safety}(o_i) + c_3\cdot \text{RM}_\text{rule}(o_i)$$

其中，$$c_1$$、$$c_2$$和$$c_3$$是相应的系数。

为了获得在强化学习训练中起关键作用的可靠奖励模型，**DeepSeek&#x20;**&#x4ED4;细收集偏好数据，并精心进行质量过滤和比例调整。这里基于编译器反馈获取代码偏好数据，并基于真实标签获取数学偏好数据。对于奖励模型的训练，使用 **DeepSeek-V2 Chat (SFT)** 初始化奖励模型，并使用 **point-wise** 或 **pair-wise** 损失进行训练。强化学习训练能够充分挖掘和激活模型的潜力，使其能够在可能的响应中选择正确且满意的答案。

在极大规模的模型上进行强化学习训练对训练框架提出了很高的要求。它需要精心的工程优化来管理 GPU 内存和 RAM 的压力，同时保持快速的训练速度。为了实现这一目标，**DeepSeek&#x20;**&#x5B9E;施了以下工程优化：

> 1. 提出了一种混合引擎，分别为训练和推理采用不同的并行策略，以实现更高的 GPU 利用率
>
> 2. 利&#x7528;**`vLLM`**&#x4E0E;大批量大小作为我们的推理后端，以加速推理速度
>
> 3) 仔细设计了一种将模型卸载到 CPU 以及从 CPU 加载回 GPU 的调度策略，从而在训练速度和内存消耗之间实现了近乎最优的平衡

**总结**

**DeepSeek-V2** 是一个支持 128K 上下文长度的大规模 MoE 语言模型。得益于其创新的架构，包括 MLA 和 DeepSeekMoE，该模型不仅性能强大，而且训练经济、推理高效。与 **DeepSeek 67B** 相比，**DeepSeek-V2** 显著提升了性能，节省了 42.5% 的训练成本，减少了 93.3% 的 KV 缓存，并将最大生成吞吐量提高了 5.76 倍。评估结果显示，仅激活 21B 参数的 DeepSeek-V2 在开源模型中表现优异，成为最强的开源 MoE 模型。

尽管如此，**DeepSeek-V2** 及其聊天版本仍存在常见 LLM 的局限性，如预训练后缺乏持续的知识更新、可能生成不准确信息以及可能出现幻觉等。此外，由于数据主要由中文和英文内容组成，该模型在其他语言上的表现可能有限。

### 4.5.3 DeepSeek-V2.5

**DeepSeek&#x20;**&#x5C06; **DeepSeek-V2-Chat** 和 **DeepSeek-Coder-V2** 两个模型的合并，形成 **DeepSeek-V2.5**。**DeepSeek-V2.5** 不仅保留了原有 **Chat** 模型的通用对话能力和 **Coder** 模型的强大代码处理能力，还更好地对齐了人类偏好。此外 **DeepSeek-V2.5** 在写作任务、指令跟随等多个方面也实现了大幅提升。

**DeepSeek** 在 6 月份对 **DeepSeek-V2-Chat** 进行了重大升级，用 **DeepSeek-Coder-V2** 的 **Base** 模型替换原有 **DeepSeek-V2** 的 **Base** 模型，显著提升了其代码生成和推理能力，并发布了 **DeepSeek-V2-Chat-0628** 版本。紧接着，**DeepSeek-Coder-V2** 在原有 **Base** 模型的基础上，通过对齐优化，大大提升通用能力后推出了 **DeepSeek-Coder-V2-0724** 版本。最终将 **Chat** 和 **Coder** 两个模型合并，推出 **DeepSeek-V2.5**。

![]()

**DeepSeek-V2.5&#x20;**&#x7684;能力得到了很大的提升：

1. **通用能力**：使用业界通用的测试集对 **DeepSeek-V2.5** 进行测评，在中文和英文四个测试集上，**DeepSeek-V2.5** 均优于之前的 **DeepSeek-V2-0628** 以及 **DeepSeek-Coder-V2-0724**。在 **DeepSeek&#x20;**&#x5185;部的中文评测中，和 **GPT-4o mini**、**ChatGPT-4o-latest** 的对战胜率相较于 **DeepSeek-V2-0628** 均有明显提升，其中裁判为 GPT-4o。此测评中涵盖创作、问答等通用能力等。

2. **安全能力**：在 **DeepSeek-V2.5** 中对模型安全问题的边界做了更加清晰的划分，在强化模型对于各种越狱攻击的安全性的同时，减少了安全策略过度泛化到正常问题中去的倾向。

3. **代码能力**：**DeepSeek-V2.5** 保留了 **DeepSeek-Coder-V2-0724** 的代码能力，&#x5728;**`HumanEval Python`**&#x548C;**`LiveCodeBench`**&#x6D4B;试中显示了较为显著的改进。&#x5728;**`HumanEval Multilingual`**&#x548C;**`Aider`**&#x6D4B;试中 **DeepSeek-Coder-V2-0724** 略胜一筹。&#x5728;**`SWE-verified`**&#x6D4B;试中，两个版本的表现都较低，表明在此方面仍需进一步优化。&#x5728;**`FIM`**&#x8865;全任务上，内部评测&#x96C6;**`DS-FIM-Eval`**&#x7684;评分提升&#x4E86;**`5.1%`**。在内部的主观评&#x6D4B;**`DS-Arena-Code`**&#x4E2D;，**DeepSeek-V2.5** 对战竞品的胜率取得了显著提升，裁判仍为 GPT-4o。

### 4.5.4 DeepSeek-V3

**DeepSeek-V3** 也是 **MoE** 语言模型，总共&#x6709;**`671B`**&#x53C2;数，每次处理标记时激&#x6D3B;**`37B`**&#x53C2;数。为了实现高效的推理和经济的训练，**DeepSeek-V3** 采用了在 **DeepSeek-V2** 中使用过的 **MLA** 和 **DeepSeekMoE** 架构。此外，**DeepSeek-V3** 首创了**无辅助损失的负载均衡策略**，并设定了**多 Token 预测训练目标**以增强性能。

**DeepSeek-V3** &#x5728;**`14.8T`**&#x4E2A;多样化且高质量的 token 上进行预训练，随后通过监督微调和强化学习阶段来充分发挥其能力。实验显示 **DeepSeek-V3** 超越了其他开源模型，并达到了与领先的闭源模型相当的性能，而且 **DeepSeek-V3** 的完整训练仅&#x9700;**`2.788M H800 GPU`**&#x5C0F;时。

* **模型结构**

**DeepSeek-V3** 采用 **MLA** 实现高效推理，并用 **DeepSeekMoE** 实现经济的训练。与 **DeepSeek-V2** 相比，额外引入了无辅助损失的负载均衡机制。除此之外还使用了多 Token 预测 **MTP**（**M**ulti-**T**oken **P**rediction）训练目标，这可以提升在评估基准上的整体性能。这里依次介绍这两方面。

**无辅助损失的负载均衡策略**

**MoE 基础架构**：对于前馈网络 FFN，**DeepSeek-V3** 采用了 **DeepSeekMoE** 架构。与传统 MoE 架构相比，**DeepSeekMoE** 使用更细粒度的专家并将部分专家隔离为共享专家。设$$u_t$$表示第$$t$$个 token 的 FFN 输入，按如下方式计算 FFN 的输出$$h_t'$$ ：

$$h_t' = u_t + \sum_{i=1}^{N_s} \text{FFN}^{(s)}_i (u_t) + \sum_{i=1}^{N_r} g_{i,t} \text{FFN}^{(r)}_i (u_t)$$

$$g_{i,t} = \frac{g_{i,t}'}{\sum_{j=1}^{N_r} g_{j,t}'}$$

$$g_{i,t}' = \begin{cases} s_{i,t}, & s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \leq j \leq N_r\}, K_r) \\ 0, & \text{otherwise} \end{cases}$$

$$s_{i,t} = \text{Sigmoid}(u_t^\top e_i)$$

其中$$N_s$$和$$N_r$$分别表示共享专家和路由专家的数量，$$\text{FFN}_i^{(s)}(\cdot)$$和$$\text{FFN}_i^{(r)}(\cdot)$$分别表示第$$i$$个共享专家和第$$i$$个路由专家，$$K_r$$表示激活的路由专家数量，$$g_{i,t}$$是第$$i$$个专家的门控值；$$s_{i,t}$$是 token 到专家的亲和度，$$e_i$$是第$$i$$个路由专家的质心向量，而$$\text{Topk}(\cdot, K)$$表示在所有路由专家中为第$$t$$个 token 计算的亲和度分数中最高的$$K$$个分数组成的集合。与 **DeepSeek-V2** 不同的是 **DeepSeek-V3** 使用 Sigmoid 函数计算亲和度分数，并对所有选定的亲和度分数进行归一化以生成门控值。

**无辅助损失的负载均衡策略**：对于 MoE 模型来说，专家负载不平衡会导致路由崩溃并降低专家并行场景下的计算效率。传统解决方案通常依靠辅助损失来避免负载不平衡。然而，过大的辅助损失会损害模型性能。

为了在负载均衡和模型性能之间取得更好的平衡，**DeepSeek-V3&#x20;**&#x63D0;出了一种无辅助损失的负载均衡策略。具体来说，为每个专家引入一个偏置项$$b_i$$，并将其添加到相应的亲和度分数$$s_{i,t}$$中，以确定$$\text{Topk}$$路由：

$$g_{i,t}' = \begin{cases} s_{i,t}, & s_{i,t} + b_i \in \text{Topk}(\{s_{j,t} + b_j | 1 \leq j \leq N_r\}, K_r) \\ 0, & \text{otherwise} \end{cases}$$

这里偏置项仅用于路由，与 FFN 输出相乘的门控值仍然由原始亲和度分数$$s_{i,t}$$得出。在训练过程中持续监控每个训练步骤中整个批次的专家负载。在每个步骤结束时，如果相应的专家负载过重，将偏置项减少$$\gamma$$；如果相应的专家负载不足，则增加$$\gamma$$，其中$$\gamma$$是一个称为**偏置更新速度**的超参数。通过这种动态调整，**DeepSeek-V3** 在训练过程中保持专家负载均衡，并实现了比纯粹依靠辅助损失来鼓励负载均衡的模型更好的性能。

**互补序列级辅助损失策略**：尽管 **DeepSeek-V3** 主要依靠无辅助损失策略来实现负载均衡，但为了防止任何单个序列内出现极端不平衡，还采用了互补序列级平衡损失：

$$\mathcal{L}_{Bal} = \alpha \sum_{i=1}^{N_r} f_i P_i$$

$$f_i = \frac{N_r}{K_r T} \sum_{t=1}^{T} \mathbb{1}(s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \leq j \leq N_r\}, K_r))$$

$$s_{i,t}' = \frac{s_{i,t}}{\sum_{j=1}^{N_r} s_{j,t}}$$

$$P_i = \frac{1}{T} \sum_{t=1}^{T} s_{i,t}'$$

其中平衡因子$$\alpha$$是一个超参数，会被赋予一个极小的值，$$1(\cdot)$$表示指示函数，$$T$$表示序列中的 token 数量。序列级平衡损失的目的是确保每个序列上的专家负载保持平衡。

**节点限制路由**：与 **DeepSeek-V2** 采用的设备限制路由类似，**DeepSeek-V3** 同样采用受限制的路由机制来控制训练过程中的通信开销：确保每个 token 最多只会被发送&#x5230;**`M`**&#x4E2A;节点，这些节点是根据每个节点上分布的专家的最高$$\frac{K_r}{M}$$个亲和度分数总和来选择的。在此约束条件下，MoE 训练框架能够实现近乎完整的计算与通信重叠。

**无 Token 丢弃**：基于高效的负载均衡策略，**DeepSeek-V3** 在整个训练过程中都能保持良好的负载平衡状态，因此 **DeepSeek-V3** 在训练过程中不会丢弃任何 token。而且还实施了专门的部署策略来确保推理时的负载均衡，使得 **DeepSeek-V3** 在推理阶段同样不会发生 token 丢弃。

**多 Token 预测 MTP**

**DeepSeek-V3** 探索并设置了多 Token 预测目标，将预测范围扩展到每个位置的多个未来 token。这种方法具有双重优势：一方面，**MTP** 目标能够使训练信号更加密集，有望提升数据使用效率；另一方面，**MTP** 使模型能够预先规划其表示，从而更好地预测未来 token。

![]()

**MTP 模块**：上图展示了 **MTP** 实现方案，这里采用顺序预测额外 token 的方式，并在每个预测深度保持完整的因果链。具体而言，**MTP** 使用$$D$$个顺序模块来预测$$D$$个额外的 token。第$$k$$个 **MTP** 模块由以下组件构成：一个与主模型共享的嵌入层$$\text{Emb}(\cdot)$$、一个共享的输出头$$\text{OutHead}(\cdot)$$、一个 Transformer 块$$\text{TRM}_k(\cdot)$$以及一个投影矩阵$$M_k \in \mathbb{R}^{d \times 2d}$$。在第$$k$$个预测深度处理第$$i$$个输入 token $$t_i$$ 时，首先将两个表示向量结合起来：第$$k-1$$深度的第$$i$$个 token 表示$$h_{k-1}^i \in \mathbb{R}^d$$和第$$i+k$$个 token 的嵌入表示$$\text{Emb}(t_{i+k}) \in \mathbb{R}^d$$。这种结合通过如下线性投影实现：

$$h_i^{'k} = M_k[\text{RMSNorm}(h^{k-1}_i);\text{RMSNorm}(\text{Emb}(t_{i+k}))]$$

其中$$[:]$$表示向量连接操作。当$$k = 1$$时，$$h_{k-1}^i$$是主模型输出的表示。每个 **MTP** 模块的嵌入层都与主模型共享。将组合得到的 $$h_i^{'k}$$ 输入到第$$k$$深度的 Transformer 块中，从而生成当前深度的输出表示$$h^k_i$$：

$$h_{1:T-k}^k = \text{TRM}_k(h_{1:T-k}^{'k})$$

这里的$$T$$代表输入序列的长度，而$$i:j$$表示切片操作（包含左右边界）。最后，系统将 $$h^k_i$$ 输入到共享输出头中，计算第$$k$$个额外预测 token 的概率分布$$p_{i+1+k}^k \in \mathbb{R}^V$$（其中 $$V$$ 表示词汇表大小）：

$$p_{i+1+k}^{k} = \text{OutHead}(h^k_i)$$

输出头$$\text{OutHead}(\cdot)$$首先将表示向量线性映射为 logits，随后通过$$\text{Softmax}(\cdot)$$函数计算第$$k$$个额外 token 的预测概率，这里每个 **MTP** 模块的输出头也与主模型共享。

**MTP 训练目标**：在每个预测深度，计算如下交叉熵损失$$\mathcal{L}^k_{\text{MTP}}$$：

$$\mathcal{L}^k_{\text{MTP}} = \text{CrossEntropy}(p_{2+k:T+1}^k, t_{2+k:T+1}) = -\frac{1}{T} \sum_{i=2+k}^{T+1} \log p_i^k[t_i]$$

其中，$$T$$代表输入序列的长度，$$t_i$$表示第$$i$$个位置的真实 token，而$$p_i^k[t_i]$$则表示第$$k$$个 **MTP** 模块对$$t_i$$的预测概率。最终通过计算所有深度 **MTP** 损失的平均值，并与权重因子$$\lambda$$相乘，得到总体 **MTP** 损失$$\mathcal{L}_{\text{MTP}}$$，这作为 **DeepSeek-V3** 的补充训练目标：

$$\mathcal{L}_{\text{MTP}} = \frac{\lambda}{D} \sum_{k=1}^D \mathcal{L}^k_{\text{MTP}}$$

**MTP 的推理**：设计 **MTP** 策略的主要目的是提升主模型的性能，因此在实际推理阶段可以直接移除 **MTP** 模块，让主模型独立运行。而且还可以将这些 **MTP** 模块重新用于推测解码，从而进一步降低生成延迟。

* **基建**

**DeepSeek-V3** 是在一个配&#x5907;**`2048`**&#x4E2A;**`NVIDIA H800 GPU`**&#x7684;集群上训练的。H800 集群中的每个节点包含 8 个通过 NVLink 和节点内的 NVSwitch 连接的 GPU。在不同节点之间，使用 **IB**（**I**nfini**B**and）互连来促进通信。

**训练框架**

**DeepSeek-V3** 的训练由 HAI-LLM 框架支持，这是 **DeepSeek&#x20;**&#x7684;工程师从头开始精心打造的高效轻量级训练框架。总体而言，**DeepSeek-V3** 应用了 16 路**流水线并行** **PP**、跨越 8 个节点的 64 路**专家并行** **EP** 以及 **ZeRO-1** **数据并行** **DP**。

为了促进 DeepSeek-V3 的高效训练，**DeepSeek&#x20;**&#x5B9E;施了细致的工程优化：

1. **设计了 DualPipe 算法来实现高效的流水线并行**

![]()

**DeepSeek-V3** 的跨节点专家并行因通信开销导致计算与通信效率比率低至 1:1。为此，提出了一种名为 DualPipe 的流水线并行算法，旨在通过重叠前向和后向计算-通信阶段来提升训练效率，并减少流水线中的气泡。DualPipe 创新性地在每个数据块中划分出**注意力机制**、**all-to-all 分发**、**MLP** 和 **all-to-all 组合**四个部分，其中后向数据块的注意力机制和 MLP 还进一步细分为输入和权重的反向传播。引入 PP 通信组件，并通过重新排列这些组件及手动调整 GPU SM 分配给计算与通信的比例，实现了 all-to-all 和 PP 通信的有效隐藏，如上图所示。采用双向流水线调度策略，从两端馈送微批次，以完全重叠大部分通信，确保随着模型规模扩大仍能保持恒定的计算与通信比率，实现接近零的 all-to-all 通信开销，一个 DualPipe 调度如下图所示，包&#x542B;**`8`**&#x4E2A; PP 等级&#x548C;**`20`**&#x4E2A;微批次，分为两个方向。

![]()

即使在通信负担较轻的场景下，DualPipe 相较于 ZB1P 和 1F1B 等方法也展示了显著的效率优势，如右表所示，大幅减少了流水线气泡，同时仅轻微增加了峰值激活内存。

![]()

尽管需要维护两个副本的模型参数，但由于使用较大的 EP 大小进行训练，这并不会显著增加内存消耗。此外，与 Chimera 方法相比，DualPipe 只要求流水线阶段和微批次数量可被 2 整除，且随微批次数量增加时不会额外增加气泡或激活内存。

* **开发了高效的跨节点 all-to-all 通信内核**

为确保 DualPipe 的计算性能，定制了高效的跨节点 all-to-all 通信内核（包括分发和组合），以减少分配给通信的 SM 数量。这些内核与 MoE 门控算法及集群网络拓扑共同设计。具体来说，集群中的跨节点 GPU 通过 IB 完全互联，而节点内部通信则通过 NVLink 处理，后者提&#x4F9B;**`160 GB/s`**&#x7684;带宽，约为IB（50 GB/s）&#x7684;**`3.2`**&#x500D;。为了有效利用 IB 和 NVLink 的不同带宽，每个 token 最多被分发到4个节点，从而减少 IB 流量。当一个 token 的路由决策确定后，首先通过 IB 传输到目标节点上具有相同节点内索引的 GPU，到达目标节点后，立即通过 NVLink 转发至特定 GPU，避免被后续到达的令牌阻塞。这样，IB 和 NVLink 的通信完全重叠，每个 token 能高效选择每节点平均 3.2 个专家，不增加 NVLink 额外开销。这意味着虽然 **DeepSeek-V3** 实际上仅选择 8 个路由专家，但可以扩展到最多13个专家（4节点×3.2专家/节点），同时保持相同的通信成本。总体而言，采用这种通信策略，仅需 20 个 SM 即可充分利用 IB 和 NVLink 的带宽。

详细来说，通过使&#x7528;**`warp specialization`**&#x6280;术，将20个SM分为10个通信通道。在分发过程中，(1) IB发送、(2) IB 到 NVLink 转发、(3) NVLink 接收分别由相应的 warp 处理，根据实际工作负载动态调整分配给每个通信任务的 warp 数量。类似地，在组合过程中，(1) NVLink 发送、(2) NVLink 到 IB 转发和累加、(3) IB 接收和累加也由动态调整的 warp 处理。此外，分发和组合内核与计算流重叠，因此还需考虑它们对其他 SM 计算内核的影响。具体措施包括使用定制的 PTX 指令并自动调优通信块大小，这显著减少了 L2 缓存的使用及对其他 SM 的干扰。这种方法不仅提高了通信效率，还优化了整体系统性能。

* **精心优化了训练期间的内存占用**

**RMSNorm 和 MLA 上投影的重计算**。 在反向传播期间重新计算所有 RMSNorm 操作和 MLA 上投影，从而无需持久存储它们的输出激活。通过少量开销，此策略显著减少了存储激活所需的内存。

**CPU 中的指数移动平均**。 在训练期间保留模型参数的指数移动平均 (EMA)，以便在学习率衰减后尽早估计模型性能。EMA 参数存储在 CPU 内存中，并在每个训练步骤后异步更新。此方法允许我们维护 EMA 参数，而不会产生额外的内存或时间开销。

**多 Token 预测的共享嵌入和输出头**。 通过 DualPipe 策略，将模型的最浅层（包括嵌入层）和最深层（包括输出头）部署在相同的 PP 秩上。这种安排使得 MTP 模块和主模型之间能够物理共享参数和梯度，包括共享的嵌入和输出头。这种物理共享机制进一步提高了我们的内存效率。

**FP8 训练**

基于低精度训练的进展，**DeepSeek&#x20;**&#x63D0;出了一种利&#x7528;**`FP8`**&#x6570;据格式的细粒度混合精度框架来训练 **DeepSeek-V3**。尽管低精度训练前景广阔，但其应用常受限于激活值、权重和梯度中的异常值。尽管推理量化已取得显著进展，但在大规模语言模型预训练中成功应用低精度技术的例子较少。

为应对这一挑战并扩展 **FP8** 格式的动态范围，采用了一种细粒度量化策略：使用$$1×N_c$$元素的 tile-wise 分组或$$N_c×N_c$$元素的 block-wise 分组。高精度累积过程中，反量化开销显著减少，这对实现精确的 **FP8** 通用矩阵乘法 GEMM 至关重要。此外，为了降低 MoE 训练中的内存和通信开销，激活值以 **FP8** 格式缓存和分派，而优化器状态则&#x4EE5;**`BF16`**&#x683C;式存储。该混合精度框架在与 **DeepSeek-V2-Lite&#x20;**&#x548C; **DeepSeek-V2** 相似规模的模型上进行了验证，训练了大&#x7EA6;**`1T`**&#x4E2A;Token。结果显示，相比 **BF16** 基线，**FP8** 训练模型的相对损失误差保持&#x5728;**`0.25%`**&#x4EE5;下，这在训练随机性的可接受范围内。

1. **混合精度框架**

基于低精度训练的广泛应用技术，**DeepSeek&#x20;**&#x63D0;出了一种用于 **FP8** 训练的混合精度框架。在此框架中，大多数计算密集型操作在 **FP8** 精度下执行，而少数关键操作则保留其原始数据格式，以平衡训练效率和数值稳定性。

![]()

整体框架如右图所示，大部分核心计算内核以 **FP8** 精度实现，这些操作接收 **FP8** 张量作为输入，并产生 **BF16** 或 **FP32** 格式的输出。具体来说，与线性算子相关的三个 **GEMM** 操作——前向传&#x64AD;**`Fprop`**、激活反向传&#x64AD;**`Dgrad`**&#x548C;权重反向传&#x64AD;**`Wgrad`**——都在 **FP8** 精度下执行。这种设计理论上使计算速度相比原始 **BF16** 方法提高一倍。此外，**FP8 Wgrad GEMM** 允许激活值以FP8格式存储，从而显著减少内存消耗。

尽管 **FP8** 格式具有效率优势，但某些算子由于对低精度计算的敏感性，仍需要更高的精度支持。因此，经过分析，以下组件保持了原始精度，如 **BF16** 或 **FP32**：嵌入模块、输出头、MoE 门控模块、归一化算子和注意力算子。这些高精度组件确保了 **DeepSeek-V3** 的稳定训练动态。为进一步保证数值稳定性，主权重、权重梯度和优化器状态均以更高精度存储。这种方法不仅利用了 **FP8** 的效率优势，同时也通过有针对性地保留高精度部分，确保了模型训练的稳定性和准确性。

* **量化和乘法带来精度提升**

基于混合精度FP8框架，**DeepSeek&#x20;**&#x63D0;出了几种策略以提高低精度训练的准确性，重点在于量化方法和乘法过程：

**细粒度量化**：在低精度训练中，由于 **FP8** 格式动态范围有限，溢出和下溢是常见挑战。标准做法是将输入张量的最大绝对值缩放到 **FP8&#x20;**&#x7684;最大可表示值。然而，这种方法对激活异常值敏感，可能降低量化精度。为此，引入了细粒度量化方法：激活值&#x6309;**`1x128`** tile 分组，即每个 token 128个通道，权重则&#x6309;**`128x128`**&#x5757;分组，即128个输入通道和128个输出通道，如右图。

![]()

这种细化的缩放策略确保量化过程能更好地适应异常值。此外在 **GEMM** 操作的内部维度引入每组缩放因子，结合高精度 **FP32** 累积策略，提高了计算效率。

**提高累加精度**：低精度 GEMM 操作常遇到下溢问题，依赖于高精度累加。然而，在 NVIDIA H800 GPU 上，FP8 GEMM 的累加精度限制在&#x7EA6;**`14`**&#x4F4D;，低于FP32精度。为解决此问题，**DeepSeek&#x20;**&#x91C7;用了提升到 CUDA Cores 进行更高精度累加的策略：Tensor Cores 执行 MMA 时，中间结果用有限位宽累加，达到间&#x9694;**`K`**&#x540E;，部分结果复制到CUDA Cores上的 FP32 寄存器进行全精度累加。该策略通过高效地在 CUDA Cores 上相乘缩放因子实现去量化，同时保持高利用率。

**尾数优于指数**：不同于先前工作采用混合FP8格式（Fprop 使&#x7528;**`E4M3`**，Dgrad 和 Wgrad 使&#x7528;**`E5M2`**），所有张量均采&#x7528;**`E4M3`**&#x683C;式以获得更高精度。细粒度量化策略使得在较小元素组间共享指数位，减轻了有限动态范围的影响。

**在线量化**：为了简化框架并确保尺度准确，针对每&#x4E2A;**`128x128`**&#x7684;激活图块或权重块在线计算最大绝对值，并推导出缩放因子。基于此最大绝对值，在线将激活或权重量化为 **FP8** 格式，避免了延迟量化技术带来的复杂性。

* **低精度存储与通信**

结合 FP8 训练框架，通过将缓存的激活值和优化器状态压缩为低精度格式，进一步降低了内存消耗和通信开销：

**低精度优化器状态**：采用BF16数据格式来跟踪AdamW优化器中的一阶和二阶矩，未观察到明显的性能下降。然而，主权重和梯度仍保留在FP32中，以确保数值稳定性。

**低精度激活**：Wgrad操作在FP8中执行，激活值以FP8格式缓存用于线性算子的反向传播，以减少内存消耗。特别地：

> 1. **注意力算子之后线性算子的输入**：这些激活值对精度敏感，因此采用了定制&#x7684;**`E5M6`**&#x6570;据格式，并在反向传播中&#x4ECE;**`128x128`**&#x91CF;化块转换&#x4E3A;**`128x1`**&#x91CF;化块。所有缩放因子均进行舍入缩放，即 2 的整数次幂，以避免额外的量化误差。
>
> 2. **MoE 中 SwiGLU 算子的输入**：为了进一步降低内存成本，缓存 SwiGLU 算子的输入并在反向传播中重新计算其输出。这些激活值使用细粒度量化方法以 FP8 格式存储，在内存效率和计算精度之间取得平衡。

**低精度通信**：通信带宽是 MoE 模型训练的关键瓶颈。为此，在 MoE 投影层之前将激活值量化为 FP8，然后应用分发组件，与 MoE 投影层中的 FP8 前向传播兼容。类似地，激活值的缩放因子设置为 2 的整数幂。对于 MoE 下投影层之前的激活梯度，采用相同策略。前向和反向组合组件则保留在 BF16 中，以在训练管道的关键部分保持训练精度。

**推理和部署**

在 H800 集群上部署 **DeepSeek-V3**，每个节点内的 GPU 通过 NVLink 互联，整个集群的所有 GPU 则通过 IB 完全互联。为了确保在线服务的服务级别目标 SLO 和高吞吐量，采用了以下部署策略，将预填充阶段和解码阶段分离：

1. **预填充**

预填充阶段的最小部署单元由4个节点组成，每个节点配备32个GPU。注意力部分采用4路张量并行（TP4）与序列并行（SP）结合，并结合8路数据并行（DP8）。MoE部分使用32路专家并行（EP32），确保每个专家处理足够大的批次大小以提高计算效率。MoE的all-to-all通信首先通过IB在节点之间传输Token，然后通过NVLink在节点内的GPU之间转发。浅层中使用1路张量并行以节省TP通信开销。为了实现负载均衡，引入冗余专家策略，复制高负载专家并定期调整其部署（例如每10分钟一次）。每个GPU除了托管原始8个专家外，还托管一个额外的冗余专家。为提高吞吐量，同时处理两个具有相似计算负载的微批次，重叠注意力机制和MoE计算与分发和组合操作。此外，正在探索一种动态冗余策略，其中每个GPU托管更多专家（如16个），但在每个推理步骤中仅激活9个专家。

* **解码**

解码过程中，共享专家被视为一个被路由的专家，每个Token选择9个专家，其中共享专家总是被选中。解码阶段的最小部署单元由4日讯道组成，共计320个GPU。注意力部分采用TP4与SP结合，并结合DP80，而MoE部分使用EP320。每个GPU仅托管一个专家，64个GPU负责托管冗余专家和共享专家。所有到所有通信通过IB上的直接点对点传输执行，利用IBGDA技术进一步减少延迟并提高通信效率。根据在线服务的统计专家负载，定期确定冗余专家集合，但无需重新排列专家。正在探索用于解码的动态冗余策略，需优化全局最优路由方案算法及与调度内核的融合以减少开销。

* **预训练**

**数据**

**优化预训练语料库**：与 **DeepSeek-V2** 相比，**DeepSeek-V3&#x20;**&#x901A;过增加数学和编程样本的比例优化了预训练语料库，并扩展多语言覆盖范围至英语和中文之外。改进的数据处理流程减少了冗余，同时保持语料库的多样性。采用文档打包方法确保数据完整性，但在训练期间不使用跨样本注意力掩码。**DeepSeek-V3** 的训练语料库包&#x542B;**`14.8T`**&#x9AD8;质量和多样化的token。

**中间填充策略**：在 **DeepSeekCoder-V2&#x20;**&#x8BAD;练过程中观察到，中间填充 **FIM**（**F**ill-**I**n-**M**iddle）策略不会损害下一个 token 的预测能力，反而使模型能够根据上下文线索准确预测中间文本。因此，在 **DeepSeek-V3** 的预训练中也采用了 **FIM&#x20;**&#x7B56;略。具体实现采用前缀-后缀-中间 **PSM**（**P**refix-**S**uffix-**M**iddle）框架：

**`<|fim_begin|>`**$$f_\text{pre}$$**`<|fim_hole|>`**$$f_\text{suf}$$**`<|fim_end|>`**$$f_\text{middle}$$**`<|eos_token|>`**

此结构作为文档级别预打包过程的一部分应用，FIM策略的应用率&#x4E3A;**`0.1`**。

**Tokenizer 改进**：**DeepSeek-V3** 的 tokenizer 采&#x7528;**`BBPE`**，词汇量扩展&#x81F3;**`128K`**&#x4E2A; token。新的预分词器经过修改，优化了多语言压缩效率，并引入了结合标点符号和换行符的 token。然而，这种技术可能导致没有结尾换行符的多行提示词出现 token 边界偏差，特别是在少样本学习评估提示词中。为解决这一问题，在训练期间随机拆分一定比例的此类组合 token，使模型接触到更广泛的特殊情况，减轻了偏差。

**超参数**

**模型超参数**：**DeepSeek-V3** 的 Transformer 层数量设置&#x4E3A;**`61`**，隐藏层维度&#x4E3A;**`7168`**。所有可学习参数&#x4EE5;**`0.006`**&#x7684;标准差随机初始化。在 MLA 中，注意力头数设置&#x4E3A;**`128`**，每个头的维度&#x4E3A;**`128`**。KV 压缩维度&#x4E3A;**`512`**，查询压缩维度&#x4E3A;**`1536`**。对于解耦的 Query 和 Key，每个头的维度设置&#x4E3A;**`64`**。除前三层外，所有 FFN 层被 MoE 层替换，每个MoE层包&#x542B;**`1`**&#x4E2A;共享专家&#x548C;**`256`**&#x4E2A;路由专家，每个专家的中间隐藏层维度&#x4E3A;**`2048`**。每个 token 激&#x6D3B;**`8`**&#x4E2A;专家，并确保每个 token 最多发送&#x5230;**`4`**&#x4E2A;节点。**MTP&#x20;**&#x6DF1;度设置&#x4E3A;**`1`**。**DeepSeek-V3** 采用额外的 RMSNorm 层并乘以缩放因子。模型总参数量&#x4E3A;**`671B`**，其中每个 token 激&#x6D3B;**`37B`**&#x53C2;数。

**训练超参数**：使&#x7528;**`AdamW`**&#x4F18;化器，参数设置为$$β_1=0.9，β_2=0.95，\text{weight\_decay}=0.1$$。预训练的最大序列长度&#x4E3A;**`4K`**，&#x5728;**`14.8T`**&#x4E2A; token 上进行预训练。学习率调度策略如下：&#x524D;**`2K`**&#x6B65;线性增加至$$2.2×10^{-4}$$，然后保持该值直到消&#x8017;**`10T`**&#x4E2A; token。随后，&#x5728;**`4.3T`**&#x4E2A; token 内逐渐衰减至$$2.2×10^{-5}$$，遵循余弦衰减曲线。最&#x540E;**`500B`**&#x4E2A; token 中，&#x524D;**`333B`**&#x4FDD;持$$2.2×10^{-5}$$的学习率，剩&#x4F59;**`167B`**&#x5207;换至$$7.3×10^{-6}$$。梯度裁剪范数设&#x4E3A;**`1.0`**。批量大小从训练初期&#x7684;**`3072`**&#x9010;步增加&#x5230;**`15360`**，并在剩余训练过程中保持不变。模型的不同层通过流水线并行部署在不同的 GPU 上，每层的路由专家均匀分布&#x5728;**`64`**&#x4E2A; GPU 上，属&#x4E8E;**`8`**&#x4E2A;节点。每个 token 最多发送&#x5230;**`4`**&#x4E2A;节点。无辅助损失的负载平衡偏差更新速度设置为&#x524D;**`14.3T`**&#x4E2A; token 为`0.001`，其&#x4F59;**`500B`**&#x4E3A;**`0`**。平衡损失权重设&#x4E3A;**`0.0001`**，**MTP** 损失权重&#x524D;**`10T`**&#x4E3A;**`0.3`**，剩&#x4F59;**`4.8T`**&#x4E3A;**`0.1`**。

**上下文扩展**

**DeepSeek-V3** 通过与 **DeepSeek-V2** 相似的方法获得了长上下文处理能力。预训练后，应&#x7528;**`YaRN`**&#x65B9;法进行上下文扩展，并执行两个额外的训练阶段，每个阶段包&#x542B;**`1000`**&#x6B65;，逐步将上下文窗口&#x4ECE;**`4K`**&#x6269;展&#x5230;**`32K`**，再扩展&#x5230;**`128K`**。**YaRN** 配置与 **DeepSeek-V2** 一致，仅应用于解耦的共享键 k。训练阶段超参数：

> **第一阶段**：序列长度&#x4E3A;**`32K`**，批量大小&#x4E3A;**`1920`**
>
> **第二阶段**：序列长度增加&#x5230;**`128K`**，批量大小减少&#x5230;**`480`**

两个阶段的超参数保持相同：$$s=40，α=1，β=32$$，缩放因子$$\sqrt{t}=0.1\ln s + 1$$。学习率均设置为$$7.3×10^{-6}$$，与预训练阶段的最终学习率匹配。经过这两个阶段的扩展训练，**DeepSeek-V3&#x20;**&#x80FD;够处理长度高达 **128K&#x20;**&#x7684;输入，同时保持强大的性能。在大海捞针测试中，**DeepSeek-V3&#x20;**&#x5C55;示了显著的性能提升，表明其在长达 **128K** 的上下文窗口长度上具有一致的鲁棒性。

* **后训练**

**监督微调 SFT**

**DeepSeek&#x20;**&#x7CBE;心策划了指令微调数据集，其中包含跨多个领域&#x7684;**`1.5M`**&#x4E2A;实例，每个领域都采用针对其特定需求量身定制的独特数据创建方法。

> 1. **推理数据**：对于与推理相关的数据集，包括那些专注于数学、代码竞赛问题和逻辑谜题的数据集，通过利用内部的 **DeepSeek-R1** 模型来生成数据。虽然 **R1** 生成的数据表现出很高的准确性，但它存在诸如过度思考、格式不佳和长度过长等问题。目标是平衡 **R1** 生成的推理数据的高准确性以及格式规范的推理数据的清晰度和简洁性。
>
> 2. **非推理数据**：例如创意写作、角色扮演和简单问答，利用 **DeepSeek-V2.5** 生成回复，并聘请人工标注员来验证数据的准确性和正确性。

**DeepSeek-V3** 使用 **SFT** 数据集对 **DeepSeek-V3-Base** 进行两个 epoch 的微调，采用余弦衰减学习率调度，初始学习率为$$5 \times 10^{-6}$$，并逐渐降低至$$1 \times 10^{-6}$$。在训练过程中，每个单独的序列由多个样本打包而成。**DeepSeek&#x20;**&#x91C7;用样本掩码策略，以确保这些示例保持隔离且相互不可见。

**奖励模型 RM**

在强化学习过程中，采用了基于规则的奖励模型和基于模型的奖励模型：

1. **基于规则的奖励模型**

对于可以使用特定规则验证的问题，采用基于规则的奖励系统来确定反馈。例如，某些数学问题具有确定的结果，则要求模型在指定的格式内提供最终答案，从而允许应用规则来验证其正确性。同样，对于 LeetCode 问题，利用编译器根据测试用例生成反馈。通过尽可能利用基于规则的验证，确保了更高水平的可靠性，因为这种方法不易受到操纵或利用。

* **基于模型的奖励模型**

对于具有自由形式的真实答案的问题，依赖奖励模型来确定响应是否与预期的真实答案匹配。相反，对于没有明确的真实答案的问题，例如涉及创意写作的问题，奖励模型则负责根据问题和相应的答案提供反馈。奖励模型以问题和答案作为输入，并从 **DeepSeek-V3 SFT** 检查点进行训练。为了提高其可靠性，**DeepSeek&#x20;**&#x6784;建了偏好数据，这些数据不仅提供最终奖励，还包括导致奖励的思维链。这种方法有助于降低特定任务中奖励被黑客攻击的风险。

**群组相对策略优化 GRPO**

与 **DeepSeek-V2&#x20;**&#x7C7B;似，采用了群组相对策略优化 **GRPO**，它放弃了通常与策略模型大小相同的评论家模型，而是从群组得分中估计基线。具体来说，对于每个问题$$q$$，**GRPO** 从旧的策略模型$$\pi_{\text{old}}$$中采样一组输出$$\{o_1, o_2, \cdots, o_G\}$$，然后通过最大化以下目标来优化策略模型$$\pi_b$$：

$$\mathcal{J}_\text{GRPO}(\theta)=\mathbb{E}\left[q \sim P(Q),\left\{o_i\right\}_{i=1}^G \sim \pi_{\theta_\text{old}}(O \mid q)\right]$$

$$ \\ \frac{1}{G} \sum_{i=1}^G\left(\min \left(\frac{\pi_\theta\left(o_i \mid q\right)}{\pi_{\theta_\text{old}}\left(o_i \mid q\right)} A_i, \operatorname{clip}\left(\frac{\pi_\theta\left(o_i \mid q\right)}{\pi_{\theta_\text{old}}\left(o_i \mid q\right)}, 1-\varepsilon, 1+\varepsilon\right) A_i\right)-\beta \mathbb{D}_\text{KL}\left(\pi_\theta \| \pi_\text{ref}\right)\right)$$

$$\mathbb{D}_\text{KL}\left(\pi_\theta \| \pi_\text{ref}\right)=\frac{\pi_\text{ref}\left(o_i \mid q\right)}{\pi_\theta\left(o_i \mid q\right)}-\log \frac{\pi_\text{ref}\left(o_i \mid q\right)}{\pi_\theta\left(o_i \mid q\right)}-1$$

其中$$\epsilon$$和$$\beta$$是超参数，$$\pi_{\text{ref}}$$是参考模型，$$A_i$$是优势，它来自每个组内输出对应的奖励$$\{r_1, r_2, \cdots, r_G\}$$：

$$A_i=\frac{r_i-\operatorname{mean}\left(\left\{r_1, r_2, \cdots, r_G\right\}\right)}{\operatorname{std}\left(\left\{r_1, r_2, \cdots, r_G\right\}\right)}$$

在强化学习过程中，**DeepSeek-V3&#x20;**&#x878D;入了来自不同领域的提示词，例如编码、数学、写作、角色扮演和问答。这种方法不仅使模型更贴近人类偏好，而且还提高了在基准测试中的性能，尤其是在可用的监督微调数据有限的情况下。

### 4.5.5 DeepSeek-R1

## 4.6 Moonshot 研究

### 4.6.1 Mooncake

### 4.6.2 Kimi k1.5

### 4.6.3 Kimi K2

# 5. 训练优化

## 5.1 模型压缩

### 5.1.1 量化

* 线性量化 vs. 非线性量化

* 训练后量化 vs. 量化感知训练

* 经典算法：QAT, PTQ, DoReFa-Net

### 5.1.2 剪枝

* 结构化剪枝 vs. 非结构化剪枝

* 基于幅值的剪枝 vs. 基于重要性的剪枝

* 经典算法：Magnitude Pruning, Lottery Ticket Hypothesis

### 5.1.3 蒸馏

* 离线蒸馏 vs. 在线蒸馏

* 基于logits的蒸馏 vs. 基于特征的蒸馏

* 经典算法：KD, FitNets, Attention Transfer

## 5.2 高效模型架构

### 5.2.1 高效注意力机制

> 论文：[**Reformer**](https://arxiv.org/pdf/2001.04451)  **[Longformer](https://arxiv.org/pdf/2004.05150)  [Linformer](https://arxiv.org/pdf/2006.04768)  [Big Bird](https://arxiv.org/pdf/2007.14062)  [Performer](https://arxiv.org/pdf/2009.14794)**

* **Longformer**



* **Big Bird**



* **Reformer**



* **Linformer**



* **Performer**



### 5.2.2 混合专家模型 MoE

> 论文：[**Switch Transformer**](https://arxiv.org/pdf/2101.03961)  **[SoftMoE](https://arxiv.org/pdf/2308.00951)  [LoRAMoE](https://arxiv.org/pdf/2312.09979)  [DeepSeekMoE](https://arxiv.org/pdf/2401.06066)**

* **Switch Transformer**





* **SoftMoE**





* **LoRAMoE**





* **DeepSeekMoE**



## 5.3 优化算法改进

### 5.3.1 高级优化技术

### 5.3.2 学习率调度

## 5.4 并行训练

> 论文：**[ZeRO](https://arxiv.org/pdf/1910.02054)  [Megatron-LM](https://arxiv.org/pdf/1909.08053)  [GPipe](https://arxiv.org/pdf/1811.06965)**

这里主要讲 3 种并行策略：数据并行 **DP**（**D**ata **P**arallelism）、张量并行 **TP**（**T**ensor **P**arallelism）和流水线并行 **PP**（**P**ipeline **P**arallelism）。各种并行策略示意图如下所示：

![          数据并行]()

![           张量并行]()

![           流水线并行]()

### 5.4.1 **数据并行**

在近年来的深度学习模型训练中，使用更多的训练数据和更大的模型趋势未改。更大的模型和数据量意味着更多的计算量和存储需求，也意味着更久的训练时间。那么如何将计算和存储需求分布到多个训练设备来提升训练速度，是关键问题。
数据并行 DP 是解决上述问题的的一种并行策略，其主要逻辑遵&#x5FAA;**`Single Program Multiple Data`**&#x7684;原则，即在数据并行的模型训练中，训练任务被切分到多个设备上，每个进程维护相同的模型参数和相同的计算任务，但是处理不同的数据。通过这种方式，同一全局数据下的数据和计算被切分到了不同的进程，从而减轻了单个设备上的计算和存储压力。在深度学习模型训练中，数据并行可作为通过增加并行训练设备来提高训练吞吐量的方法。

> **注**：**Single Program Multiple Data&#x20;**&#x5E76;行模式中的要求：
>
> 1. **`Single Program`**：在深度学习训练中每个进程上模型的组网和参数相同
>
> 2. **`Multiple Data`**：在深度学习训练中为每个进程上模型处理不同的数据

**例**：以常见&#x7684;**`ResNet50`**&#x6A21;型使&#x7528;**`32GB V100`**&#x5361;训练为例。假设训练时单卡最大能支持的 local batch size 为256，训练一个 step 的耗时&#x4E3A;**`1`**&#x79D2;，则单卡训练时的吞吐&#x4E3A;**`256 images/s`**。如果我们使&#x7528;**`32`**&#x5F20;**`V100`**&#x505A;数据并行训练，假设没有损耗，那么理论上的训练吞吐可达&#x5230;**`32 x 256 = 8192 images/s`**。实际上由于数据并行时多机多卡的通信消耗等，实际加速效率会有折扣，但在加速效率&#x4E3A;**`0.8`**&#x65F6;，训练吞吐也可达&#x5230;**`32 x 256 x 0.8 = 6554 images/s`**。如果使用更多的 GPU，并行训练的速度将会更高，大大减少训练需要的时间。
深度学习训练中数据并行的实现方式可以有多种，这里介绍目前主流深度学习训练框架中数据并行的实现方式：基&#x4E8E;**`Distributed Synchronous SGD`**&#x7684;梯度同步数据并行。

数据并行分为了两种模式：**DP**（**D**ata **P**arallel）和 **DDP**（**D**istributed **D**ata **P**arallel） 。

**Data Parallel**

DP是一种单进程多线程的并行策略，只能在单机上进行训练，从卡做 Forward 和 Backward 并行，主卡做梯度聚合和优化器更新，具体步骤如下：

> 1. 单进程控制多GPU，即本质上是单进程多线程
>
> 2. 首先将模型加载到主 GPU 上，再复制到各个指定从 GPU；
>
> 3. 将输入数据按照 Batch 维度进行拆分，各个 GPU 独立进行 forward 计算；
>
> 4. 将结果同步给主 GPU 完成梯度计算和参数更新，将更新后的权重参数复制到各个 GPU

**存在的问题：** 由于其是单进程控制多个GPU，故会存在GPU之间负载不均衡的问题，主GPU负载较大。

**Distributed Data Parallel**

**DDP** 采&#x7528;**`AllReduce`**&#x67B6;构，多进程的方式，突破锁的束缚。在单机和多机上都可以使用。负载分散在每个 GPU 节点上，通信成本（时间）是恒定的，与 GPU 数量无关，等于参数量$$V$$除以带宽$$B$$。**DDP** 不需要通过主 GPU 分发全模型的参数到每个 GPU 上。使&#x7528;**`ring-all-reduce`**&#x7684;方式进行通讯，随着 GPU 数量$$N$$增加，总传输量恒定。也就是理论上，随着 GPU 数量的增加，ring all-reduce有线性加速能力。

![]()

**学习率设置**

数据并行模式下学习率的设置基本原则是学习率正比于全局 **batch size。** 与单卡训练相比，数据并行训练通常有两种配置：

* 一种是保持保持所有计算设备的 batch size 的总和与单卡训练的 batch size 保持一致。这种情形下，由于数据并行训练和单卡训练的 batch size 是一致的，通常保持数据并行模式下各个计算设备上的学习率与单卡训练一致。

* 另一种情形是保持每个计算设备的 batch size 和单卡训练的 batch size 一致。这种情形下，数据并行模式的全局 batch size 是单卡训练的$$N$$倍，$$N$$是数据并行计算的设备数。因此，通常需要将数据并行模式下每个计算设备的学习率相应的设置为单卡训练的$$N$$倍。这样，数据并行模式下的初始学习率通常较大，不利于模型的收敛。因此，通常需要使用 warm-up 机制。即，在初始训练时使用较小的学习率，并逐步缓慢增加学习率，经过一定迭代次数后，学习率增长到期望的学习率。

**数据集切分**

输入数据切分实现上比较简单，一般有两种常用的实现方式：

> 1. 在每个训练 epoch 开始前，将整个训练数据集根据并行进程数划分，每个进程只读取自身切分的数据
>
> 2. 数据的读取仅由具体某个进程负责，一般为rank0，该进程在数据读取后同样根据并行进程数将数据切分成多块，再将不同数据块发送到对应进程上

数据并行中，通常将数据集切分为$$N$$份，每个训练卡负责训练其中的一份数据。这里$$N$$是数据并行的并行度。由于每一个迭代中，各个训练卡均需要做一次梯度同步。因此需要确保对于每个 epoch，各个训练卡经历相同的迭代数，否则，运行迭代数多的训练卡会一直等待通信完成。实践中，通常通过**数据补齐**或者**丢弃**的方式保证各个训练卡经历相同的迭代数。

> **注**：数据补齐的方式指的是，为某些迭代数少训练数据补充部分数据，从而保证切分后的各份数据集的迭代次数相同；丢弃的方式则是丢弃部分迭代次数较多的数据，从而保证各份数据集的迭代次数相同。

通常，在每个 epoch 需要对数据做 shuffle 处理。因此，根据 shuffle 时机的不同，有两种数据切分的方法。

**在数据切分前做 shuffle**

首先对完整的数据做 shuffle 处理，做相应的数据补充或丢弃，然后做数据的切分。

**在数据切分后做 shuffle**

首先做数据的补充或丢弃和数据切分，然后对切分后的每一份数据分别做 shuffle 处理。

**参数同步**

数据并行实现的关键问题在于如何保证训练过程中每个进程上模型的参数相同。因为训练过程的每一个 step 都会更新模型参数，每个进程处理不同的数据会得到不同的 Loss。由 Loss 计算反向梯度并更新模型参数后，如何保证进程间模型参数正确同步，是数据并行需要解决的最主要问题。根据梯度更新公式，只要保证两点就能解决这个问题：

> 1. 每个进程上模型初始化参数$$W_0$$相同，保证每个进程模型参数初始相同有两种常用的实现方法：
>
>    1. 所有进程在参数初始时使用相同的随机种子并以相同的顺序初始化所有参数
>
>    2. 通过某个具体进程初始化全部模型参数，之后由该进程向其他所有进程广播模型参数
>
> 2. 每个进程每次更新时的梯度$$\Delta W$$相同

基于上述方法使每个进程得到一份相同的模型初始化参数后，梯度同步的数据并行训练就可以进一步拆解为如下三个部分：

**前向计算**：每个进程根据自身得到的输入数据独立前向计算，因为输入数据不同每个进程会得到不同的Loss。

**反向计算**：每个进程根据自身的前向计算独立进行反向计算，因为每个进程上的 Loss 不同，每个进程上在反向中会计算出不同的梯度。这时一个关键的操作是要在后续的更新步骤之前，对所有进程上的梯度进行同步，保证后续更新步骤中每个进程使用相同的全局梯度更新模型参数。

![]()

这一个梯度同步过程是用一&#x4E2A;**`AllReduce Sum`**&#x540C;步通信操作实现的，对梯度使用 **AllReduce Sum** 操作后每个进程上得到的梯度是相同的，这时候的梯度值等于所有进程上梯度对应位置相加的和，然后每个进程&#x7528;**`AllReduce`**&#x540E;的梯度和除以数据并行中的进程数，这样得到的梯度是同步之前所有进程上梯度的平均值。如下图所示。

**参数更新**：每个进程经过上述步骤后得到相同全局梯度，然后各自独立地完成参数更新。因为更新前模型各进程间的参数是相同的，更新中所使用的梯度也是相同的，所以更新后各进程上的参数也是相同的。
上述是主流框架中数据并行的实现过程。和单卡训练相比，最主要的区别在于反向计算中的梯度需要在所有进程中进行同步，保证每个进程上最终得到的是所有进程上梯度的平均值。

### 5.4.2 **张量并行**

随着技术的发展，业界内训练的模型越来越大，模型朝着更深和更宽的方向发展。以 NLP 领域为例，模型从 BERT 发展到 GPT，模型规模从数亿参数量增加到数百亿甚至是数千亿。当参数规模为千亿时，存储模型参数就需要数百 GB 的显存空间，超出单个 GPU 卡的显存容量。显然，仅靠数据并行无法满足超大规模模型训练对于显存的需求。为了解决这个问题，可以采用模型并行技术。与数据并行在不同设备都有完整的计算图不同，模型并行是不同设备负责单个计算图不同部分的计算。

模型并行从计算图的切分角度，可以分为以下几种：

1. 按模型的层切分到不同设备，即**层间并行**，称为**流水线并行**。

2. 将层内的参数切分到不同设备，即**层内并行**，称为**张量并行**，这里讲张量并行。

![层间并行]()

![层内并行]()

张量并行总体来说就是将张量操作划分到多个设备上，以加速计算或增加模型大小；对模型每一层的层内参数进行切分，即对参数矩阵切片，并将不同切片放到不同 GPU 上；比如将原本在单卡中的矩阵乘法，切分到不同卡中进行矩阵乘法。训练过程中，正向和反向传播计算出的数据通过使&#x7528;**`All gather`**&#x6216;&#x8005;**`All reduce`**&#x7684;方法完成整合。

![]()

如上图，在 Tansformer 中，张量并行会把 **MHA** 和 **FFN** 都进行切分以并行化。利用 Transformer 网络的结构，通过添加一些同步原语来创建一个简单的模型并行实现。张量并行适用于模型单层网络参数较大的情况。同时缺点也是十分明显：

> 1. 当环境是多机多卡，张量并行所需的 **all-reduce** 通信需要跨服务器进行连接，这比单机多 GPU 服务器内的高带宽通信要慢，因为机间通信比卡间通信成本高
>
> 2. 高度的模型并行会产生很多小矩阵乘法，这可能会降低 GPU 的利用率

**张量并行**解决两个问题：

> 1. **切分方式**：参数如何切分到不同设备
>
> 2. **数学等价**：切分后，如何保证数学一致性

Transformer 结构主要由嵌入式表示 **Embedding** 层、矩阵乘 **MatMul** 层和交叉熵 loss 计算 **CrossEntropyLoss** 层构成。以上三种类型的组网层有较大的特性差异，需要设计对应的张量模型并行策略，但总体上看核心思想都是利用分块矩阵的计算原理，实现其参数切分到不同的设备。下面详细介绍这三种层的切分方式：

**Embedding 切分**

对于 **Embedding** 算子，如果总的词表非常大，会导致单卡显存无法容纳 **Embedding** 层参数。

**例**：当词表数量是$$50304$$，词表表示维度为$$5120$$，类型&#x4E3A;**`FP32`**，那么整层参数需要显存大约为$$\frac{50304\times 5120\times 4}{1024\times 1024}=982\text{MB}$$，反向梯度同样需要$$982\text{MB}$$，仅仅存储就需要将近$$2\text{GB}$$。

对于 **Embeding** 层的参数，可以按照词的维度切分，即每张卡只存储部分词向量表，然后通&#x8FC7;**`AllReduce`**&#x6C47;总各个设备上的部分词向量结果，从而得到完整的词向量结果。

![Embedding 切分]()

上图描述了单卡 **Embedding** 和 **Embedding** 两卡张量模型并行的示意图。在单卡上，执行 Embedding 操作，bz是 batch size 大小，**Embedding** 的参数大小&#x4E3A;**`[word_size, hidden_size]`**，计算得&#x5230;**`[bz, hidden_size]`**&#x5F20;量。下半部分为 **Embedding** 张量模型并行示例，其将 **Embedding** 参数沿 **word\_size** 维度，切分为两块，每块大小&#x4E3A;**`[word_size/2, hidden_size]`**，分别存储在两个设备上，即每个设备只保留一半的词表。当每张卡查询各自的词表时，如果无法查到，则该词的表示&#x4E3A;**`0`**，各自设备查询后得&#x5230;**`[bz, hidden_size]`**&#x7ED3;果张量，最后通&#x8FC7;**`AllReduceSum`**&#x901A;信，跨设备求和得到完整的全量结果，可以看出这里的输出结果和单卡执行的结果一致。

**MatMul 切分**

矩阵乘的张量模型并行充分利用矩阵分块乘法的原理。假设要实现如下矩阵乘法$$Y=XA$$，其中$$X$$是维度为$$M\times N$$的输入矩阵，$$A$$是维度为$$N\times K$$的参数矩阵，$$Y$$是结果矩阵，维度为$$M\times K$$。如果参数矩阵$$A$$非常大，甚至超出单张卡的显存容量，那么可以把参数矩阵$$A$$切分到多张卡上，并通过集合通信汇集结果，保证最终结果在数学计算上等价于单卡计算结果。这里，参数矩阵$$A$$存在两种切分方式：

**列切分**

![]()

参数矩阵$$A$$按列切块。如上图所示，将矩阵$$A$$按列切成$$A=[A_1| A_2]$$，分别将$$A_1, A_2$$放置在两张卡上。两张卡分别计算$$Y_1=XA_1$$和$$Y_2=XA_2$$。计算完成后，通&#x8FC7;**`AllGather`**&#x64CD;作获取其它卡上的计算结果，拼接在一起得到最终的结果矩阵$$Y$$。



**行切分**

![]()

参数矩阵$$A$$按行切块。如上图所示，将矩阵$$A$$按行切成$$A=\begin{bmatrix}A_1 \\
A_2\end{bmatrix}$$，为了满足矩阵乘法规则，输入矩阵$$X$$需要按列切分$$X=[X_1|X_2]$$。同时将矩阵分块，分别放置在两张卡上，每张卡分别计算$$Y_1=X_1A_1$$，$$Y_2=X_2A_2$$。计算完成后，通&#x8FC7;**`AllReduce`**&#x5F52;约其他卡上的计算结果，可以得到最终的结果矩阵$$Y$$。

Transformer 中的 **FFN** 结构均包含两层全连接层，即存在两个矩阵乘，这两个矩阵乘分别采用上述两种切分方式，如右图所示。对第一个全连接层的参数矩阵按列切块，对第二个全连接层参数矩阵按行切块。这样第一个全连接层的输出恰好满足第二个全连接层数据输入要求即按列切分，因此可以省去第一个全连接层后&#x7684;**`AllGather`**&#x901A;信操作。

![]()

**CrossEntropyLoss 计算**

分类网络最后一层一般会选用 softmax 和 cross\_entropy 算子来计算交叉熵损失。如果类别数量非常大，会导致单卡显存无法存储和计算 logit 矩阵。针对这一类算子，可以按照类别数维度切分，同时通过中间结果通信，得到最终的全局的交叉熵损失。

首先计算的是 softmax 值，如下公式：

$$x_\text{max} = \max_N(\max_k(x_k))$$

$$\text{softmax}(x_i)=\frac{e^{x_i}}{\sum_je^{x_j}}=\frac{e^{x_i-x_\text{max}}}{\sum_je^{x_j-x_\text{max}}}=\frac{e^{x_i}-x_\text{max}}{\sum_N\sum_ke^{x_j-x_\text{max}}}$$

其中$$N$$表示张量模型并行的设备号



![]()

得到 softmax 之后，同时对标签 target 按类别切分，每个设备得到部分 loss，最后在进行一次通信，得到全量的 loss。整个过程，只需要进行三次小量的通信，就可以完成 **CrossEntropyLoss** 的计算，流程图如图所示。

### 5.4.3 **流水线并行**

模型切分一般有两种方式：参数（Tensor）切分和图切分。在上一节张量并行中介绍过，张量并行可以把 shape 较大的参数切分到多个卡，从而有效减小在每个卡上的参数量。但是，采用这种方式在单机 8 卡上，&#x5982;**`32GB V100`**，最多训练 **10B** 左右的 **Dense** 模型，而且由于其通信和计算不能重叠的特点一般不适合多机之间使用。更大的模型需要从层级别进行进一步的图切分以减少单卡存储的容量需求，同时隐藏卡之间的通信时间，更加充分利用 GPU 卡的算力。为此业界提出了**流水线并行**的方式解决上述切分和调度的问题。流水线原理是将不同的层分配给指定 GPU 进行计算，流水线并行只需其之间点对点地通讯传递部分信息。具体步骤包括：

> 1. 在流水线并行之中，一个模型的各层会在多个GPU上做切分
>
> 2. 一个 batch 被分割成较小的 micro-batch，并在这些微批上进行流水线式执行
>
> 3. 通过流水线并行，一个模型的层被分散到多个设备上
>
> 4. 当用于具有相同 Transformer 块重复的模型时，每个设备可以被分配相同数量的 Transformer 层
>
> 5. 在流水线模型并行中，训练会在一个设备上执行一组操作，然后将输出传递到流水线中下一个设备，下一个设备将执行另一组不同操作

广义上讲，流水线并行从图切分的角度，将模型进一步按层切分到不同的设备上，相邻设备间在计算时只需要传递邻接层的中间变量和梯度。如下图左所示，这里对 FFN 采用**张量并行**，把全连接层参数切分到设备内的不同卡上，在同一设备内进&#x884C;**`AllReduce`**&#x6C42;和的全量通信参数为$$2MK$$，$$M$$是矩阵行维度，$$K$$是矩阵列维度。而采用流水线并行将 FFN 层作为切分点切分模型层，在设备间只需要发送或接收$$MK$$的参数量，因此相比张量模型并行，流水线并行通信参数量更少，如下图右所示。这里展示的是 Gpipe 中的朴素流水线并行示例，同一时刻只有一个设备进行计算，其余设备处于空闲状态，计算设备利用率通常较低。

![张量并行]()

![流水线并行]()

与之相对应，将朴素流水线并行的 batch 再进行切分，减小设备间空闲状态的时间，可以显著提升流水线并行设备利用率。如下&#x56FE;**`F-then-B`**&#x8C03;度方式所示，将原本的数据并行切分后的 **mini-batch** 划分成多个 **micro-batch**，每个流水线并行的计算单元先整体进行前向计算，再进行反向计算。通过在同一时刻分别计算模型的不同部分，**F-then-B** 可以显著提升设备资源利用率。但也不难看出这种 **F-then-B** 模式由于缓存了多个 **micro-batch** 的中间变量和梯度，显存的实际利用率并不高。

![]()

由此产生&#x51FA;**`1F1B`**&#x6D41;水线并行方式解决了这个问题。在 **1F1B** 模式下，前向计算和反向计算交叉进行，可以及时释放不必要的中间变量，如下图所示。**1F1B** 方式相比 **F-then-B** 方式峰值显存可以节&#x7701;**`37.5%`**，对比朴素流水线并行峰值显存明显下降，设备资源利用率显著提升。

![]()

**例**：以上图 **1F1B** 中 stage4 &#x7684;**`F42`**&#x4E3A;例，**`F42`**&#x5728;计算前，**`F41`**&#x7684;反&#x5411;**`B41`**&#x5DF2;经计算结束，即可释&#x653E;**`F41`**&#x7684;中间变量，从&#x800C;**`F42`**&#x53EF;以复&#x7528;**`F41`**&#x4E2D;间变量的显存。

**F-then-B** 和 **1F1B** 调度方式都采用增加 batch 数量，即将 **mini-batch** 切分为 **micro-batch** 增加设备资源利用率。这里从 **micro-batch** 角度从理论上推导下计算空闲，即 **bubble&#x20;**&#x548C; **micro-batch** 实际计算时间之间的关系。bubble 时间的计算公式：

$$t_\text{pb}=(p-1)(t_f+t_b)$$

其中$$t_\text{pb}$$是一个 mini-batch 中 bubble 的时间，$$p$$是流水线计算单元的数量，$$t_f$$是每个 micro-batch 的前向时间，$$t_b$$是每个 micro-batch 的反向时间。

$$t_\text{id}=m(t_f + t_b)$$

其中$$t_\text{id}$$是一个 mini-batch 的计算时间，$$m$$是一个 mini−batch 中 micro-batch 的数量。由上述两个公式可以得到：

$$S_\text{Bubble}=\frac{t_\text{pb}}{t_\text{id}}=\frac{p-1}{m}$$

由此不难看出，在流水线计算单元数$$p$$一定的情况下，影响 bubble 占比的是一个 mini-batch 中 micro-batch 的数量$$m$$。在保证$$m\gg p$$的情况下，可以有效降低 bubble 占比。

### 5.4.4 **序列并行**

目前，有两篇关于序列并行的文章：

> 1. **Colossal-AI** 发表的论文：[**Sequence Parallelism**](https://arxiv.org/pdf/2105.13120)
>
> 2. **Megatron-LM** 发表的论文：[**Sequence Parallelism**](https://arxiv.org/pdf/2205.05198)

这两者解决的问题并不相同，前者主要是解决模型的输入长度限制，而后者是主要是减少模型显存的。下面分别讲解。

* **Colossal-AI 序列并行**

**Colossal-AI** 序列并行诞生的背景是 Self Attention 的内存需求是输入长度的平方。其复杂度为$$O(n^2)$$，其中，$$n$$是序列长度。也就是说长序列数据将增加中间内存使用量，从而限制设备的训练能力。而现有的工作侧重于从算法的角度降低时间和空间复杂度。因此，作者提出了序列并行，这是一种内存高效的并行方法，可以打破输入序列长度限制，并在 GPU 上有效地训练更长的序列；同时，该方法与大多数现有的并行技术兼容，如**数据并行**、**流水线并行**和**张量并行**。并且序列并行不再需要单个设备来保存整个序列。 即在稀疏注意力的情况下，序列并行能够训练具有无限长序列的 Transformer。这里展示了序列并行与流水线并行和张量并行的区别

![]()

具体来说就是将输入序列分割成多个块，并将每个块输入到其相应的 GPU 设备中。为了计算注意力输出，将环状通信与自注意力计算相结合，并提出了环自注意力 **RSA**（**R**ing **S**elf-**A**ttention），如下图所示。

![在设备之间传输 key 以计算注意力得分]()

![在设备之间传输 value 以计算注意力层的输出]()

实验表明，当按批量大小和序列长度进行缩放时，序列并行表现良好。当扩展&#x5230;**`64`**&#x4E2A; NVIDIA P100 GPU 时，与张量并相比，该算法分别实现&#x4E86;**`13.7`**&#x500D;&#x548C;**`3.0`**&#x500D;的最大批量大小和序列长度。

通过稀疏注意力，序列可以处理具有超&#x8FC7;**`114K`**&#x4E2A; Token 的序列，这比现有的在单个设备上保存整个序列的稀疏注意力运行长度超&#x8FC7;**`27`**&#x500D;。

除此之外，与张量并行和流水线并行不同，序列并行不受超参数，如**注意力头数**、**层数**等的限制。 因此，只要序列长度能被序列并行大小整除，序列并行就可以使用。

* **Megatron-LM 序列并行**

**Megatron-LM** 的初衷是考虑通过其他方式分摊张量并行中无法分摊的显存，因此提出了序列并行的方法。**Megatron-LM** 借用了 **Colossal-AI** 把 Sequence 这个维度进行平均划分的思想。在张量的基础上，将 Transformer 层中的 **LayerNorm** 以及 **Dropout** 的输入按输入长度维度进行了切分，使得各个设备上面只需要做一部分的 **Dropout** 和 **LayerNorm** 即可。

这样做的好处有：

> 1. **LayerNorm&#x20;**&#x548C; **Dropout&#x20;**&#x7684;计算被平摊到了各个设备上，减少了计算资源的浪费
>
> 2. **LayerNorm&#x20;**&#x548C; **Dropout&#x20;**&#x6240;产生的激活值被平摊到各个设备上，进一步降低了显存开销

Megatron-LM 首先分析了 Transformer 模型运行时的显存占用情况。假设输入长度为$$s$$，batch size为$$b$$，hidden dim为$$h$$，attention head数量为$$a$$，则每一层 Transformer 显存占用为$$sbh(34+5\frac{as}{h})$$

![Transformer 模型结构]()

当开启了张量并行之后，Transformer 层中的部分模块的显存可以被分摊到不同的设备之间。如右图所示，不能被分摊的部分主要是两个 **LayerNorm** 块的输入和输出$$4bsh$$；两个 **dropout mask** 块$$2bsh$$；一共是$$10bsh$$。

![带有张量并行的 Transformer 层]()

假设张量并行大小为$$t$$，因此，每个设备每一层 Transformer 的显存占用为$$sbh(10+\frac{24}{t}+5\frac{as}{ht})$$

下面开启张量并行以及序列并行，Transformer 层中的 **LayerNorm** 和 **Dropout** 块也会被切分，对 Tensor 在 Sequence 维度进行切分，切分数量等于张量并行大小。

![带有张量并行和序列并行的 Transformer 层]()

每个设备每一层 Transformer 的显存占用为$$sbh(\frac{10}{t}+\frac{24}{t}+5\frac{as}{ht})=sbh(\frac{34}{t}+5\frac{as}{ht})$$

当然，做了额外的切分就会带来通信方式的改变。Transformer 层的张量并行通信是由正向传播两&#x4E2A;**`All-Reduce`**&#x4EE5;及反向传播两&#x4E2A;**`All-Reduce`**&#x7EC4;成。

而序列并行由于对 Sequence 维度进行了划分，**`All-Reduce`**&#x5728;这里已经不合适了。为了收集在各个设备上进行序列并行所产生的结果，需要插&#x5165;**`All-Gather`**&#x7B97;子；而为了使得张量并行所产生的结果可以传入序列并行层，需要插&#x5165;**`Reduce-Scatter`**&#x7B97;子。

![带有张量并行和序列并行的 MLP 层]()

在右图中，$$g$$所代表的就是前向传播的 **All-Gather**，反向传播的 **Reduce-Scatter**，$$\overline{g}$$则是相反的操作。

因此在 Megatron-LM 同时开启序列并行和模型并行时，每一个 Transformer 层完成一次前向传播和反向传播一共&#x6709;**`4`**&#x4E2A; **All-Gather** &#x548C;**`4`**&#x4E2A; **Reduce-Scatter** 算子。乍一看，通信的操作比 Megatron-LM 仅开启张量并行多，但其实不然，因为一个 **All-Reduce** 就相当于一个 **Reduce-Scatter** 和一个 **All-Gather**，所以他们的总通信量是一样的。通过添加序列并行并没有增加额外的通信开销，反而在后向传播代码的实现上，还把 **Reduce-Scatter** 和权重梯度的计算做了重叠，进一步减少了通信所占用的时间，使得提高设备的 FLOPs Utilization 成为了可能。

作者发现在 Transformer 层里有一些操作是产生的激活值大，但计算量小。因此需要去掉这一部分的激活值，通过选择性激活重新计算（Selective Activation Recomputation）来进一步降低显存。与此同时，其他的激活值就通通保存，以节省重计算量。通过对激活值的占比分析，序列并行降低&#x4E86;**`40%`**&#x5DE6;右的激活值开销。选择性激活重新计算也降低&#x4E86;**`40%`**&#x5DE6;右的激活值开销。当两个特性都打开的时候，总共可以降&#x4F4E;**`80%`**&#x5DE6;右的激活值开销，尽管比全部激活值重计算的结果要稍高，但是在吞吐率上的提升还是非常的明显的。

### 5.4.5 **MoE 并行**







## 5.5 系统优化

> **论文**：**[MPT](https://arxiv.org/pdf/1710.03740)  [FlashAttention-1](https://arxiv.org/pdf/2205.14135)  [FlashAttention-2](https://arxiv.org/pdf/2307.08691)  [FlashAttention-3](https://arxiv.org/pdf/2407.08608)**
>
> 代码：[**FlashAttention-1**](https://github.com/HazyResearch/flash-attention)

### 5.5.1 混合精度训练

通常训练神经网络模型的时候默认使用的数据类型为单精&#x5EA6;**`FP32`**。近年来，为了加快训练时间、减少网络训练时候所占用的内存，并且保存训练出来的模型精度持平的条件下，业界提出越来越多的混合精度训练的方法。这里的混合精度训练是指在训练的过程中，同时使用单精&#x5EA6;**`FP32`**&#x548C;半精&#x5EA6;**`FP16`**。

* **浮点数据类型**

浮点数据类型主要分为双精&#x5EA6;**`FP64`**、单精&#x5EA6;**`FP32`**、半精&#x5EA6;**`FP16`**。在神经网络模型的训练过程中，一般默认采用单精度 **FP32** 浮点数据类型，来表示网络模型权重和其他参数。在了解混合精度训练之前，这里简单了解浮点数据类型。

根据 **IEEE&#x20;**&#x4E8C;进制浮点数算术标&#x51C6;**`IEEE 754`**&#x7684;定义，浮点数据类型分为双精度 **FP64**、单精度 **FP32**、半精度 **FP16** 三种，其中每一种都有三个不同的位来表示。

![]()

**FP64** 表示的是采&#x7528;**`8`**&#x4E2A;字节&#x5171;**`64`**&#x4F4D;来进行编码存储的一种数据类型；同理，**FP32&#x20;**&#x8868;示采&#x7528;**`4`**&#x4E2A;字节&#x5171;**`32`**&#x4F4D;来表示；**FP16&#x20;**&#x5219;是采&#x7528;**`2`**&#x5B57;节&#x5171;**`16`**&#x4F4D;来表示，如上图所示。从图中可以看出，与 **FP32** 相比，**FP16** 的存储空间是 **FP32** 的一半，**FP32** 则是 **FP64** 的一半。每个浮点数都分为三个部分：

> 1. 最高位表示**符号位`sign`** bit
>
> 2. 中间表示**指数位`exponent`** bit
>
> 3. 低位表示**分数位`fraction`** bit

**例**：假设使用 **FP16**，第一位符号&#x4F4D;**`sign`**&#x8868;示正负符&#x53F7;**，用$$S$$表示，**&#x63A5;着 **5** 位表示指&#x6570;**`exponent`，用**$$E$$**表示**，最后 **10** 位表示分&#x6570;**`fraction`，用**$$M$$**表示**。公式为：$$x=(−1)^S×2^{E−15}×(1+\frac{M}{1024})$$

同理，一个规则化的 **FP32** 的真值为：$$x=(−1)^S×2^{E−127}×(1+\frac{M}{2^{23}})$$；一个规格化的 **FP64** 的真值为：$$x=(−1)^S×2^{E−1023}×(1+\frac{M}{2^{52}})$$。

**FP16** 的数值表示范围取决于其格式和特殊值的定义。根据 IEEE 754 标准，**FP16** 的数值范围可以分为以下几部分：**正规数范围** 、**非规格化数范围** 和 **特殊值**:

**正规数范围**

正规数是指指数位$$E\ne0$$且$$E\ne31$$的数。指数的实际值为$$E−15$$；尾数部分隐含一个前&#x5BFC;**`1`**，因此尾数的范围是 **`[1,2)`**。

**最大正规数**：指数位$$E = 30$$，尾数部分全为 1，即$$M = 1023$$，因此最大正规数为：

$$x_\text{max}=(−1)^0×2^{15}×(1+\frac{1023}=65504$$

**最小正规数**：指数位$$E = 1$$，尾数部分全为 0，因此最小正规数为：

$$x_\text{min, normal}=(−1)^0×2^{−14}×(1+0)=2^{−14}=0.00006103515625$$

**非规格化数范围**

非规格化数是指指数位$$E=0$$的数。这时指数固定为 $$−14$$；尾数部分没有隐含的前导`1`，为 $$\frac{M}{1024}$$。

**最小非零数**：指数位$$E = 0$$，实际指数为$$-14$$；尾数部分最低有效位$$M = 1$$，即尾数为$$\frac{1}{1024}$$。因此最小非零数为：

$$x_\text{min, subnormal}=(−1)^0×2^{−14}×\frac{1}{1024}=2^{−24}=5.9604644775390625×10^{−8}$$



**特殊值**

**零**：当$$E=0$$且$$M=0$$时表示零。符号位$$S$$决定是正零$$+0$$还是负零$$−0$$。

**无穷大**：当$$E=31$$且$$M=0$$时表示无穷大。符号位$$S$$决定是正无穷$$+∞$$还是负无穷$$−∞$$。

**NaN**：当$$E=31$$且$$M\ne0$$时，表示 **NaN**（**N**ot **a** **N**umber）。

总的来说，FP16 的数值表示范围如下：

> 1. **正规数范围**：正数$$[2^{-14}, 65504]$$，负数$$[-65504, -2^{-14}]$$
>
> 2. **非规格化数范围**：正数$$[2^{-24}, 2^{-14})$$，负数$$(-2^{-14}, -2^{-24}]$$
>
> 3. **特殊值** ：零$$+0$$和$$−0$$，无穷大$$+\infty$$和$$-\infty$$，NaN
>
> 4. **完整数值范围**：$$[−65504,−5.96×10^{−8}]∪\{−0,+0\}∪[5.96×10^{−8},65504]$$

使用 **FP16&#x20;**&#x8BAD;练神经网络，相对比使用 **FP32&#x20;**&#x6709;一些优点和缺点：

**优点**

1. **减少内存占用**：**FP16&#x20;**&#x7684;位宽是 **FP32** 的一半，因此权重等参数所占用的内存也是原来的一半，节省下来的内存可以放更大的网络模型或者使用更多的数据进行训练。

2. **加快通讯效率**：针对分布式训练，特别是在大模型训练的过程中，通讯的开销制约了网络模型训练的整体性能，通讯的位宽少了意味着可以提升通讯性能，减少等待时间，加快数据的流通。

3. **计算效率更高**：在特殊的 AI 加速芯片如华为Ascend 910 和 310 系列或 NVIDIA VOTAL 架构的 Titan V and Tesla V100 的 GPU 上，使用 **FP16** 的执行运算性能比 **FP32** 更加快。

**缺点**

1. **数据溢出**：**FP16** 相比 **FP32** 的有效范围要窄很多，使用 **FP16** 替换 **FP32** 会出现上&#x6EA2;**`Overflow`**&#x548C;下&#x6EA2;**`Underflow`**&#x7684;情况。而在深度学习中，需要计算网络模型中权重的梯度，因此梯度会比权重值更加小，往往容易出现下溢情况。

2. **舍入误差**：**`Rounding Error`**&#x6307;示是当网络模型的反向梯度很小，一般 **FP32** 能够表示，但是转换到 **FP16** 会小于当前区间内的最小间隔，会导致数据溢出。&#x5982;**`0.00006666666`**&#x5728; **FP32** 中能正常表示，转换到 **FP16** 后会表示成&#x4E3A;**`0.000067`**，不满足 **FP16** 最小间隔的数会强制舍入。

为了想让深度学习训练可以使用 **FP16** 的好处，又要避免精度溢出和舍入误差。于是可以通过 **FP16** 和 **FP32** 的**混合精度训练**，混合精度训练过程中可以引入**权重备份 Weight Backup**、**损失放大 Loss Scaling**、**精度累加 Precision Accumulated** 三种相关的技术。

* **权重备份 Weight Backup**

这种方法主要是用于解决舍入误差的问题。其主要思路，可以概括为：weights, activations, gradients 等数据在训练中都利用 FP16 来存储，同时拷贝一份 **FP32** 的 weights，用于更新。由于在更新权重公式为$$w=w+η∗g$$，在深度模型中，$$η∗g$$的参数值可能会非常小，利用 **FP16** 来进行相加的话，则很可能会出现舍入误差问题，导致更新无效。

![]()

因此通过将权重 weights 拷贝成 **FP32** 格式，并且确保整个更新过程是在 **FP32** 格式下进行的。即$$w_{32} = w_{32} + \eta * g_{16}$$

权重用 **FP32** 格式备份一次，使得内存占用反而更高，额外拷贝一份 weights 增加了训练时候内存的占用。 但是实际上，在训练过程中内存中分为动态内存和静态内容，其中动态内存是静态内存&#x7684;**`3-4`**&#x500D;，主要是中间变量值和激活 activations 的值，而这里备份的权重增加的主要是静态内存。只要动态内存的值基本都是使用 **FP16** 来进行存储，则最终模型与整网使用 **FP32** 进行训练相比起来，内存占用也基本能够减半。

* **损失缩放 Loss Scaling**

如下图左所示，如果仅仅使用 **FP32** 训练，模型收敛得比较好，但是如果用了混合精度训练，会存在网络模型无法收敛的情况。原因是梯度的值太小，使用 **FP16** 表示会造成了数据下溢出 Underflow 的问题，导致模型不收敛，如图中灰色的部分。于是需要引入损失缩放 Loss Scaling 技术。

![]()

![]()

上图右是在网络模型训练阶段， 某一层的激活函数梯度分布式中，其中&#x6709;**`68%`**&#x7684;网络模型激活参数位 **0**，另外&#x6709;**`4%`**&#x7684;精度在$$[2^{-32}, 2^{-20}]$$这个区间内，直接使用 FP16 对这里面的数据进行表示，会截断下溢的数据，所有的梯度值都会变为 **0**。

右图&#x662F;**`SSD`**&#x68C0;测器网络在 **FP32** 训练过程中收集的各层激活梯度值直方图。可以看到，**FP16** 的大部分可表示范围都未使用，而许多值都低于最小可表示范围，变成了零。因此如果直接使用 **FP16** 进行训练，网络是发散的。但如果我们把梯度缩放 8 倍，即指数增加 3 倍，那么小于$$2^{−27}$$的梯度值才会变为 0，$$[2^{−27},2^{−24})$$的值得以保留，此时网络就可以正常训练了。

![]()

![]()

为了解决梯度过小的问题，论文中对计算出来的 loss 值进行缩放，由于链式法则的存在，loss 上的缩放会作用也会作用在梯度上。这样比起对每个梯度进行缩放更加划算。缩放过后的梯度，就会平移到 **FP16** 有效的展示范围内。这样，缩放过的梯度就可以一直使用 **FP16** 进行存储了。只有在进行更新的时候，才会将缩放过的梯度转化为 **FP32**，同时将缩放抹去。

可以选择一个恒定的缩放因子，论文使用&#x4E86;**`8`**&#x5230;**`32K`**&#x8BAD;练了各种网络；可以根据经验选择。如果有梯度统计数据，可以选择一个较大的因子，同时保证其与最大绝对梯度值的乘积小&#x4E8E;**`65504`**，即 **FP16** 的最大值。只要不导致溢出，缩放因子较大也没有坏处。通过检查计算出的权重梯度，例如权重梯度值未缩放时，可以有效地检测到溢出。一种方法是在检测到溢出时跳过权重更新，直接进入下一次迭代。

损失放大是需要结合混合精度实现的，其主要的主要思路是：

> 1. **Scale Up** 阶段：网络前向计算后在反响传播前，将损失变化值增大$$2^K$$倍
>
> 2. **Scale Down** 阶段：反向传播后将权重梯度缩$$2^K$$倍，恢复 **FP32&#x20;**&#x503C;进行存储

**动态损失缩放 Dynamic Loss Scaling**

上面提到的损失缩放都是使用一个默认值对损失值进行缩放，为了充分利用 **FP16** 的动态范围，可以更好地缓解舍入误差，尽量使用比较大的放大倍数。总结动态损失缩放算法，就是每当梯度溢出时候减少损失缩放规模，并且间歇性地尝试增加损失规模，从而实现在不引起溢出的情况下使用最高损失缩放因子，更好地恢复精度。

动态损失缩放的算法如下：

> 1. 动态损失缩放的算法会从比较高的缩放因子开始，如$$2^{24}$$，然后开始进行训练迭代中检查数是否会溢出：
>
>    1. 没有梯度溢出：不进行缩放，继续进行迭代
>
>    2. 梯度溢出：缩放因子减半，重新确认梯度更新情况，直到数不产生溢出的范围内
>
> 2. 在训练的后期，loss 已经趋近收敛稳定，梯度更新的幅度往往小了，这个时候可以允许更高的损失缩放因子来再次防止数据下溢
>
> 3. 因此，动态损失缩放算法会尝试在每$$N=2000$$次迭代将损失缩放增加$$F$$倍数，然后检查是否溢出

* **精度累加 Precision Accumulated**

在混合精度的模型训练过程中，使用 **FP16** 进行矩阵乘法运算，利用 **FP32** 来进行矩阵乘法中间的累加，然后再将 **FP32** 的值转化为 **FP16** 进行存储。简单而言，就是利用 **FP16** 进行矩阵相乘，利用 **FP32** 来进行加法计算弥补丢失的精度。 这样可以有效减少计算过程中的舍入误差，尽量减缓精度损失的问题。

在 Nvidia Volta 结构中带&#x6709;**`Tensor Core`**，可以利用 **FP16** 混合精度来进行加速，还能保持精度。Tensor Core 主要用于实现 **FP16** 的矩阵相乘，在利用 **FP16** 或者 **FP32** 进行累加和存储。在累加阶段使用 **FP32** 能够大幅减少混合精度训练的精度损失。

![]()

* **混合精度训练策略** **AMP**（**A**utomatic **M**ixed **P**recision）

混合精度训练不仅仅是在深度学习，另外在HPC的迭代计算场景下，从迭代的开始、迭代中期和迭代后期，都可以使用不同的混合精度策略来提升训练性能的同时保证计算的精度。以动态的混合精度达到计算和内存的最高效率比是一个较为前沿的研究方向。

**例**：NVIDIA &#x7684;**`APEX`**&#x6DF7;合精度库，里面提供了4种策略，分别是

![使用 FP32 进行训练的O0           只优化前向计算部分的O1      除梯度更新外都使用混合精度的O2     使用 FP16 进行训练的O3    ]()

O1 策略中，会根据实际 Tensor 和操作之间的关系建立黑白名单来使用 FP16。例如 GEMM 和 CNN 卷积操作对于 FP16 操作特别友好的计算，会把输入的数据和权重转换成 FP16 进行运算，而 Softmax、BN 等标量和向量在 FP32 操作好的计算，则是继续使用 FP32 进行运算，另外还提供了动态损失缩放。

O2 策略中，模型权重参数会转化为 FP16，输入的网络模型参数也转换为 FP16，BN 使用 FP32，另外模型权重文件复制一份 FP32 用于跟优化器更新梯度保持一致都是 FP32，另外还提供动态损失缩放。使用了权重备份来减少舍入误差和使用损失缩放来避免数据溢出。

**总结**

混合精度训练的计算流程：

![]()

> 1. 参数以 **FP32** 存储
>
> 2. 正向计算，用 **FP16** 算子把参数从 **FP32** 转换成 **FP16** 进行计算
>
> 3. 将 Loss 层设置为 **FP32** 进行计算
>
> 4. 反向计算，乘以损失缩放因子，避免反向梯度过小而产生下溢
>
> 5. **FP16** 参数参与梯度计算，结果转换回**FP32**
>
> 6. 除以损失缩放因子，还原放大的梯度
>
> 7. 判断梯度是否存在溢出，溢出则跳过更新，否则优化器以 **FP32** 对参数进行更新

### 5.5.2 内存优化

* 梯度检查点：通过重计算减少显存占用

* 梯度累计

* 激活重计算：在前向传播中不保存中间激活值，反向传播时重新计算



### 5.5.3 FlashAttention-1

* **基础知识**

GPU 内存层次结构包含多种不同大小和速度的内存形式，**内存容量越小，读写速度越快**。&#x4EE5;**`A100 GPU`**&#x4E3A;例，主要有两种类型：

> 1. **高带宽内存`HBM`**：也就是 GPU 显存，**A100** 具有 **`40-80GB`** HBM，带宽为 **`1.5-2.0TB/s`**
>
> 2. **SRAM**：位于 GPU 片上，&#x6BCF;**`108`**&#x4E2A;流式多处理器都&#x6709;**`192KB`**&#x7684; SRAM，带宽约&#x4E3A;**`19TB/s`**

二者的位置分布不同，HBM 在 VRAM 结构中，而 SRAM 在 GPU 内部。

![]()

可以看到，片上 **SRAM** &#x6BD4;**&#x20;HBM** 快一个数量级，但内存容量小很多数量级。随着计算相对于内存速度变得更快，内存 **HBM** 访问越来越成为操作瓶颈。因此，利用快速 **SRAM** 变得更加重要。

GPU 有大量线程来执行操作，称为内核 **Kernel**。每个 **Kernel** 将输入从 **HBM** 加载到寄存器和 **SRAM**，进行计算，然后将输出写入 **HBM**。根据计算和内存访问的平衡，操作可以分为**计算限制**或**内存限制**。这通常通过算术强度来衡量，即内存访问的每个字节的算术运算数量：

> 1. **计算限制**：操作所花费的时间由算术运算的数量决定，而访问 **HBM** 的时间要少得多。典型的例子是大内部维度的矩阵乘法，以及大量通道的卷积
>
> 2. **内存限制**：操作所花费的时间由内存访问次数决定，而计算所花费的时间要少得多。如逐元素操&#x4F5C;**`Activation`**、**`Dropout`**&#x548C;归约操&#x4F5C;**`Sum`**、**`Softmax`**、**`BN`**、**`LN`**

![]()

如右图所示，**masking**，**softmax&#x20;**&#x548C; **dropout&#x20;**&#x662F;占用大量时间的操作，而不是矩阵乘法，即使大部分运算量是在 matmul 中。

* **常规 Attention**

传统的 Attention 计算伪代码如右表所示，下面展示了这一过程的详细步骤：

![]()

> 1. 计算$$Q,K,V$$矩阵，放在 **HBM** 中
>
> 2. 为了计算$$QK$$注意力得分，将$$Q,K$$从 **HBM** 中取出来，写入 **SRAM**，然后计算$$S=QK^\top$$，再把$$S$$从 **SRAM** 写入 **HBM**
>
> 3. 从 **HBM** 加载$$S$$到 **SRAM**，计算$$P=\rm{Softmax}(\it{S})$$，然后再把$$P$$从 **SRAM** 写入 **HBM**
>
> 4. 从 **HBM&#x20;**&#x52A0;载$$P,V$$到 **SRAM**，计算最终的输出$$O=PV$$，然后把$$O$$写入 **HBM**

可以看到，这个过程中存在多次 **HBM** 和 **SRAM** 之间的读写操作。同时由于 Attention 的计算方式，导致中间的临时变量$$S,P$$的参数量和输入序列长度的平方成正比。因此在训练时，长序列输入在计算 Attention 时会产生更大参数量的临时变量，会占用更大显存空间，导致更多的访问消耗。也就是说，Attention 操作主要是**内存限制**问题，通信时间是制约计算效率的主要因素。

* **FlashAttention 算法**

FlashAttention 主要的思想：减少通信时间，也就是减少 IO 操作，使得计算尽可能多的访问片上的 SRAM，尽可能少的访问片外的 HBM。论文主要包含两个重要贡献：

> 1. 通过分块计算，融合多个操作，减少中间结果缓存
>
> 2. 反向传播时，重新计算中间结果，类似于梯度检查点的原理

**除去 Softmax 操作的分块计算**

在计算 Attention 主要有两个临时变量$$S,P$$，FlashAttention 的分块计算，使得不需要存储这两个临时变量，而是直接在 SRAM 计算得到部分最终结果$$O$$，从而减少了内存访问开销。这里先忽略 Softmax 操作，因为在分块计算时比较麻烦。

**例**：假设$$Q,K,V$$矩阵的大小为$$(4, 3)$$，那么 FlashAttention 的分块计算过程如右图所示，由于矩阵乘法的性质，每次分块计算得到的结果，都是最终结果矩阵中的一部分值。



![]()

**Softmax 分块计算**

下面来解决 Softmax 分块计算，Softmax 的计算公式如下：

$$\text{softmax}({x_j})=\frac{e^{x_j}}{\sum_{i=1}^ke^{x_i}}$$

但是，如果数据类型&#x4E3A;**`FP16`**，那么最大可以表示&#x4E3A;**`65504`**，因此当$$x_i=12$$时，$$e^{12} = 162754$$，超过了 **FP1**所能表示的最大值。因此需要使&#x7528;**`Safe Softmax`**&#x65B9;法来避免这个问题：

令$$m(X) = \max(x_1, x_2, \dots, x_N)$$表示全局最大值，对于输入向量$$X = [x_1, x_2, \dots, x_N]$$，Softmax 计算公式为：

$$\text{softmax}(x_i) = \frac{e^{x_i - m(X)}}{\sum_{j=1}^N e^{x_j - m(X)}}$$

也就是在计算 Softmax 之前，先对输入数据进行归一化处理。此时计算 Softmax 的流程为：

> 1. $$X = [x_1, x_2, \dots, x_N]$$
>
> 2. $$m(X)=\max(x_1, x_2, \dots, x_N)$$
>
> 3. $$p(X)=\left[e^{x_1-m(X)},\cdots,e^{x_N-m(X)}\right]$$
>
> 4. $$l(X)=\sum_{i}p(X)_i$$
>
> 5. $$\text{softmax}(X)=\frac{p(X)}{l(X)}$$

接下来看分块处理时，假设这里分为两块处理，首先需要在每个块内找到最大值$$m(X^*)$$，类似的得到$$p(X^*),l(X^*)$$：

$$X=[x_1,\cdots,x_N,x_{N+1},\cdots,x_{2N}], X^1 = [x_1, x_2, \dots, x_N], X^2 = [x_{N+1},\dots, x_{2N}]$$

$$m(X^1)=\max(x_1, x_2, \dots, x_N), m(X^2)=\max(x_{N+1},\dots, x_{2N})$$

$$p(X^1)=\left[e^{x_1-m(X^1)},\cdots,e^{x_N-m(X^1)}\right], p(X^2)=\left[e^{x_{N+1}-m(X^2)},\cdots,e^{x_{2N}-m(X^2)}\right]$$

$$l(X^1)=\sum_{i}p(X^1)_i, l(X^2)=\sum_{i}p(X^2)_i$$

然后再计算数据的全局最大值，并且更新$$p(x),l(x)$$，最后计算得到输入数据的 Softmax 值。由于全局最大值，一定是各个块内最大值中的一个，因此在更新$$p(x),l(x)$$，只需要乘以每个块最大值相对于全局最大值的差值的指数，就可以了：

$$m(X)=m(\left[X^1, X^2\right])=\max(m(X^1),m(X^2))$$

$$p(X)=\left[e^{m(X^1)-m(X)}p(X^1), e^{m(X^2)-m(X)}p(X^2)\right]$$

$$l(X)=l(\left[X^1,X^2\right])=e^{m(X^1)-m(X)}l(X^1)+e^{m(X^2)-m(X)}l(X^2)$$

$$\text{softmax}(X)=\frac{p(X)}{l(X)}$$

**Attention 分块计算**

**FlashAttention** 的完整计算流程如下：

![]()

![在红色外层循环中，遍历 K 和 V 矩阵的分块，并加载到快速的片上 SRAM 中。在每个分块中，会遍历 Q 矩阵的分块，并加载到 SRAM 中，并将结果写回到 HBM 中。]()

这里详细展开讲解：

0\.  初始化：**HBM** 的容量以 GB 为单位衡量，因此可以直接分配$$Q, K$$和$$V$$。

1. 计算行/列块大小：这里取$$\lceil
   \frac{M}{4d}\rceil$$是因为$$q,k$$和$$v$$向量是$$d$$维的，而且还需要组合输出的$$d$$维向量。所以这个大小基本上允许用$$q,k,v$$和$$o$$向量最大化 SRAM 的容量。假设$$M = 1000, d = 5$$，那么块大小为$$\frac{1000}{4\times5}= 50$$。所以一次加&#x8F7D;**`50`**&#x4E2A;$$q, k, v, o$$向量的块，这样可以减少 HBM/SRAM 之间的读/写次数。

2. 用&#x5168;**`0`**&#x521D;始化输出矩阵$$O$$，它将作为一个累加器，$$l$$也类似，它的目的是保存 **softmax** 的累积分母，即 exp 分数的总和。$$M$$初始化为$$-\infty$$，保存逐行最大分数。因为是对其进行$$\max$$运算符，因此无论第一个块的$$\max$$是什么，它肯定大于$$-\infty$$。

3. 把$$Q∈R^{N×d}$$切分成$$T_r=⌈\frac{N}{B_r}⌉$$个大小为$$B_r×d$$的块，把$$K$$和$$V$$切分成$$T_c=⌈\frac{N}{B_c}⌉$$个大小为$$B_c×d$$的块。因此每次计算$$QK^\top V$$是$$B_r×d$$的$$Q_i$$和$$d×B_c$$的$$K^\top_j$$和$$B_c×d$$的$$V_j$$，这样得到的最终大小是$$(B_r×d)×(d×B_c)×(B_c×d)=(B_r×d)$$。

4. 根据前面的计算，结果矩阵$$O$$需要切分成$$B_r×d$$的块来存放中间结果。长度为$$N$$的$$l$$和$$m$$也要切分成$$B_r$$个元素的块，用于存放这些行当前的指数累加值和当前最大值。

5. 外循环，即跨列循环，跨$$K, V$$向量

6. 将$$K_j$$和$$V_j$$块从 HBM 加载到 SRAM。在这个时间点&#x6709;**`50%`**&#x7684; SRAM 未被占用，专用于$$Q$$和$$O$$。

7. 开始跨行内部循环，即跨$$Q$$向量。

8. 把$$Q_i(B_r×d)$$和$$O_i(B_r×d)$$加载进 SRAM，同时把$$l_i(B_r)$$和$$m_i(Br)$$也加载进去。$$Q_i$$和$$O_i$$会占据另一半的显存。而$$l_i$$和$$m_i$$比较小，可以放到寄存器里。

9) 计算分块矩阵$$Q_i(B_r×d)$$和$$K_j^\top(d×B_c)$$的乘积，得到score $$S_{ij}(B_r×B_c)$$。注意这里不需要计算$$N×N$$的得分$$S$$矩阵，而只需要很小的$$S_{ij}$$。假设外层循环下标$$j=3$$，内层循环下标$$i=2$$，$$N=25$$，块大小是$$5$$，那么计算如右图所示。这里计算的 attention 得分是$$Q$$为&#x7B2C;**`6-10`**&#x4E2A; token，$$K$$是&#x7B2C;**`11-15`**&#x4E2A; token。

10) 计算$$\tilde{m}_{ij}, \tilde{l}_{ij}$$和$$\tilde{P}_{ij}$$，使用前面的公式就可以简单的得出。$$\tilde{m}_{ij}$$是逐行计算的，找到每一行的最大值。$$\tilde{P}_{ij}$$是逐点运算，把$$S_{ij}$$减去第i行的最大值$$\tilde{m}_{ij}$$，然后在计算指数。$$\tilde{l}_{ij}$$也是逐行计算，把每一行的$$\tilde{P}_{ij}$$加起来。

11) 计算$$m^\text{new}_i$$和$$l^\text{new}_i$$。如右图，$$m_i$$包含了在当前块$$j=3$$之前所有行块的最大值，即保存了$$j=1$$和$$j=2$$的绿色块&#x7B2C;**`6~10`**&#x884C;的最大值。而$$\tilde{m}_{ij}$$是上一步得到的当前黄色块的最大值。因此取两者的最大值就得到前 3 个块，绿色加黄色块共 15 列，的最大值。$$l^\text{new}_i$$的计算也是类似的，只不过求和前需要用当前的$$e^{−m^\text{new}_i}$$修正。

![]()

![]()

12. 第12步公式的加号前半部分是更新当前块$$j=3$$之前的块$$j<3$$的 softmax 值，那么当更新当前块时，这些变量有可能都会变，如最大值$$m$$等。所以第一步需要重新计算。因为之前的$$PV$$没有保存，所以我们可以用$$l$$乘以$$O$$恢复出$$PV$$。论文中是矩阵的形式，也就是$$\text{diag}(l_i)×O_i$$。恢复出来的$$R$$再乘以$$e^{m_i−m^\text{new}_i}$$就是修正后的$$PV$$，也就是$$e^{x_i−\max(x)}v_j$$。而公式中加号后半部分是当前块$$j=3$$，$$e^{\tilde{m}_i−m^\text{new}_i}$$是当前块的最大值减去$$j\le3$$所有块的最大值，这是对当前指数$$\tilde{P}_{ij}$$的修正。由于第10步中$$\tilde{P}_{ij}=e^{S_{ij}-\tilde{m}_{ij}}$$，那么这里其实就是$$e^{S_{ij}−m^\text{new}_i}$$。最后把新的$$PV$$除以新的$$l$$存到$$O$$里，只不过这里的除非也是用矩阵乘法来表示，也就是最前面的$$(\text{diag}(l^\text{new}_i))^{−1}$$。因为对角矩阵的逆就是它对角线元素的逆，也就是变成了除法。

13. 把最新的累计量$$l_r,m_r$$写回 HBM，它们的大小都是$$B_r$$。

14. 终止内层循环

15. 终止外层循环

16. 返回结果$$O$$

* **多头注意力**

要扩展到 **batch\_size > 1** 和 **num\_heads > 1** 实际上并不难。算法基本上是由单个线程块处理的。这个线程块在单个流多处理&#x5668;**`SM`**&#x4E0A;执行，例如，A100 上有 108 个这样的处理器。为了并行化计算，只需要在不同的 **SM** 上并行运行 **batch\_size \* num\_heads** 线程块。该数字与系统上可用的 **SM** 数量越接近，利用率就越高。

* **反向传播**

在反向传播中，一旦输入的$$Q、K、V$$已经加载到 **SRAM**，通过重新计算注意力矩阵$$S$$和$$P$$的值，就可以避免存储较大的中间值。通过不保存大小为$$𝑁×𝑁$$的大矩阵$$S$$和$$P$$，**FlashAttention** 产&#x751F;**`10-20`**&#x500D;内存节省，取决于序列长度，这里序列长度$$𝑁$$为线性内存，而不是二次内存。由于减少内存读写，反向传播也实现&#x4E86;**`2-4`**&#x500D;的加速。

反向传播也应用了分块。虽然反向传播在概念上比前向传播更简单，因为没有 softmax 重缩放，但实现明显更复杂。这是因为在 SRAM 中有更多的值需要保留，以便在反向传播中执行 5 个矩阵乘法，而在前向传播中只有 2 个矩阵乘法。

### 5.5.4 FlashAttention-2



### 5.5.5 FlashAttention-3

# 6. 检索增强生成 RAG

## 6.1 概述

**RAG** 近年来发展迅速，如下图所示。在大模型时代，**RAG** 的发展轨迹呈现出几个明显的阶段特征：

> 1. 最初，**RAG** 的诞生与 **Transformer** 架构的兴起相吻合，其重点是通过预训练模型引入额外的知识来增强语言模型。这一早期阶段以改进预训练技术的基础性工作为特征。
>
> 2. 随后，**ChatGPT** 的出现成为了一个关键转折点，大型语言模型展示了强大的上下文学习能力。**RAG** 的研究方向逐渐转向在推理阶段为LLM提供更优质的信息，以应对更加复杂且知识密集型的任务，这推动了 **RAG** 研究的快速发展。
>
> 3. 随着研究的深入，**RAG** 的优化不再局限于推理阶段，而是开始更多地结合 **LLM** 的微调技术。

![]()

**RAG** 的一般流程如下图所示：用户向 LLM 提出一个问题。由于 LLM 依赖于预训练数据，它最初无法提供有关最新动态的信息更新。**RAG** 通过从外部数据库中获取并整合相关知识，弥补了这一信息鸿沟。这里 **RAG** 收集了与用户查询相关的新闻文章。这些文章与原始问题结合，形成一个全面的 prompt，使 LLM 能够生成一个基于充分信息的答案。

![]()

RAG 的研究在不断发展，从时间线上来说，可以分为三个阶段：

> 1. **初级 RAG**（Naive RAG）
>
> 2. **高级 RAG**（Advanced RAG）
>
> 3. **模块化 RAG**（Modular RAG）

* **初级 RAG**

这一范式是最早的 **RAG** 实现方法，在 **ChatGPT** 出来后就被广泛采用。初级 **RAG** 遵循一个传统的流程，包括索引、检索和生成。

1. **索引 Indexing**

首先清洗和提取多种格式的原始数据，&#x5982;**`PDF`**、**`HTML`**、**`Word`**&#x548C; **`Markdown`**。这些数据随后被转换为统一的纯文本格式。为了适应语言模型的上下文限制，文本被分割成更小、更易处理的片段。接着，这些片段通过 **Enbedding** 模型编码为向量表示，并存储在向量数据库中。这一步骤对于在后续检索阶段实现高效的相似性搜索至关重要。

* **检索 Retrieval**

当接收到用户的 Query 时，**RAG** 系统会使用与索引阶段相同的 **Embedding** 模型，将查询转化为向量表示。然后，系统计算查询向量与索引语料库中各片段向量之间的相似度得分。系统根据相似度得分优先选择并检索出与查询最相关的前 **K** 个片段。这些片段随后被用作 prompt 中的扩展上下文。

* **生成 Generation**

![]()

用户的 Query 和选中的文档被整合为一个连贯的 prompt，LLM 负责基于此提示生成回答。模型的回答方式可能因任务特定的标准而有所不同，它既可以依赖其固有的参数化知识，也可以仅限于所提供文档中的信息进行回答。在持续对话的情况下，任何现有的对话历史都可以被整合到提示中，从而使模型能够有效地参与多轮对话交互。

当然，初级的 **RAG** 存在很多局限性：

> 1. **检索挑战**：检索阶段常常面临精确性和召回率的问题，导致选择的片段可能与查询不匹配或无关，同时遗漏关键信息。面对复杂问题时，仅基于原始查询的单次检索可能不足以获取足够的上下文信息。
>
> 2. **生成困难**：在生成回答时，模型可能会出现幻觉问题，即生成的内容并未得到检索上下文的支持。此外，生成结果可能还存在无关性、毒性或偏见等问题，从而影响响应的质量和可靠性。而且生成模型可能过度依赖增强信息，导致输出只是简单地复述检索到的内容，而未能提供有洞察力或综合性的信息。
>
> 3. **增强障碍**：将检索到的信息与不同任务结合可能会带来挑战，有时会导致输出内容不连贯或支离破碎。当从多个来源检索到相似信息时，过程可能会遇到冗余问题，从而导致重复的回答。确定不同段落的重要性与相关性，以及确保风格和语气的一致性，进一步增加了复杂性。

* **高级 RAG**

高&#x7EA7;**&#x20;RAG&#x20;**&#x5F15;入了特定的改进措施，以解决初级 **RAG** 的局限性。为了提升检索质量，采用了 pre-retrieval 和 post-retrieval 策略。为了解决索引问题，高级 **RAG** 通过滑动窗口方法、细粒度分段以及元数据的引入来优化其索引技术。此外，还结合了多种优化方法以简化检索过程。

1. **Pre-retrieval**

此阶段重点在于优化索引结构和原始 Query。 &#x20;

* **Post-Retrieval**

将相关上下文被检索出来，与 Query 有效整合就显得尤为重要。此阶段的主要方法包括重新排序片段和上下文压缩。 &#x20;

![]()

* **模块化 RAG**

模块化 **RAG** 架构相比前两种 **RAG** 范式，提供了更强的适应性和多功能性。它通过多种策略改进组件，例如增加用于相似性搜索的搜索模块以及通过微调优化检索器。除此之外还引入重构的 RAG 模块和重新排列的 RAG 流程等。模块化 RAG 支持组件间的顺序处理和集成的端到端训练。目前模块化 RAG 用的越来越多。

**新模块**

模块化 RAG 框架引入了额外的专用组件来增强检索和处理能力：

![]()

> 1. **Search 模块**适应特定场景，能够利用 LLM 生成的代码和查询语言直接跨多种数据源进行搜索，如搜索引擎、数据库和知识图谱
>
> 2. **Fusion 模块**通过多查询策略扩展用户查询为不同视角，利用并行向量搜索和智能重排序来揭示显性和转化性知识，解决了传统搜索的局限性
>
> 3. **Memory** **模块**利用 LLM 的记忆引导检索，创建一个无界记忆池，通过迭代自我增强使文本更贴近数据分布
>
> 4. **Routing 模块**在多样数据源中导航，为查询选择最佳路径，无论是摘要生成、特定数据库搜索还是合并不同信息流
>
> 5. **Predict 模块**通过 LLM 直接生成上下文，减少冗余和噪声，确保相关性和准确性
>
> 6. **Adapter 模块**将 RAG 适配到各种下游任务，自动化零样本输入的提示检索，并通过少量样本查询生成创建任务专用检索器

这种综合方法不仅简化了检索过程，还显著提高了检索信息的质量和相关性，以更高的精度和灵活性满足广泛的任务和查询需求。

**新模式**

模块化 RAG 通过允许模块替换或重新配置，相比于初级和高级 RAG 的 Retrieve 和 Read 的固定结构，展现出显著的适应性。此外，模块化 RAG 通过集成新模块或调整现有模块之间的交互流程，进一步扩展了这种灵活性，增强了其在不同任务中的适用性。

> **例**：&#x50CF;**`Rewrite-Retrieve-Read`**&#x8FD9;种模型，利用了 LLM 的能力，通过一个重写模块和语言模型反馈机制来优化检索查询，并不断更新重写模型，从而提升任务表现。类似的，**`Generate-Read`**&#x65B9;法直接用大语言模型生成的内容取代传统检索，&#x800C;**`Recite-Read`**&#x5219;更注重从模型权重中提取信息，增强了模型处理知识密集型任务的能力。此外，混合检索策略结合了关键词、语义和向量搜索，以应对各种不同的查询需求。还有一种方法是使&#x7528;**`sub-queries`**&#x548C;**`HyDE`**，通过关注生成答案和真实文档之间的嵌入相似性，来提高检索的相关性。

调整模块的排列和交互方式，可以动态利用一个模块的输出来增强另一个模块的功能，这体现了提升模块协同作用的好处。模块化 **RAG** 流程的灵活设计展现了自适应检索的优势，这种方法会根据不同的场景来判断是否需要进行检索，强于传统固定的检索流程。此外，灵活的架构还让 **RAG** 更容易和其他技术结合起来，比如微调或者强化学习，从而进一步提升性能。例如，微调检索器以获得更好的检索结果，微调生成器以实现更个性化的输出，或进行协作微调。

* **RAG 与微调的抉择**

在优化 LLM 的方法中，**RAG** 经常和**微调 Fine-tuning** 以及**提示工程 prompt engineering** 作比较。每种方法都有自己的特点，如下图。这里用坐标系来对比这三种方法在两个维度上的差异：**对外部知识的需求**和**对模型适配的需求**：

![]()

> 1. **提示工程**利用模型本身的能力，几乎不需要外部知识，也不需要对模型进行太多调整
>
> 2. **RAG** 适合用来完成精准的信息检索任务
>
> 3. **微调**需要慢慢内化知识，适合需要复制特定结构、风格或格式的场景

**RAG** 在动态环境中表现突出，因为它能实时更新知识，还能高效利用外部知识源，而且解释性很强。不过，它也有缺点，比如延迟较高，还涉及到数据检索中的伦理问题。相比之下，微调更静态一些，虽然更新时需要重新训练，但它可以让模型的行为和风格深度定制。不过，微调需要大量的计算资源来准备数据集和进行训练，虽然能减少幻觉问题，但面对不熟悉的数据时可能会遇到困难。

研究还表明，在针对不同主题的知识密集型任务的多次评估中，尽管无监督微调有一些改进，但 **RAG** 在处理训练中遇到的已有知识和全新的知识时，始终表现得更好。此外 LLM 很难通过无监督微调学习到新的事实信息。

选择 **RAG** 还是微调，取决于应用场景中对数据动态性、定制化程度和计算能力的具体需求。**RAG** 和微调并不是非此即彼的关系，它们可以互相补充，在不同层面上提升模型的能力。有时候，把两者结合起来使用，可能会达到最佳效果。不过，涉及 **RAG** 和微调的优化过程可能需要多次迭代，才能得到满意的结果。

## 6.2 RAG 核心组件

### 6.2.1 检索模块

* **检索源**

RAG 依赖外部知识库，检索源的类型和检索单元的粒度都会影响最终的生成结果。

**数据结构**

最开始文本是检索的主要来源。后来检索源逐渐扩展到包括半结构化数据，&#x5982;**`PDF`**&#x7B49;，和结构化数据，如`知识图谱`，**`KG`**&#x7B49;，以进一步增强效果。除了从原始外部来源检索外，最近的研究还越来越多地倾向于利用 LLM 自身生成的内容来进行检索和优化。

1. **非结构化数据**：比如文本，是最广泛使用的检索来源，主要从语料库中获取。在**开放域问答 ODQA** 任务中，主要的检索来源是维基百科的数据集，目前主流版本包&#x62EC;**`HotpotQA`**&#x548C;**`DPR`**。此外，常见的非结构化数据还包括跨语言文本以及领域特定数据，如`医学`和`法律`领域。

2. **半结构化数据**：通常指包含文本和表格信息混合的数据，比&#x5982;**`PDF`**。处理半结构化数据对传统的RAG系统来说是个挑战，主要有两个原因：

   > 1. 文本分割过程可能会无意间拆散表格，导致检索时数据损坏
   >
   > 2. 将表格融入数据会增加语义相似性搜索的复杂性

   在处理半结构化数据时有两种主流的方法：

   > 1. 利用 LLM 的代码能力，在数据库中的表格上执行 **Text-2-SQL** 查询，比&#x5982;**`TableGPT`**
   >
   > 2. 将表格转换为文本格式，然后用基于文本的方法进行进一步分析

   但是目前这两种方法都有一定的弊端，还需要进一步探索。

3. **结构化数据**：比如知识图谱 **KGs**，通常是经过验证的，能够提供更精确的信息。

   > **例**：**`KnowledGPT`**&#x901A;过生成知识库搜索 Query 并将知识存储在个性化数据库中，增强了 RAG 模型的知识丰富性。**`G-Retriever`**&#x9488;对 LLM 在理解和回答基于文本图问题上的局限性，将图神经网络 GNN、LLM 和 RAG 结合起来，通过 LLM 的 soft prompting 提升对图的理解和问答能力，并利用 **PCST**（**P**rize-**C**ollecting **S**teiner **T**ree）优化问题实现针对性的图检索。

然而，结构化数据的构建、验证和维护结构化数据库需要额外的努力，这与非结构化数据不同。

* **LLM 生成内容**：为了解决RAG中外部辅助信息的局限性，一些研究聚焦于挖掘大语言模型的内部知识。

  > **例**：**`SKR`**&#x5C06;问题分类为已知或未知，有选择地应用检索增强。**`GenRead`**&#x76F4;接用 LLM 生成器取代了检索器，发现由于 LLM 生成的上下文与因果语言建模的预训练目标更一致，一般会包含更准确的答案。**`Selfmem`**&#x901A;过检索增强生成器迭代创建一个无限制的记忆池，并利用记忆选择器挑选出与原始问题形成**对偶问题**的输出，从而实现生成模型的自我增强。

**检索粒度**

除了检索源的数据格式外，另一个重要因素是检索单元的粒度。

> * **粗粒度的检索单元**可以为问题提供更多相关的信息，但可能也包含冗余内容，这会在下游任务中分散检索器和语言模型的注意力
>
> * **细粒度的检索单元**增加了检索的负担，但也不能完全保证语义的完整性和满足所需的知识需求

在推理过程中选择适当的检索单元粒度，可以提升密集检索器的性能以及下游任务的效果。

1. 在**文本**中，检索单元的粒度从细到粗包括：`词元`**` Token`**、`短语`**` Phrase`**、`句子`**` Sentence`**、`命题`**` Proposition`**、`段落块`**` Chunks`**、`文档`**` Document`**

   > **例**：**`DenseX`**&#x63D0;出了以命题作为检索单元的概念。命题被定义为文本中的原子表达，每个命题封装了一个独特的事实片段，并以简洁、自包含的自然语言格式呈现。这种方法旨在提高检索的精确性和相关性。

2. 在**知识图谱**中，检索单元的粒度包括：`实体`**` Entity`**、`三元组 `**`Triplet`**、`子图 `**`sub-Graph`**

这里检索单元的粒度还可以根据下游任务进行调整。

* **索引 Indexing**

在索引阶段，文档会被处理、分割并转化为嵌入向量 Embedding，然后存储到向量数据库中。索引构建的质量决定了在检索阶段能否获取到正确的上下文。

**分块**

最常见的方法是将文档按照固定数量的 Token 分割成块，例&#x5982;**`100`**、**`256`**、**`512`**&#x4E2A;Token。

> * 较大的块能够捕获更多上下文信息，但也会引入更多噪声，同时需要更长的处理时间和更高的成本
>
> * 较小的块虽然噪声较少，但可能无法充分传达必要的上下文信息

分块可能导致句子被截断，因此出现了递归分割和滑动窗口等优化方法，这些方法通过合并多个检索过程中的全局相关信息，实现了分层检索。然而，这些方法仍然难以在语义完整性和上下文长度之间取得平衡。因此，一些方法&#x50CF;**`Small2Big`**&#x88AB;提出，以小范围的句子作为检索单元，并提供前后相邻的大范围的句子作为上下文输入给 LLM。

**附加元数据**

分块可以附加元数据信息，例如`页码`、`文件名`、`作者`、`分类`和`时间戳`等。随后可以根据这些元数据对检索进行过滤，从而缩小检索范围。在检索过程中为文档时间戳分配不同的权重，可以实现时间感知型 **RAG**，确保知识的新鲜度并避免使用过时的信息。除了从原始文档中提取元数据外，还可以人工构建。例如，添加段落的摘要，或者引入假设性问题。这种方法也被称为反向 HyDE 。具体来说，利用 LLM 生成可以由文档回答的问题，然后在检索过程中计算原始问题与假设问题之间的相似性，从而缩小问题与答案之间的语义差距。

**结构化索引**

增强信息检索的一种有效方法是为文档建立分层结构。通过构建这种结构，RAG系统可以加速相关数据的检索和处理。

1. **分层索引结构**：文件以父子关系排列，分块 chunks 与它们相关联。每个节点存储数据摘要，有助于快速遍历数据，并帮助 RAG 系统确定需要提取哪些分块。这种方法还可以缓解因分块提取问题而导致的幻觉现象。

2. **知识图谱索引**：利用知识图谱构建文档的分层结构有助于保持一致性。这可以表示不同概念和实体之间的联系，显著减少了幻觉的可能性。另一个优势是将信息检索过程转化为 LLM 能够理解的指令，从而提高知识检索的准确性，并使 LLM 能够生成上下文连贯的响应，提升 RAG 系统的整体效率。

   > **例**：为了捕捉文档内容与结构之间的逻辑关系，**`KGP`**&#x63D0;出了一种使用知识图谱在多个文档之间构建索引的方法。该知识图谱由节点（表示文档中的段落或结构，如页面和表格）和边（表示段落之间的语义/词汇相似性或文档结构内的关系）组成，有效解决了多文档环境中的知识检索和推理问题。

* **用户问题 Query**

初级 RAG 的主要挑战之一是其直接依赖用户的原始查询作为检索基础。构建一个精确且清晰的问题很困难，不恰当的查询会导致检索效果不佳。有时问题本身复杂，语言表达也不够条理清晰。另一个难点在于语言的复杂性和歧义性。语言模型在处理专业术语或多义缩写时常常遇到困难。例如，它们可能无法判&#x65AD;**`LLM`**&#x662F;指`大语言模型`还是法律背景下的`法学硕士`。

**查询扩展 Query Expansion**

将单个查询扩展为多个查询可以丰富查询内容，提供更多上下文以弥补具体细节的缺失，从而确保生成答案的最佳相关性。

**多查询** **Multi-Query**

通过提示工程利用 LLM 扩展查询，这些查询可以并行执行。查询扩展并非随机，而是经过精心设计。

**子查询** **Sub-Query**

子问题规划的过程是生成必要的子问题，结合这些子问题可以为原始问题提供上下文并完整回答。这一过程在原则上类似于查询扩展。具体而言，复杂问题可以通过**从简单到复杂**的提示方法分解为一系列更简单的子问题。

**Chain-of-Verification, CoVe**

扩展后的查询通过大语言模型进行验证，以减少幻觉现象。经过验证的扩展查询通常具有更高的可靠性。

**查询转换 Query Transformation**

核心概念是基于转换后的查询检索片段，而非用户的原始查询。 &#x20;

**查询重写 Query Rewrite**

原始查询并不总是适合 LLM 检索，尤其是在现实场景中。因此，可以提示 LLM 对查询进行重写。除了使用 LLM 进行查询重写外，还可以使用专门的小型语言模型，例如 **RRR**（**R**ewrite-**R**etrieve-**R**ead）。在淘宝中实现的查询重写方法，称为 BEQUE，显著提高了长尾查询的召回效果，从而提升了 GMV。 &#x20;

**查询优化**

另一种查询转换方法是使用提示工程让 LLM 基于原始查询生成新查询以用于后续检索。HyDE 构建假设文档，即对原始查询的假定答案，其重点是从答案到答案的嵌入相似性，而非问题或查询的嵌入相似性。使用`回退提示法`，将原始查询抽象化以生成高层次的概念问题（或回退问题）。在RAG系统中，回退问题和原始查询都被用于检索，两者的结果都被用作语言模型生成答案的基础。 &#x20;

**查询路由 Query Routing**

根据不同的查询，将其路由到不同的 RAG 管道，适用于为多样化场景设计的多功能 RAG 系统。 &#x20;

**元数据路由/过滤 Metadata Router/Filter**

第一步是从查询中提取关键词或者实体，然后根据关键词和片段中的元数据进行过滤，以缩小搜索范围。&#x20;

**语义路由 Semantic Router**

另一种路由方法是利用查询的语义信息。具体方法可以看[详细资料](https://github.com/aurelio-labs/semantic-router)。

当然，也可以采用混合路由方法，结合语义和元数据的方法以增强查询路由效果。

* **嵌入 Embedding**

在 RAG 中，检索是通过计算问题和文档片段嵌入之间的相似性来实现的，例如余弦相似性，其中嵌入模型的语义表示能力起着关键作用。这主要包括稀疏编码器（如 BM25）和密集检索器（如基于 BERT 架构的预训练语言模型）。最近的一些工作引入了一些更加优秀的嵌入模型，例&#x5982;**`AngIE`**、**`Voyage`**、**`BGE`**&#x7B49;，这些模型得益于多任务指令微调。Hugging Face &#x7684;**`MTEB`**&#x6392;行榜评估了嵌入模型&#x5728;**`8`**&#x4E2A;任务上的表现，涵&#x76D6;**`58`**&#x4E2A;数据集。此外，**`C-MTEB`**&#x4E13;注于中文能力，覆&#x76D6;**`6`**&#x4E2A;任务&#x548C;**`35`**&#x4E2A;数据集。对于**使用哪种嵌入模型**这个问题，并没有放之四海而皆准的答案。然而，某些特定模型更适合特定的使用场景。

**混合检索 Mix/Hybrid Retrieval**

稀疏和密集嵌入方法捕获不同的相关性特征，并可以通过利用互补的相关性信息相互受益。例如，稀疏检索模型可以用于提供初始搜索结果以训练密集检索模型。此外，预训练语言模型（PLMs）可以用于学习词权重，从而增强稀疏检索。具体而言，研究表明，稀疏检索模型可以增强密集检索模型的零样本检索能力，并帮助密集检索器处理包含罕见实体的查询，从而提高鲁棒性。

**微调嵌入模型 Fine-tuning Embedding Model**

当上下文与预训练语料库显著偏离时，特别是在医疗、法律实践等高度专业化的领域，这些领域充满专有术语，针对自己的领域数据集微调嵌入模型变得至关重要，以缓解这种差异。 &#x20;

除了补充领域知识外，微调的另一个目的是对齐检索器和生成器。

> **例**：
>
> * 使用 LLM 的结果作为微调的监督信号，这种方法被称为 **LSR**（**L**M-**S**upervised **R**etriever）。
>
> * **`PROMPTAGATOR`**&#x5229;用 LLM 作为少样本查询生成器，创建特定任务的检索器，解决了监督微调中的挑战，尤其是在数据稀缺的领域。
>
> * **`LLM-Embedder`**&#x5229;用 LLM 为多个下游任务生成奖励信号。检索器通过两种监督信号进行微调：数据集的硬标签和来自LLM的软奖励。这种双信号方法促进了更有效的微调过程，使嵌入模型适应多样化的下游应用。
>
> * **`REPLUG`**&#x5229;用检索器和 LLM 计算检索文档的概率分布，然后通过计算KL散度进行监督训练。这种简单而有效的训练方法通过使用语言模型作为监督信号提升了检索模型的性能，无需特定的交叉注意力机制。此外，受 RLHF 启发，利用基于语言模型的反馈通过强化学习增强检索器。

* **适配器 Adapter**

微调模型可能会带来一些挑战，例如通过 API 集成功能或解决因本地计算资源有限而产生的限制。因此，一些方法选择引入外部适配器以辅助对齐。

> **例**：
>
> * **`UPRISE`**&#x8BAD;练了一个轻量级的提示检索器，能够从预构建的提示池中自动检索适合给定零样本任务输入的提示。
>
> * **`AAR`**（**A**ugmentation-**A**dapted **R**etriver）引入了一种通用适配器，旨在适应多个下游任务。
>
> * **`PRCA`**&#x6DFB;加了一个可插拔的奖励驱动上下文适配器，以提升特定任务的性能。
>
> * **`BGM`**&#x56FA;定了检索器和 LLM，并在两者之间训练了一个桥接&#x7684;**`Seq2Seq`**&#x6A21;型。该桥接模型旨在将检索到的信息转换为 LLM 可以有效处理的格式，使其不仅能重新排序，还能动态地为每个查询选择段落，如重复可能采用更高级的策略。
>
> * **`PKG`**&#x63D0;出了一种创新方法，通过指令微调将知识整合到白盒模型中。在该方法中，检索器模块被直接替换，以根据查询生成相关文档。这种方法有助于解决微调过程中遇到的困难，并提升模型性能。

### 6.2.2 生成模块

在检索完成后，直接将所有检索到的信息输入 LLM 以回答问题是不推荐的做法。以下将从两个角度介绍调整方法：调整检索内容和调整 LLM。

* **上下文管理**

冗余信息可能干扰 LLM 的最终生成结果，而过长的上下文还会导致 LLM 出现中间信息丢失的问题。与人类似，LLM 往往只关注长文本的开头和结尾，而忽略中间部分。因此，在 RAG 系统中，通常需要对检索到的内容进行进一步处理。

**重排序 Reranking**

重排序本质上是对文档片段重新排序，以突出最相关的结果，从而有效减少整体文档池的规模。这种方法在信息检索中具有双重作用，既作为增强器又作为过滤器，为更精确的语言模型处理提供优化后的输入。重排序可以通过基于规则的方法实现，依赖预定义指标，如`多样性`、`相关性`&#x548C;**`MRR`**，也可以通过基于模型的方法实现，例如 BERT 系列的编码器-解码器模&#x578B;**`SpanBERT`**、专门的重排序模&#x578B;**`Cohere rerank`**&#x6216;**`bge-reranker-large`**，以及通用的大语言模型`GPT`。

**上下文选择/压缩 Context Selection/Compression**

在 RAG 过程中，一个常见的误解是认为检索尽可能多的相关文档并将它们拼接成一个长的检索提示是有益的。然而，过多的上下文可能会引入更多噪声，削弱 LLM 对关键信息的感知能力。因此通常会对上下文进行选择，以减少不相关的上下文：

> **例**：
>
> **`LLMLingua`**&#x5229;用小型语言模型 SLM，&#x5982;**`GPT-2 Small`**&#x6216;**`LLaMA-7B`**，检测并移除不重要的 Token，将其转换为人类难以理解但 LLM 能够很好理解的形式。这种方法提供了一种直接且实用的提示压缩方式，在无需额外训练 LLM 的情况下平衡了语言完整性和压缩率。
>
> **`RECOMP`**&#x91C7;用了一种对比学习的方法训练信息压缩器。每个训练数据点包含一个正样本和五个负样本，编码器在整个过程中通过对比损失进行训练。 &#x20;

除了压缩上下文外，减少文档数量也有助于提高模型答案的准确性：

> * 一种方法是`过滤-重排序`范式，结合了 LLM 和 SLM 的优势。其中 SLM 充当过滤器，而 LLM 则负责重新排序。一些工作表明让 LLM 对 SLM 识别出的困难样本进行重新排序，可以显著提升各种**信息抽取 IE** 任务的效果。
>
> * 另一种简单而有效的方法是让 LLM 在生成最终答案之前评估检索到的内容。这使得 LLM 可以通过自我批评过滤掉相关性较差的文档。例如，&#x5728;**`Chatlaw`**&#x4E2D;，LLM 被提示对引用的法律条款进行自我建议，以评估其相关性。

* **LLM 微调**

基于场景和数据特征对 LLM 进行针对性微调可以获得更好的效果。这也是使用本地部署 LLM 的最大优势之一。当 LLM 在特定领域缺乏数据时，可以通过微调向其提供额外的知识。微调的另一个好处是可以调整模型的输入和输出。例如，可以让 LLM 适应特定的数据格式，并按照指示生成某种风格的响应。对于涉及结构化数据的检索任务，一些框架实施三阶段训练方案，&#x5982;**`SANTA`**，以有效封装结构和语义上的细微差别。其中第一阶段聚焦于检索器，利用对比学习来优化用户问题和文档嵌入。

通过强化学习将 LLM 的输出与人类或检索器的偏好对齐也是一种方法。例如，手动标注最终生成的答案，然后通过强化学习提供反馈。除了与人类偏好对齐外，还可以与经过微调的模型和检索器的偏好对齐。当无法访问强大的专有模型或更大参数量的开源模型时，一种简单而有效的方法是对更强大的模型进行蒸馏，&#x5982;**`GPT-4`**。LLM 的微调还可以与检索器的微调协调进行，以对齐偏好。&#x5982;**`RA-DIT`**，它通过KL散度对齐检索器和生成器之间的评分函数。

### 6.2.3 RAG 范式

根据检索器如何增强生成器，可以将 RAG 范式分为四类：

> * **基于 Query 的 RAG**
>
> * **基于潜在表示的 RAG**
>
> * **基于 Logit 的 RAG**
>
> * **推测型 RAG**

* **基于 Query 的 RAG**

受启发于提示增强的思想，基于 Query 的 RAG 将用户的问题与检索到的信息无缝结合，并直接输入到生成器的初始阶段。这种方法在 RAG 应用中非常普遍。在检索完成后，获取的内容会与用户的原始查询合并，形成一个复合输入，然后由生成器处理以生成响应。基于查询的 RAG 被广泛应用于多种任务：

![]()

> * **文本生成**：
>
>   * **`REALM`**&#x4F7F;用`双`**`BERT`**`框架`来简化知识检索和整合，将预训练模型与知识提取器结合起来。
>
>   * **`Self-RAG`**&#x5F15;入一个批评模块，用于判断是否需要进行检索。
>
>   * **`REPLUG`**&#x901A;过 API 调用 LLM，将语言模型视为黑箱，并有效地将相关外部文档整合到查询中。
>
>   * **`In-Context RALM`**&#x4F7F;&#x7528;**`BM25`**&#x8FDB;行文档检索，并训练一个预测性重排序器，对排名靠前的文档重新排序并整合。 &#x20;
>
> * **代码领域**：将文本或代码中的上下文信息整合到提示中，从而提高了下游任务的效果。 &#x20;
>
> * **知识库问答**：一些研究表明，结合检索和语言模型在知识库问答中具有显著效果。例如，**`Uni-Parser`**、**`RNG-KBQA`**&#x548C;**`ECBRF`**&#x901A;过将查询和检索到的信息合并到提示中，有效提升了问答系统的性能和准确性。 &#x20;
>
> * **AI for Science**：**`Chat-Orthopedist`**&#x5E2E;助用户进行共享决策，通过将检索到的数据整合到模型提示中，提高了大语言模型的有效性和信息精度。 &#x20;
>
> * **图像生成任务**：
>
>   * **`RetrieveGAN`**&#x901A;过将检索到的数据，如`选定的图像块及其边界框`，整合到生成器的输入阶段，提升了生成图像的相关性和精度。
>
>   * **`IC-GAN`**&#x901A;过将噪声向量与实例特征连接起来，调节生成图像的具体条件和细节。 &#x20;
>
> * **3D 生成**：**`RetDream`**&#x9996;先利用 CLIP 检索相关的 3D 资产，然后在输入阶段将检索到的内容与用户输入合并。 &#x20;

基于 Query 的 RAG 通常与大语言模型生成器结合使用，提供了模块化的灵活性，能够快速集成预训练组件以实现快速部署。在此框架下，提示设计对于利用检索到的数据至关重要。

* **基于潜在表示的 RAG**

在基于潜在表示的 RAG 框架中，检索到的对象以潜在表示的形式被整合到生成模型中。这增强了模型的理解能力，并提升了生成内容的质量。基于潜在表示的 RAG 具有多模态和任务适应性，能够融合检索器和生成器的隐藏状态，但需要额外的训练来对齐潜在空间。它使得开发能够无缝整合检索信息的复杂算法成为可能。



![]()

> * **文本领域**：**`FiD`**&#x548C;**`RETRO`**&#x662F;基于潜在表示的 RAG 的两种经典结构，许多后续工作都基于它们进行了改进：
>
>   * **FiD** 使用不同的编码器分别处理每个检索到的段落及其标题和查询，然后将生成的潜在表示合并，由单个解码器解码以生成最终输出。 &#x20;
>
>   * **RETRO&#x20;**&#x4E3A;每个分割的子查询检索相关信息，并引入了一种名为分块交叉注意力 **CCA**（**C**hunked **C**ross-**A**ttention）的新模块，用于将检索到的内容与每个子查询的标记进行整合。此外，在基于潜在表示的 RAG 范畴内还有其他值得注意的新结构。一些工作在 Transformer 块中集成了 k 近邻搜索 kNN，允许输入分块，理论上解决了 Transformer 模型长期以来受到批评的上下文长度限制问题。
>
> * **代码与科学领域**：**FiD** 在这些领域得到了广泛应用，涵盖了多个**与代码相关的任务**以及 **AI for Science**。
>
> * **图像领域**：一些工作使用交叉注意力机制，通过整合潜在表示来融合检索结果。除此之外还可以搭建文本-图像仿射组合模块 **ACM**（**A**ffine **C**ombination **M**odule），直接连接隐藏特征。
>
> * **知识领域**：一些工作采用了 **FiD** 及其衍生方法用于下游任务：
>
>   * **`EaE`**&#x901A;过实体特定参数化增强生成器的理解能力。 &#x20;
>
>   * **`TOME`**&#x5219;转向对提及的细微编码，优先考虑提及的粒度而非仅关注实体表示。
>
> * **3D 生成领域**：
>
>   * **`ReMoDiffuse`**&#x5F15;入了一种语义调制注意力机制，能够根据文本描述更准确地生成相应的 3D 动作。
>
>   * **`AMD`**&#x901A;过将原始扩散过程与参考扩散过程融合，实现了从文本到 3D 动作的高效转换。
>
> * **音频领域**：
>
>   * 一些工作使用 LLM，在注意力模块中结合密集特征编码以指导音频字幕的生成。
>
>   * **`Re-AudioLDM`**&#x4F7F;用不同的编码器从文本和音频中提取深度特征，然后将其整合到其潜在扩散模型**LDM**（**L**atent **D**iffusion **M**odel）的注意力机制中。
>
> * **视频字幕生成**：
>
>   * **`R-ConvED`**&#x4F7F;用卷积编码器-解码器网络，通过注意力机制处理检索到的视频-句子对，生成隐藏状态以生成字幕。
>
>   * **`CARE`**&#x5F15;入概念检测器，生成概念概率，并将概念表示整合到混合注意力机制中。
>
>   * **`EgoInstructor`**&#x4F7F;用门控交叉注意力合并文本和视频特征，提高了第一视角视频字幕的相关性和连贯性。

* **基于 Logit 的 RAG**

在基于 logit 的 RAG 中，生成模型在解码过程中通过 logit 整合检索信息。通常，logit 会通过简单的求和或模型计算逐步生成的概率。基于 logit 的 RAG 利用历史数据推断当前状态，并在 logit 层面融合信息，非常适合序列生成任务。它专注于生成器的训练，并允许开发利用概率分布的新方法，为未来任务提供支持。



![]()

> * **文本领域**：
>
>   * **`kNN-LM`**&#x53CA;其变体在每个解码步骤中将语言模型的概率与相似前缀的检索距离概率相结合。
>
>   * **`TRIME`**&#x548C;**`NPM`**&#x662F;传统 **kNN-LM** 方法的激进演进版本，使用本地数据库中高度对齐的标记作为输出，尤其在长尾分布场景中显著提升了性能。
>
> * **其他模态**：超出文本领域，代码和图像等其他模态也利用了基于 logit 的 RAG。
>
> * **代码领域**：
>
>   * 一些工作采用了 kNN 的概念以增强最终输出的控制，从而实现了更优的性能。
>
>   * **`EDITSUM`**&#x901A;过在 logit 层面整合原型摘要，提升了代码摘要的质量。
>
> * **图像字幕生成**：**`MA`**&#x76F4;接应用 kNN-LM 框架解决图像字幕问题，取得了良好的效果。

* **推测型 RAG**

推测型 RAG 用检索代替纯生成，从而节省资源并加速响应速度。 &#x20;

> * **`REST`**&#x5C06;推测解码中的小模型替换为检索，从而实现草稿的生成。 &#x20;
>
> * **`GPTCache`**&#x901A;过构建语义缓存来存储 LLM 的响应，解决了使用 LLM API 时高延迟的问题。 &#x20;
>
> * **`COG`**&#x5C06;文本生成过程分解为一系列复制粘贴操作，从文档中检索单词或短语，而非进行生成。 &#x20;

![]()

推测型 RAG 目前主要适用于序列数据。它将生成器与检索器解耦，使得可以直接使用预训练模型作为组件。在此范式下可以探索更广泛的策略，以有效利用检索到的内容。

## 6.3 RAG 系统优化

一般来说，提升系统性能主要&#x6709;**`5`**&#x4E2A;方面：`输入`、`检索器`、`生成器`、`结果`以及`整个流程`。

### 6.3.1 输入优化

输入最初被送入检索器，对检索阶段的最终结果有显著影响。一般有两种输入增强方法：**查询转换**和**数据增强**。

* **Query** **转换**

Query 转换通过修改输入查询，查询转换可以提升检索结果的质量。 &#x20;

> **例**：
>
> 1. **`Query2doc`**&#x548C;**`HyDE`**&#x4F7F;用原始查询生成一个伪文档，随后将其用作检索查询。伪文档包含更丰富的相关信息，有助于检索到更准确的结果。 &#x20;
>
> 2. **`TOC`**&#x5229;用检索到的内容将模糊查询分解为多个清晰的子查询，这些子查询被发送到生成器并聚合以生成最终结果。 &#x20;
>
> 3. **`RQ-RAG`**&#x5BF9;于复杂或模糊的查询，将其分解为清晰的子查询，进行细粒度检索，并综合响应以提供针对原始查询的连贯答案。 &#x20;

* **数据增强**

数据增强在检索之前改进数据，包括删除无关信息、消除歧义、更新过时文档、合成新数据等技术。 &#x20;

> **例**：
>
> 1. **`Make-An-Audio`**&#x4F7F;用字幕生成和音频-文本检索为无语言音频生成字幕，以缓解数据稀疏性问题，并添加随机概念音频以改进原始音频。 &#x20;
>
> 2. **`LESS`**&#x901A;过分析梯度信息优化下游任务的数据集选择，旨在增强模型对指令提示的响应性能。 &#x20;
>
> 3. **`ReACC`**&#x4F7F;用数据增强，包括重命名和插入无效代码等，对代码检索模型进行预训练。 &#x20;
>
> 4. **`Telco-RAG`**&#x901A;过应用`3GPP 规范词汇表`提高检索准确性，并使用路由模块将这些词汇与用户查询匹配。

### 6.3.2 检索模块优化

在 RAG 系统中，检索内容的质量决定了输入生成器的信息质量。较低的内容质量会增加模型产生幻觉或其他性能下降的风险。提高检索效率的有效方法主要有以下几种：

* **递归检索**

递归检索是通过多次搜索来获取更丰富、更高质量的内容。 &#x20;

> **例**：
>
> 1. **`ReACT`**&#x4F7F;用思维链 **CoT**（**C**hain-**o**f-**T**hought）将 Query 分解为多个部分，进行递归检索，从而提供更丰富的信息。 &#x20;
>
> 2. **`RATP`**&#x4F7F;用蒙特卡洛树搜索进行模拟，选择最佳的检索内容，并将其模板化后传递给生成器以生成输出。

* **片段优化**

片段优化是指调整片段大小以改善检索结果。 &#x20;

> **例**：
>
> 1. **`LlamaIndex`**&#x5305;含了一系列片段优化方法，其中之一基于`从小到大`的原则。其核心思想是定位更细粒度的内容，但返回更丰富的信息。例如，句子窗口检索 Sentence-Window Retrieval 提取小文本片段，并返回围绕该片段的相关句子窗口。&#x20;
>
> 2. **`RAPTOR`**&#x4E3A;了解决上下文信息不足的问题，使用递归嵌入、聚类和文本片段摘要，直到进一步聚类不可行为止，从而构建一个多层级的树状结构。 &#x20;
>
> 3. **`Prompt-RAG`**&#x901A;过预先生成目录来提高检索准确性，使模型能够根据查询自主选择相关章节。

* **检索器微调**

作为 RAG 系统核心的检索器，依赖于一个高效的嵌入模型来表示相关内容并为生成器提供输入，从而提升系统性能。具有强大表达能力的嵌入模型可以通过领域特定或任务相关的数据进行微调，以提升在目标领域的表现。

> **例**： &#x20;
>
> 1. **`REPLUG`**&#x5C06; LLM 视为黑箱，并根据最终结果更新检索器模型。 &#x20;
>
> 2. **`APICoder`**&#x4F7F;用 Python 文件、API 名称、签名和描述对检索器进行微调。 &#x20;
>
> 3. **`EDITSUM`**&#x5BF9;检索器进行微调，以减少检索后摘要之间的杰卡德距离。 &#x20;
>
> 4. **`SYNCHROMESH`**&#x5728;损失函数中加入抽象语法树 AST 的树距离，并使用目标相似性调优 **TST**（**T**arget **S**imilarity **T**uning）对检索器进行微调。 &#x20;
>
> 5. **`R-ConvED`**&#x4F7F;用与生成器相同的数据对检索器进行微调。 &#x20;

* **混合检索**

混合检索指的是同时采用多种不同的检索方法，或从多个不同来源提取信息。 &#x20;

> **例**：
>
> 1. **`RAP-Gen`**、**`BlendedRAG`**&#x548C;**`ReACC`**&#x540C;时使用密集检索器和稀疏检索器以提高检索质量。 &#x20;
>
> 2. **`Rencos`**&#x4F7F;用稀疏检索器在语法层面检索相似代码片段，并使用密集检索器在语义层面检索相似代码片段。 &#x20;
>
> 3. **`BASHEXPLAINER`**&#x9996;先使用密集检索器捕获语义信息，然后使用稀疏检索器获取词汇信息。 &#x20;
>
> 4. **`RetDream`**&#x9996;先通过文本进行检索，然后通过图像嵌入进行检索。 &#x20;
>
> 5. **`CRAG`**&#x5F15;入了一个检索评估器，用于衡量文档与查询的相关性，并根据置信度提示三种检索响应：如果准确则直接使用结果进行知识优化，如果不准确则进行网络搜索，对于模糊情况则采用混合方法。   &#x20;
>
> 6. **`UniMSRAG`**&#x5F15;入了一种新型 Token，称&#x4E3A;**`Acting Token`**，用于决定从哪个来源检索信息。 &#x20;

* **重排序**

重排序技术指的是对检索到的内容重新排序，以实现更高的多样性和更好的结果。 &#x20;

> **例**：
>
> 1. **`Re2G`**&#x5728;传统检索器之后应用了一个重排序模型，以减少因将文本压缩为向量而导致的信息损失影响。 &#x20;
>
> 2. **`AceCoder`**&#x4F7F;用选择器对检索到的程序进行重排序，减少冗余程序并获得多样化的检索结果。 &#x20;
>
> 3. **`XRICL`**&#x5728;检索后使用基于蒸馏的示例重排序器。 &#x20;
>
> 4. **`Rangan`**&#x4F7F;用量化影响度量 **QIM**（**Q**uantized **I**nfluence **M**easure），评估查询与参考之间的统计偏差，以评估数据子集的相似性并对检索结果进行重排序。 &#x20;
>
> 5. **`UDAPDR`**&#x4F7F;用 LLM 高效生成合成查询，用于训练领域特定的重排序器，并通过多教师知识蒸馏开发出一个连贯的检索器。 &#x20;
>
> 6. **`LLM-R`**&#x901A;过使用静态 LLM 进行文档排名和奖励模型训练来迭代优化其检索器，并辅以知识蒸馏。每次训练周期逐步改进检索器，从而实现渐进式优化。 &#x20;

* **检索转换**

检索转换涉及对检索到的内容进行改写，以更好地激活生成器的潜力，从而改善输出效果。 &#x20;

> **例**：
>
> 1. **`FILCO`**&#x9AD8;效地从检索到的文本中清除无关内容，仅保留相关支持内容，简化生成器的任务并促进准确答案预测。 &#x20;
>
> 2. **`FiD-Light`**&#x6700;初使用编码器将检索到的内容转换为向量，然后对其进行压缩，从而显著减少了延迟时间。 &#x20;
>
> 3. **`RRR`**&#x901A;过模板将当前查询与每轮中的前 k 个文档整合，随后通过预训练的 LLM 对其进行重构。

### 6.3.3 生成模块优化

在 RAG 系统中，生成器的质量通常决定了最终输出结果的质量。因此，生成器的能力决定了整个 RAG 系统效果的上限。

* **提示工程** **Prompt Engineering**

专注于提升大语言模型输出质量的提示工程技术，如提示压缩、Stepback Prompt、Active Prompt、思维链提示 Chain of Thought Prompt 等，都适用于 RAG 系统中的大语言模型生成器。&#x20;

> **例**：&#x20;
>
> 1. **`LLMLingua`**&#x4F7F;用一个小模型压缩查询的整体长度，以加速模型推理，减轻无关信息对模型的负面影响，并缓解中间丢失现象。 &#x20;
>
> 2. **`ReMoDiffuse`**&#x4F7F;用 ChatGPT 将复杂描述分解为解剖学文本脚本。 &#x20;
>
> 3. **`ASAP`**&#x5C06;包含输入代码、函数定义、分析结果和相应注释的示例元组整合到提示中，以获得更好的结果。 &#x20;
>
> 4. **`CEDAR`**&#x4F7F;用设计好的提示模板，将代码演示、查询和自然语言指令组织成提示。 &#x20;
>
> 5. **`XRICL`**&#x5229;用思维链技术，在跨语言语义解析和推理中增加翻译对作为中间步骤。 &#x20;
>
> 6. **`ACTIVERAG`**&#x4F7F;用认知纽带机制校准大语言模型的内在认知，并在答案生成中应用思维链提示。 &#x20;
>
> 7. **`Make-An-Audio`**&#x80FD;够使用其他模态作为输入，为后续过程提供更丰富的信息。

* **解码微调 Decoding Tuning**

解码微调涉及通过微调超参数来增强生成器控制，例如增加多样性、限制输出词汇表等调整。 &#x20;

> **例**：&#x20;
>
> 1. **`InferFix`** 通过调整解码器中的温度参数来平衡结果的多样性和质量。 &#x20;
>
> 2. **`SYNCHROMESH`** 通过实现一个补全引擎限制解码器的输出词汇表，从而消除实现错误。

* **生成器微调**

生成器的微调可以增强模型拥有更精确领域知识的能力，或更好地与检索器适配。 &#x20;

> **例**：&#x20;
>
> 1. **`RETRO`** 固定检索器的参数，并在生成器中使用分块交叉注意力机制，结合查询和检索器的内容。 &#x20;
>
> 2. **`APICoder`** 使用打乱后的新文件，并结合 API 信息和代码块对生成&#x5668;**`CODEGEN-MONO 350M`**&#x8FDB;行微调。 &#x20;
>
> 3. **`CARE`&#x20;**&#x4F7F;用图像、音频和视频-文本对训练编码器，然后微调解码器，同时减少字幕和概念检测损失，同时保持编码器和检索器固定。 &#x20;
>
> 4. **`Animate-AStory`** 使用图像数据优化视频生成器，然后微调一个 LoRA 适配器以捕捉给定角色的外观细节。 &#x20;
>
> 5. **`RetDream`** 使用渲染图像对一个 LoRA 适配器进行微调。

### 6.3.4 结果优化

在许多场景中，RAG 的结果可能无法达到预期效果，而一些结果增强技术可以帮助缓解这一问题。

* **输出重写**

输出重写指的是在某些场景下对生成器生成的内容进行改写，以满足下游任务的需求。

> **例**：
>
> **`SARGAM`**&#x5728;代码相关任务中通过使用一种特殊的 Transformer，并结合删除、占位符和插入分类器来优化输出，从而更好地与实际代码上下文对齐。
>
> **`Ring`**&#x901A;过对生成器生成的每个标记的对数概率平均值对候选结果进行重排序，从而获得多样化结果。
>
> **`CBRKBQA`**&#x901A;过将生成的关系与知识图谱中查询实体局部邻域中的关系对齐来修正结果。

### 6.3.5 RAG 链路优化

一般优化 RAG 链路有以下几种方法：

* **自适应检索**

一些关于 RAG 的研究表明，检索并不总是能提升最终结果。过度检索可能导致资源浪费，并在模型的固有参数化知识足以回答相关问题时引发潜在混淆。因此，这里介绍两种确定检索必要性的方法：**基于规则**和**基于模型**的方法。

**基于规则**

基于规则的方法通过预定义的逻辑或条件来决定是否进行检索，通常依赖于统计分析、超参数调整或明确的决策标准。这类方法的优势在于其透明性和可控性，适用于需要快速判断检索必要性的场景。例如，可以通过问题频率、输入不确定性或生成概率等规则，动态决定是否调用检索模块，从而避免不必要的资源消耗。

> **例**：
>
> 1. **`FLARE`**&#x4E3B;动根据生成过程中的概率决定是否以及何时进行搜索。
>
> 2. **`Efficient-KNNLM`**&#x5C06; **KNN-LM** 和 **NPM** 的生成概率与超参数$$\lambda$$结合，以确定生成和检索的比例。

**基于模型**

基于模型的方法利用训练好的模型或大语言模型的能力来智能判断是否需要检索。这种方法更加灵活，能够根据具体查询内容和上下文动态调整决策。例如，模型可以评估自身对问题的回答置信度，或者通过外部知识库验证答案的准确性，从而决定是否进行进一步检索。这种方法在复杂任务中表现出色，但可能需要更高的计算成本。

> **例**：
>
> 1. **`Self-RAG`**&#x4F7F;用训练好的生成器，根据不同用户查询下的检索标记决定是否执行检索。
>
> 2. **`SKR`**&#x5229;用大语言模型自身的能力预先判断是否能回答问题，如果可以回答，则不进行检索。
>
> 3. **`Rowen`**&#x5C06;问题翻译成多种语言，并检查这些语言中答案的一致性，利用结果判断是否需要进行信息检索。
>
> 4. **`AdaptiveRAG`**&#x4F7F;用一个较小的语言模型通过分类器动态决定是否根据查询复杂性进行检索。

* **迭代 RAG**

迭代 RAG 通过反复循环检索和生成阶段逐步优化结果，而不是仅进行一轮操作。

> **例**： &#x20;
>
> 1. **`RepoCoder`**&#x4F7F;用迭代检索-生成方法完成代码补全，通过先前生成的代码优化查询，以更好地利用分散的信息并改善结果。 &#x20;
>
> 2. **`ITER-RETGEN`**&#x901A;过使用生成器的输出识别知识空白，检索必要信息，并为未来的生成周期提供信息，从而迭代提升内容质量。 &#x20;
>
> 3. **`SelfMemory`**&#x8FED;代使用检索增强生成器形成一个庞大的记忆池，记忆选择器从中挑选输出以指导下一个生成周期。 &#x20;
>
> 4. **`RAT`**&#x9996;先通过零样本思维链提示由大语言模型生成内容，然后通过从外部知识库检索知识逐步修正每个思维步骤。

## 6.4 RAG 的训练

根据是否需要训练，现有的 RAG 方法可以分为两类：**无训练方法**和**基于训练的方法**。无训练方法通常在推理阶段直接利用检索到的知识，通过将检索到的文本插入 prompt 中来避免额外的训练，这种方法计算效率高。然而，一个潜在的挑战是检索器和生成器组件并未针对下游任务进行专门优化，这可能导致检索到的知识无法得到充分利用。为了充分挖掘外部知识，许多方法被提出用于微调检索器和生成器，从而引导 LLM 有效地适应并整合检索到的信息。

根据训练策略，将这些基于训练的方法分为三类：

> 1. **独立训练方法**分别独立训练 RAG 流程中的每个组件
>
> 2. **顺序训练方法**先训练一个模块，然后冻结已训练好的组件以指导其他部分的调优过程
>
> 3. **联合训练方法**同时训练检索器和生成器

接下来详细展开**无训练**、**独立训练**、**顺序训练**和**联合训练**方法。这些不同训练方法的比较如下图所示：

![]()

### 6.4.1 无训练方法

由于具有庞大的参数量，LLM 展现了接近人类水平的智能，并在各种下游任务中取得了令人满意的预测性能。然而，频繁进行微调和更新存储在模型参数中的知识极具挑战性，因为这需要大量的时间和计算资源。最近，许多公式发现，可以通过检索机制增强大语言模型的能力，使其能够从外部来源动态获取新知识，而无需额外的训练过程。这些方法不再仅仅依赖于模型参数中编码的隐性知识，而是显著提升了各种知识密集型任务的性能，例如开放域问答。根据大语言模型利用检索信息的不同方式，可以将这些无训练方法分为两类：

> 1. **基于提示工程的方法**：直接将检索到的知识整合到原始提示中
>
> 2. **基于检索的 Token 生成方法**：通过检索信息来校准标记生成过程

* **基于提示工程的方法**

由于 LLM 的生成性能高度依赖输入的 Query，许多无训练的 RAG 方法通过优化原始 prompt 来利用外部知识。具体而言，检索到的文本通常被用作上下文信息，并与原始提示结合以指导大语言模型的生成过程。

> **例**：
>
> 1. **`In-Context RALM`**&#x5728;保持大语言模型参数不变的情况下，直接将检索到的文档插入原始 prompt 之前，以增强生成过程。
>
> 2. **`IRCoT`**&#x5C06;思维链生成步骤与知识检索步骤交错进行，从而为后续推理步骤检索到比仅依赖问题作为查询的标准检索方法更相关的信息。
>
> 3. **`GENREAD`**&#x4E0E;从大型语料库中检索知识不同，它首先提示大语言模型基于查询生成上下文文档，然后根据给定的上下文和问题生成答案。
>
> 4. **`SKR`**&#x5F15;导大语言模型判断是否能够基于其内部知识回答给定问题，从而通过选择性调用检索器灵活利用内部和外部知识。
>
> 5. **`TOC`**&#x9996;先为模糊问题检索相关知识，并通过将模糊问题细化为多个明确的问题递归构建树状结构，最终聚合生成长篇答案。

* **基于检索的 Token 生成方法**

除了直接将外部知识整合到原始提示中外，辅助信息还可以用于调整标记生成过程。

> **例**：
>
> 1. **`KNN-KMs`**&#x9996;先根据给定查询从数据存储中检索出 𝑘 个最相关的上下文，并基于距离计算邻居分布。输出分布通过对邻居分布和原始模型的输出分布进行插值来校准。
>
> 2. **`Rest`**&#x7528;非参数化的检索数据存储取代参数化的草稿模型，并根据当前上下文检索相关标记以进行推测性解码。

### 6.4.2 独立训练

独立训练是指将检索器和 LLM 作为两个完全独立的过程进行训练，在训练过程中检索器和大语言模型之间没有交互。与无训练方法相比，通过训练大语言模型利用检索到的知识，或训练检索器弥合信息检索与语言生成之间的差距，可以有效提升 RAG 赋能模型的性能。对于大语言模型的训练，负对数似然损失是最具代表性的训练目标，旨在引导大语言模型基于给定输入生成期望的输出。关于检索器，它可以分为两类：

> 1. **稀疏检索器**：稀疏检索器通常利用稀疏特征（如词频）来表示文档，并基于特定任务的指标（如 TF-IDF 和 BM25）计算相关性分数。
>
> 2. **密集检索器**：深度神经网络被用于将 Query 和文档编码为密集表示，然后通常使用内积计算相关性分数并检索相关的外部知识。例如，**`DPR`**&#x4F7F;用两个独立的 BERT 网络分别编码查询和段落，并通过对比学习训练这些模型。**`CoG`**&#x8BAD;练前缀编码器和短语编码器用于检索，并将文本生成重新表述为从现有源文本集合中进行多次复制粘贴操作。

### 6.4.3 顺序训练

独立训练是一种在生成过程中利用外部知识的高效方法，因为检索器和生成器可以离线训练，并且可以直接使用现成的模型，避免了额外的训练成本。为了更好地增强检索器和生成器之间的协同作用，一些方法对检索器和大语言模型进行顺序训练。在这些顺序训练方法中，过程通常从检索器或生成器的独立预训练开始，随后固定预训练模块，同时对另一个模块进行训练。需要注意的是，各种现有的模型，&#x5982;**`BERT`**、**`CLIP`**、**`T5`**&#x7B49;，可以直接用作固定的检索器和生成器，从而跳过最初的预训练过程。与独立训练相比，顺序训练涉及检索器和生成器的协同训练，其中可训练模块受益于固定模块的辅助。根据检索器和生成器的训练顺序，顺序训练可以分为**先训练检索器**还是**先训练 LLM**。

* **先训练检索器**

首先训练检索模型，然后将其固定，接着利用检索到的知识对 LLM 进行训练。

> **例**：
>
> **`RETRO`**&#x4F7F;用独立预训练的 BERT 模型作为检索器，并训练一个编码器-解码器架构以将检索片段整合到模型的预测中。
>
> **`RALMs`**&#x4F7F;用 Google 搜索和开源的 COLBERTV2 作为预训练检索器，并微调大语言模型以有效利用检索到的段落。
>
> **`ITERRTGEN`**&#x4F7F;用预训练的 S-BERT 作为检索器，并引入了一种自适应混合检索策略以检索示例。此外，它利用 T5 作为生成器，基于目标标签和结合原始提示与检索示例的输入对其进行进一步微调。
>
> **`SMALLCAP`**&#x4F7F;用 CLIP 对输入图像和外部数据存储中的文本数据进行编码，并基于余弦相似度检索最相关的内容。通过训练一个交叉注意力层并使用 GPT-2 作为解码器来生成字幕。

* **先训练 LLM**

类似地，也可以先预训练大语言模型，然后在经过良好训练的大语言模型的监督下对检索器进行微调。

> **例**：
>
> **`DKRR`**&#x53D1;现序列到序列模型的注意力分数可以指示文档的相关性。因此，他们提议利用阅读器模型的注意力分数生成合成标签以训练检索器。
>
> **`AAR`**&#x4F7F;用一个小语言模型生成用于训练检索器的监督信号。经过良好训练的检索器可以进一步用于提升黑箱大语言模型的性能。
>
> **`RA-DIT`**&#x9996;先微调大语言模型以增强其利用检索知识的能力，然后训练检索器以使其输出更好地与大语言模型对齐。
>
> **`UPRISE`**&#x63D0;出了一种轻量级方法，通过引入提示检索器来增强大语言模型在未见任务中的零样本性能。一个冻结的大语言模型被用来指导提示检索器的微调过程，随后该检索器在推理过程中为不同任务检索适用于各种大语言模型的提示。

### 6.4.4 联合训练

联合训练方法采用端到端的范式，同时优化检索器和生成器。与顺序训练每个模块不同，联合训练方法有效增强了检索器定位外部知识以用于生成的能力，以及生成器有效利用检索信息的能力。

> **例**：
>
> **`REALM`**&#x4F7F;用**最大内积搜索 MIPS** 技术来定位最相关的文档。为了应用 MIPS，所有外部文档首先被嵌入表示，并为每个嵌入生成一个搜索索引。提出了一种异步索引更新策略，每隔数百个训练步骤刷新一次索引，以避免重新索引所有文档的时间消耗。

## 6.5 RAG 的变体

> 论文：[**Self-RAG**](https://arxiv.org/pdf/2310.11511)  **[GraphRAG](https://arxiv.org/pdf/2404.16130)  [HybridRAG](https://arxiv.org/pdf/2408.04948)  [LightRAG](https://arxiv.org/pdf/2410.05779)**

### 6.5.1 Self-RAG

### 6.5.2 GraphRAG

### 6.5.3 HybridRAG

### 6.5.4 LightRAG

# 7. 大模型智能体 Agent

## 7.1 概述

智能体长期以来被认为是一种实现人工通用智能 **AGI** 的有前景的方法，**AGI** 预期能够通过自我导向的规划和行动来完成任务。在以往的研究中，这些智能体通常被假设基于简单且启发式的策略函数进行行动，并在孤立和受限的环境中学习。这种假设与人类的学习过程存在显著差异，因为人类的思维高度复杂，并且个体能够从更广泛的环境中学习。由于这些差距，以往研究中获得的智能体通常远未达到复制人类水平的决策过程，尤其是在无约束、开放领域的场景中。

近年来， LLM 取得了显著的成功，展现了实现类人智能的巨大潜力。这种能力源于利用全面的训练数据集以及大量的模型参数。基于这一能力，AI Agent 逐渐兴起，它将 LLM 用作核心控制器，以构建具有类人决策能力的自主智能体。与强化学习相比，基于 LLM 的智能体拥有更全面的内部世界知识，这使它们即使在没有特定领域数据训练的情况下也能做出明智的行动。此外，基于 LLM 的智能体可以提供自然语言接口用于人机交互，从而具备更高的灵活性和更强的可解释性。

沿着这一方向，相关研究者开发了许多有前景的模型，其核心思想是为 LLM 赋予诸如记忆和规划等人类能力，使其表现得像人类并有效完成各种任务。

![]()



## 7.2 Agent 核心组件

基于 LLM 的智能体概念框架包含三个组成部分：大脑、感知和行动。作为控制器，大脑模块负责执行记忆、思考和决策等基本任务。感知模块负责感知和处理来自外部环境的多模态信息，而行动模块则通过工具执行操作并对周围环境产生影响。下图是一个示例来说明工作流程：当一个人询问是否会下雨时，感知模块将指令转化为大语言模型可以理解的表示形式。然后，大脑模块根据当前天气状况以及互联网上的天气预报开始推理。最后，行动模块作出响应，并将雨伞递给人类。通过重复上述过程，智能体能够不断获取反馈并与环境进行交互。

![]()

如果一个个体想要在外部环境中生存，就必须高效地适应周围环境。这要求他具备认知能力，能够感知并对外部世界的变化作出反应，这与智能体定义是一致的。因此基于 LLM 的通用框架通常由三个关键部分组成：**大脑**、**感知**和**行动**。该框架可以根据不同的应用场景进行调整，即并非所有研究都会用到每一个具体组件。总体而言，智能体按照以下工作流程运行：

> 1. **感知模块**（对应于人类的感觉系统，如眼睛和耳朵）感知外部环境的变化，并将多模态信息转化为智能体可以理解的表示形式。
>
> 2. **大脑模块**作为控制中心，进行思考、决策以及与存储相关的操作（包括记忆和知识）。
>
> 3. **行动模块**（对应于人类的四肢）借助工具执行任务并对周围环境产生影响。

通过重复上述过程，智能体能够不断获取反馈并与环境进行交互。

### 7.2.1 感知模块

人类和动物都依赖感官器官，如眼睛和耳朵等，从周围环境中获取信息。这些感知输入被转化为神经信号并传递到大脑进行处理，使我们能够感知并与世界互动。类似地，对于基于 LLM 的智能体来说，从多种来源和模态中接收信息至关重要。扩展的感知空间有助于智能体更好地理解环境、做出明智决策，并在更广泛的任务中表现出色，因此这是一个重要的发展方向。智能体通过感知模块将这些信息传递给大脑模块进行处理。

* **文本输入**

文本是承载数据、信息和知识的一种方式，文本交流已经成为人类与世界互动的重要途径之一。目前基于 LLM 的智能体已经具备通过文本输入和输出与人类交流的基本能力。在用户的文本输入中，除了显式的内容外，还隐藏着信念、欲望和意图等隐含信息。理解这些隐含意义对于智能体把握用户潜在和深层次的意图至关重要，从而提升其与用户沟通的效率和质量。

然而，当前基于 LLM 的智能体在理解文本输入中的隐含意义方面仍然面临挑战。例如，一些研究采用强化学习来感知隐含意义，并通过模型反馈推导奖励值。这有助于推断说话者的偏好，从而使智能体能够提供更加个性化和准确的响应。此外，由于智能体被设计用于复杂的现实场景中，它不可避免地会遇到许多全新的任务。对未知任务的文本指令的理解对智能体的文本感知能力提出了更高的要求。但是经过指令微调的 LLM 能够展现出卓越的 zero-shot 指令理解和泛化能力，从而无需针对特定任务进行微调。

* **视觉输入**

尽管 LLM 在语言理解和多轮对话方面表现优异，但它们天生缺乏视觉感知能力，只能理解离散的文本内容。视觉输入通常包含有关世界的丰富信息，包括物体属性、空间关系、场景布局以及智能体周围环境中的更多细节。因此，将视觉信息与其他模态的数据相结合，可以为智能体提供更广泛的上下文和更精确的理解，从而加深智能体对环境的感知。

为了帮助智能体理解图像中包含的信息，一种直接的方法是为图像输入生成相应的文本描述，这种方法被称为图像字幕生成。生成的字幕可以直接与标准文本指令关联并输入智能体。这种方法具有高度的可解释性，并且无需额外训练即可生成字幕，从而节省大量计算资源。然而，字幕生成是一种低带宽方法，在转换过程中可能会丢失大量潜在信息。此外，智能体对图像的关注可能引入偏见。

受 **Transformer** 在自然语言处理中的出色表现启发，一些科研人员将其应用扩展到了计算机视觉领域。代表性的工作&#x5982;**`ViT`**/**`VQVAE`**&#x6210;功使用 Transformer 对视觉信息进行编码。如右图所示，它首先将图像划分为固定大小的块，然后对这些块进行线性投影，作为 Transformer 的输入 Token。最终，通过计算 Token 之间的自注意力机制，他们能够整合整个图像的信息，从而实现一种高效的视觉内容感知方法。

![]()

因此，一些工作尝试直接结合图像编码器和 LLM，以端到端的方式训练整个模型。虽然智能体可以实现卓越的视觉感知能力，但这需要消耗大量的计算资源。

预训练的视觉编码器和 LLM 可以显著增强智能体的视觉感知和语言表达能力。在训练过程中冻结其中一个或两者是一种广泛采用的范式，能够在训练资源和模型性能之间取得平衡。然而，LLM 无法直接理解视觉编码器的输出，因此需要将图像编码转换为 LLM 能够理解的嵌入表示。也就是说需要将视觉编码器与 LLM 对齐。通常需要在两者之间添加一个额外的可学习接口层。

例如，**`BLIP-2`**&#x548C;**`InstructBLIP`**&#x4F7F;&#x7528;**`Q-Former`**&#x6A21;块作为视觉编码器和 LLM 之间的中间层。如上图，Q-Former 是一种 Transformer，使用可学习的 Query 向量，能够提取富含语言信息的视觉表示。

![]()

它可以为 LLM 提供最有价值的信息，减轻智能体学习视觉-语言对齐的负担，从而缓解灾难性遗忘的问题。

除此之外，一些工作，&#x5982;**`LLaVA`**&#x91C7;用了一种计算效率较高的方法，通过使用**单个投影层**实现视觉-文本对齐，减少了对额外参数训练的需求。此外，投影层可以有效地与可学习接口集成，调整其输出维度以使其与 LLM 兼容。

![]()

视频输入由一系列连续的图像帧组成。因此，智能体用于感知图像的方法也可能适用于视频领域，从而使智能体能够良好地感知视频输入。与图像信息相比，视频信息增加了时间维度。因此，智能体对不同帧之间时间关系的理解对于感知视频信息至关重要。一些工作&#x5982;**`Flamingo`**&#x5728;理解视频时通过掩码机制确保时间顺序。掩码机制限制智能体仅能访问早于当前帧的时间点的视觉信息，从而在感知特定帧时保持时间一致性。

* **听觉输入**

听觉信息也是世界信息的重要组成部分。当智能体具备听觉能力时，它可以增强对交互内容、周围环境甚至潜在危险的感知能力。目前已有许多成熟的模型和方法用于处理音频作为独立的模态。然而，这些模型通常在特定任务上表现出色。鉴于 LLM 出色的工具使用能力，一个非常直观的想法就是，智能体可以将 LLM 用作控制中心，以级联方式调用现有的工具集或模型库来感知音频信息。例如，**`AudioGPT`**&#x5145;分利用&#x4E86;**`FastSpeech`**、**`GenerSpeech`**、**`Whisper`**&#x4EE5;及其他模型的能力，这些模型在文本转语音、风格迁移和语音识别等任务中取得了优异成果。

音频频谱图提供了音频信号频率随时间变化的直观表示。对于一段时间内的音频数据段，可以将其抽象为有限长度的音频频谱图。音频频谱图具有二维表示形式，可以可视化为平面图像。因此，一些研究尝试将视觉领域的感知方法迁移到音频领域。**AST**（**A**udio **S**pectrogram **T**ransformer）采用与 **ViT** 类似的 **Transformer** 架构来处理音频频谱图。通过将音频频谱图分割为块，它实现了对音频信息的有效编码。除此之外，还有一些工作从冻结编码器的思想中获得启发，以减少训练时间和计算成本。他们通过添加相同的可学习接口层，将音频编码与其他模态的数据编码对齐。

* **其他输入**

目前许多研究已经探讨了针对文本、视觉和音频的感知单元。然而，基于 LLM 的智能体可能会配备更丰富的感知模块。未来，它们可能像人类一样感知和理解现实世界中的多样化模态。例如，智能体可能拥有独特的触觉和嗅觉器官，使其在与物体交互时能够收集更详细的信息。同时，智能体还可以清晰感知周围环境的温度、湿度和亮度，从而采取基于环境意识的行动。

此外，通过高效整合视觉、文本和光敏性等基本感知能力，智能体可以开发出各种面向人类的用户友好感知模块。**`InternGPT`**&#x5F15;入了指向指令。用户可以通过手势或移动光标选择、拖动或绘制，与图像中难以描述的具体部分进行交互。指向指令的加入有助于为单个文本指令提供更精确的说明。在此基础上，智能体有潜力感知更复杂的用户输入。例如，增强现实 **AR** / 虚拟现实 **VR** 设备中的眼动追踪技术、身体动作捕捉，甚至是脑机交互中的脑电波信号。

一个类人的基于 LLM 的智能体应具备对更广泛整体环境的感知能力。目前许多成熟的硬件设备可以帮助智能体实现这一目标：

> **激光雷达**可以创建 3D 点云地图，帮助智能体检测和识别周围的物体
>
> **全球定位系统 GPS** 可以提供精确的地理位置坐标，并与地图数据集成
>
> **惯性测量单元 IMU** 可以测量和记录物体的三维运动，提供关于物体速度和方向的详细信息

然而，这些传感器数据复杂，无法被基于 LLM 的智能体直接理解。探索智能体如何感知更全面的输入是研究 Agent 的一个重要方向。

### 7.2.2 中枢模块

人类大脑是一个复杂的结构，由大量相互连接的神经元组成，能够处理各种信息、产生多样化的思想、控制不同的行为，甚至创造艺术和文化。与人类类似，大脑是人工智能智能体的核心部分，主要由一个大型语言模型构成。

为了确保有效的沟通，进行**自然语言交互的能力**至关重要。在接收到感知模块处理过的信息后，大脑模块首先会转向存储功能，从**知识库中检索信息**，并从**记忆**中回忆相关信息。这些结果有助于智能体**制定计划、进行推理并作出明智的决策**。此外，大脑模块还可以以摘要、向量或其他数据结构的形式记录智能体过去的观察、思考和行动。同时，它还可以更新常识和领域知识，以备将来使用。基于大语言模型的智能体还可以通过其固有的**泛化能力和迁移能力**适应陌生场景。

* **自然语言交互**

语言作为交流的媒介，包含着丰富的信息。除了直观表达的内容外，背后可能还隐藏着说话者的信念、欲望和意图。得益于 LLM 本身强大的自然语言理解和生成能力，智能体不仅能够熟练地进行多语言的基本互动对话，还能表现出深度的理解能力，使人类可以轻松理解和与智能体交互。此外，以自然语言进行交流的基于 LLM 的智能体能够赢得更多的信任，并更有效地与人类合作。

1. **多轮互动对话**

多轮对话能力是实现高效且一致沟通的基础。作为大脑模块的核心，像 GPT 系列、LLaMA 系列和 T5 这样的大语言模型能够理解自然语言并生成连贯且上下文相关的回应，这有助于智能体更好地理解和处理各种问题。然而，即使是人类也很难在一次对话中毫无混淆地交流，因此需要多轮对话。与传统的仅限文本的阅读理解任务相比，&#x5982;**`SQuAD`**，多轮对话具有以下特点：

> * 互动性强，涉及多个发言者，缺乏连续性
>
> * 可能涉及多个主题，对话信息也可能冗余，使得文本结构更加复杂

总体而言，多轮对话主要分为三个步骤：

> 1. **理解自然语言对话的历史记录**
>
> 2. **决定采取什么行动**
>
> 3. **生成自然语言回应**

基于 LLM 的智能体能够利用现有信息不断优化输出，进行多轮对话并有效实现最终目标。

* **高质量的自然语言生成**

近期的大语言模型展现了卓越的自然语言生成能力，能够在多种语言中持续生成高质量的文本。&#x4ECE;**`GPT-3`**&#x5230;**`InstructGPT`**&#x518D;&#x5230;**`GPT-4`**，LLM 生成内容的连贯性和语法准确性得到了稳步提升。研究表明，这些语言模型能够适应条件文本的风格和内容。同时，**`ChatGPT`**&#x5728;语法错误检测方面的表现突出，彰显了其强大的语言能力。在对话场景中，大语言模型在对话质量的关键指标上也表现出色，包括**内容**、**相关性**和**适当性**。重要的是，它们不仅仅是复制训练数据，而是展现了一定程度的创造力，生成的文本与人类制作的基准一样新颖，甚至更加新颖。与此同时，通过可控 prompt 的使用，人类监督仍然有效，确保对这些语言模型生成的内容进行精确控制。

* **意图与隐含意义的理解**

尽管在大规模语料库上训练的模型已经足够智能以理解指令，但大多数模型仍然无法模拟人类对话或充分利用语言传递的信息。理解隐含意义对于与其他智能体进行有效的沟通和协作至关重要，并使人们能够解读他人的反馈。大语言模型的出现突显了基础模型理解人类意图的潜力，但在面对模糊指令或其他隐含意义时，这对智能体来说仍然是一个重大挑战。对于人类来说，从对话中捕捉隐含意义是自然而然的，而对于智能体来说，则需要将隐含意义形式化为奖励函数，使其能够在未见过的上下文中选择符合说话者偏好的选项。奖励建模的主要方法之一是基于反馈推断奖励，反馈主要以比较的形式呈现和无限制的自然语言描述。另一种方法则是通过动作空间作为桥梁，从描述中恢复奖励。研究表明，人类行为可以映射为从一组隐式选项中做出的选择，这有助于用单一统一的形式化方法解释所有信息。通过利用对上下文的理解，智能体可以采取高度个性化且精准的行动，满足特定需求。

* **知识**

由于现实世界的多样性，许多自然语言处理研究人员尝试利用更大规模的数据。这类数据通常是非结构化且未标注的，但其中包含语言模型可以学习的海量知识。理论上，语言模型的参数越多，能够学习的知识就越多，甚至有可能学习和理解自然语言中的所有内容。研究表明，在大规模数据集上训练的语言模型可以将广泛的知识编码到其参数中，并对各种类型的查询作出正确回应。此外，这些知识可以帮助基于大语言模型的智能体做出明智的决策。所有这些知识大致可以分为以下几类：

> * **语言学知识**：语言学知识表现为一种约束系统或语法规则，定义了一种语言中所有可能的句子。它包括词法、句法、语义和语用学。只有掌握语言学知识的智能体才能理解句子并参与多轮对话。此外，这些智能体可以通过在包含多种语言的数据集上进行训练来获得多语言知识，从而无需额外的翻译模型。
>
> * **常识知识**：常识知识是指通常在早期教育中传授给大多数人的普遍世界事实。例如，人们普遍知道药物用于治病，雨伞用于防雨。这类信息通常不会在上下文中明确提及。因此，缺乏相应常识知识的模型可能无法理解或误解预期含义。同样，缺乏常识知识的智能体可能会做出错误的决策，例如在大雨时不带伞。
>
> * **专业领域知识**：专业领域知识是指与特定领域相关的知识，如编程、数学、医学等。对于模型来说，掌握这些知识是有效解决特定领域问题的关键。例如，设计用于执行编程任务的模型需要具备编程知识，如代码格式；而用于诊断目的的模型则需要具备医学知识，如特定疾病名称和处方药。

尽管大语言模型在获取、存储和利用知识方面表现出色，但仍存在潜在问题和未解决的难题。例如，模型在训练过程中学到的知识可能会过时，甚至从一开始就可能是错误的。一个简单的解决方案是重新训练，但这需要先进的数据、大量时间和计算资源。更糟糕的是，这可能导致灾难性遗忘。因此，一些研究人员尝试编辑大语言模型，以定位和修改模型中存储的特定知识。这种方法涉及卸载错误知识的同时获取新知识。实验表明，这种方法可以部分编辑事实知识，但其底层机制仍需进一步研究。此外，大语言模型可能会生成与源信息或事实信息相冲突的内容，这种现象通常被称为幻觉。这是大语言模型无法广泛应用于事实严谨任务的关键原因之一。为了解决这一问题，一些研究人员提出了一种衡量幻觉程度的指标，为开发者提供了一个评估大语言模型输出可信度的有效参考。此外，还有一些研究人员使大语言模型能够利用外部工具来避免错误知识。这两种方法都可以缓解幻觉的影响，但仍需进一步探索更有效的解决方案。

* **记忆**

**记忆**存储了智能体过去的观察、思考和行动序列，这与Nuxoll等人提出的定义相似。正如人类大脑依赖记忆系统回顾性地利用先前的经验来制定策略和做出决策，智能体也需要特定的记忆机制以确保它们能够熟练处理一系列连续任务。当面对复杂问题时，记忆机制帮助智能体有效回顾并应用先前的策略。此外，这些记忆机制使个体能够通过借鉴过去的经验来适应陌生环境。随着基于 LLM 的智能体交互周期的扩展，两个主要挑战浮现出来：

> 1. 第一个挑战涉及**历史记录的长度问题**。基于 LLM 的智能体以自然语言格式处理先前的交互，并将历史记录附加到每个后续输入中。随着这些记录的增长，它们可能会超出大多数基于 LLM 的智能体所依赖的 Transformer 架构的限制。当这种情况发生时，系统可能会截断部分内容。
>
> 2. 第二个挑战是**提取相关记忆的困难**。随着智能体积累了大量历史观察和行动序列，它们面临着日益沉重的记忆负担。这使得建立相关主题之间的联系变得更加困难，可能导致智能体的回应与当前上下文不一致。

1. **增强记忆能力的方法**

在此，介绍几种提升基于 LLM 智能体记忆能力的方法：

> * **提高 Transformer 的长度限制**。第一种方法试图解决或缓解 Transformer 架构固有的序列长度限制。由于内在限制，Transformer 在处理长序列时表现不佳。随着序列长度的增加，计算需求因自注意力机制中的成对 token 计算而呈指数级增长。缓解这些长度限制的策略包括文本截断、分割输入以及强调文本的关键部分。一些其他研究则通过修改注意力机制来降低复杂性，从而适应更长的序列。
>
> * **总结记忆内容**。第二种增强记忆效率的策略基于记忆总结的概念。这确保智能体能够轻松提取历史交互中的关键细节。已有多种技术被提出用于总结记忆。一些方法使用提示词简洁地整合记忆，另一些则强调反思过程以创建浓缩的记忆表示。分层方法将对话简化为每日快照和总体摘要。值得注意的是，某些策略将环境反馈转化为文本封装形式，增强智能体对未来交互的上下文理解。此外，在多智能体环境中，重要通信元素被捕获并保留。
>
> * **用向量或数据结构压缩记忆**。通过使用适当的数据结构，智能体提升了记忆检索效率，从而快速响应交互。值得注意的是，一些方法依赖嵌入向量来表示记忆片段、计划或对话历史。另一种方法将句子转化为三元组配置，还有一些方法将记忆视为独特的数据对象，促进多样化的交互。此外，**`ChatDB`**&#x548C;**`DB-GPT`**&#x901A;过将 LLM 与 SQL 数据库集成，允许通过 SQL 命令进行数据操作。

* **记忆检索方法**

当智能体与其环境或用户交互时，从记忆中检索最相关内容至关重要。这确保智能体能够获取相关且准确的信息以执行特定行动。一个重要的问题随之而来：智能体如何选择最适合的记忆？通常，智能体会以自动化方式检索记忆。一种重要的自动化检索方法考虑三个指标：**时效性**、**相关性**和**重要性**。记忆得分被确定为这三个指标的加权组合，得分最高的记忆在模型上下文中优先使用。一些研究引入了**交互式记忆对象**的概念，这些对象是对对话历史的表示，可以通过总结进行移动、编辑、删除或合并。用户可以查看和操作这些对象，从而影响智能体对对话的理解。类似地，其他研究允许根据用户提供的特定命令进行记忆操作，如删除。这些方法确保记忆内容与用户期望紧密对齐。

* **推理与规划**

**推理**

推理以证据和逻辑为基础，是人类智力活动的核心，同时也是解决问题、决策制定和批判性分析的基石。演绎、归纳和溯因推理是智力活动中公认的三种主要推理形式。对于基于 LLM 的智能体而言，与人类一样，推理能力对于解决复杂任务至关重要。

关于大语言模型的推理能力，学术界存在不同的观点。一些人认为语言模型在预训练或微调阶段就具备推理能力，而另一些人则认为这种能力是在模型达到一定规模后才显现出来的。具体来说，具有代表性的 CoT 方法已被证明能够通过引导大语言模型在输出答案之前生成推理过程，从而激发其推理能力。此外，还有其他策略被提出以增强大语言模型的表现，例如自一致性、自我优化、自我改进和选择-推理等。一些研究表明，逐步推理的有效性可以归因于训练数据的局部统计结构，变量之间的局部依赖关系比对所有变量进行训练时具有更高的数据效率。

**规划**

规划是人类面对复杂挑战时采用的关键策略。对于人类而言，规划有助于组织思想、设定目标并确定实现目标的步骤。与人类类似，规划能力对智能体也至关重要，而推理能力则是规划模块的核心。这为基于大语言模型的智能体提供了一个结构化的思考过程。通过推理，智能体将复杂任务分解为更易管理的子任务，并为每个子任务制定适当的计划。此外，随着任务的推进，智能体可以通过内省调整其计划，确保更好地适应现实情况，从而实现灵活且成功的任务执行。通常，规划分为两个阶段：计划制定和计划反思。

> * **计划制定**：在计划制定过程中，智能体通常会将一个总体任务分解为多个子任务，并在此阶段提出了多种方法。值得注意的是，一些研究提倡基于大语言模型的智能体一次性全面分解问题，制定完整计划后按顺序执行。相比之下，像思维链系列研究则采用了一种自适应策略，逐一规划并处理子任务，从而在整体上更灵活地应对复杂任务。此外，一些方法强调分层规划，而另一些则主张从树状结构的推理步骤中推导出最终计划。后者认为智能体应在最终确定计划之前评估所有可能的路径。虽然基于大语言模型的智能体展现出广泛的一般知识，但在面对需要专业知识的任务时，它们偶尔会遇到挑战。通过将这些智能体与特定领域的规划器集成，已显示出更好的性能。
>
> * **计划反思**：在制定计划后，必须对其进行反思和评估优劣。基于大语言模型的智能体利用内部反馈机制，通常从现有模型中汲取见解，以优化和完善其策略与规划方法。为了更好地与人类价值观和偏好保持一致，智能体会积极与人类互动，允许他们纠正一些误解并将定制化反馈融入其规划方法中。此外，智能体还可以从实际或虚拟环境中获取反馈，例如任务完成的线索或行动后的观察结果，从而帮助它们修订和完善计划。

* **泛化**

智能不应局限于特定领域或任务，而应涵盖广泛的认知技能和能力。人类大脑的卓越特性在很大程度上归因于其高度的可塑性和适应性。它能够根据外部刺激和内部需求不断调整其结构和功能，从而适应不同的环境和任务。近年来，大量研究表明，在大规模语料库上预训练的模型可以学习通用的语言表示。利用预训练模型的能力，仅需少量数据进行微调，LLM 就能在下游任务中表现出色。无需从头训练新模型，这节省了大量计算资源。然而，通过这种针对特定任务的微调，模型缺乏通用性，并难以泛化到其他任务。基于大语言模型的智能体不仅是一个静态的知识库，还展现出动态学习能力，使它们能够快速且稳健地适应新任务。

1. **未见任务泛化**

研究表明，经过指令调优的大语言模型在无需任务特定微调的情况下表现出零样本泛化能力。随着模型规模和语料库规模的扩大，大语言模型在陌生任务中逐渐展现出显著的新兴能力。具体来说，大语言模型可以通过遵循指令并基于自身理解完成在训练阶段未遇到的新任务。其中一种实现方式是多任务学习，例&#x5982;**`FLAN`**&#x901A;过指令描述的一系列任务对语言模型进行微调，**`T0`**&#x5F15;入了一个统一框架，将所有语言问题转换为文本到文本的格式。尽管 GPT-4 纯粹是一个语言模型，但它在多种领域和任务中表现出显著能力，包括抽象、理解、视觉、编码、数学、医学、法律、对人类动机和情感的理解等。值得注意的是，prompt 的选择对于适当的预测至关重要，直接在 prompt 上进行训练可以提高模型在未见任务上的泛化鲁棒性。通过扩大模型规模以及增加训练指令的数量或多样性，这种泛化能力可以进一步增强。

* **上下文学习**

大量研究表明，大语言模型可以通过上下文学&#x4E60;**`ICL`**&#x6267;行各种复杂任务，这指的是模型从上下文中少量示例中学习的能力。少样本上下文学习通过将原始输入与几个完整示例连接作为提示来丰富上下文，从而提高语言模型的预测性能。ICL 的核心思想是通过类比学习，这类似于人类的学习过程。此外，由于 prompt 是以自然语言编写的，交互具有可解释性和可变性，使得将人类知识融入大语言模型变得更加容易。与监督学习过程不同，ICL 不涉及微调或参数更新，这可以大大降低模型适应新任务的计算成本。除了文本之外，研究人员还探索了上下文学习在不同多模态任务中的潜力，使得智能体能够应用于大规模真实世界任务。

* **持续学习**

最近的研究强调了大语言模型规划能力在促进智能体持续学习方面的潜力，这涉及技能的持续获取和更新。持续学习的一个核心挑战是灾难性遗忘：当模型学习新任务时，往往会丢失先前任务的知识。为了解决这一挑战，已经进行了大量努力，大致可分为三类：

> 1. **引入与先前模型相关的常用术语**
>
> 2. **近似先前的数据分布**
>
> 3. **设计具有任务自适应参数的架构**

基于大语言模型的智能体已成为一种新范式，利用大语言模型的规划能力结合现有技能以应对更复杂的挑战。**`Voyager`**&#x5C1D;试解决由 GPT-4 设计的自动课程提出的渐进式更难的任务。通过从简单程序中合成复杂技能，智能体不仅迅速提升了能力，还有效应对了灾难性遗忘。

### 7.2.3 行动模块

* **文本输出**

基于 Transformer 的生成式大语言模型的兴起与发展，赋予了基于 LLM 的智能体与生俱来的语言生成能力。它们生成的文本质量在多个方面表现出色，例如流畅性、相关性、多样性以及可控性。因此，基于 LLM 的智能体可以成为非常强大的语言生成器。

* **工具使用**

工具是工具使用者能力的延伸。面对复杂任务时，人类会使用工具来简化问题解决过程并提高效率，从而节省时间和资源。同样，如果智能体也学会使用工具，它们将能够更高效、更高质量地完成复杂任务。

基于 LLM 的智能体在某些方面存在局限性，而工具的使用可以增强这些智能体的能力。首先，尽管基于 LLM 的智能体拥有强大的知识库和专业能力，但它们无法记住每一条训练数据。此外，由于上下文提示的影响，它们可能无法调用正确的知识，甚至生成虚假内容。再加上特定领域和场景中的语料库、训练数据及调优不足，智能体在专业化领域的表现也会受到限制。通过专门设计的工具，LLM 可以以插件形式提升其专业知识水平，适应领域需求，并更好地满足特定领域的需要。此外，基于 LLM 的智能体在决策过程中缺乏透明性，这使得它们在医疗、金融等高风险领域的可信度较低。同时，LLM 容易受到对抗性攻击，对输入微小变化的鲁棒性不足。相比之下，借助工具完成任务的智能体表现出更强的可解释性和鲁棒性。工具的执行过程可以反映智能体处理复杂需求的方式，并增强其决策的可信度。而且，由于工具是为其特定使用场景设计的，利用这些工具的智能体更能应对输入微小变化，并具备更强的抗攻击能力。

基于 LLM 的智能体不仅需要使用工具，也非常适合进行工具集成。通过预训练过程中积累的丰富世界知识以及 CoT 提示，LLM 在复杂交互环境中展现了卓越的推理和决策能力，这有助于智能体以适当的方式分解并解决用户指定的任务。此外，LLM 在意图理解等方面表现出显著潜力。当智能体与工具结合时，工具使用的门槛得以降低，从而充分释放人类用户的创造性潜力。

1. **对工具的理解** &#x20;

智能体有效使用工具的前提是对工具的应用场景和调用方法有全面的理解。如果缺乏这种理解，智能体使用工具的过程将变得不可靠，无法真正提升其能力。借助 LLM 强大的 Zero-Shot 和 Few-Shot 学习能力，智能体可以通过描述工具功能和参数的 Zero-Shot 提示，或提供特定工具使用场景和方法的少样本提示来获取相关知识。这些学习方式类似于人类通过查阅工具手册或观察他人使用工具来学习的方法。单一工具往往不足以应对复杂任务，因此智能体需要先将复杂任务分解为子任务，而对工具的理解在任务分解中起着重要作用。

* **学习使用工具** &#x20;

智能体学习使用工具的主要方法包括从演示中学习和从反馈中学习。这包括模仿人类专家的行为，以及理解自身行为的结果，并根据环境和人类提供的反馈进行调整。环境反馈包括任务是否成功完成的结果反馈，以及捕捉行为引起环境状态变化的中间反馈；人类反馈则包括明确的评价和隐含的行为，例如点击链接。如果智能体僵化地应用工具而不具备适应性，它无法在所有场景中达到可接受的表现。智能体需要将在特定情境中学到的工具使用技能推广到更普遍的情况中，例如将针&#x5BF9;**`Yahoo`**&#x641C;索训练的模型迁移到Google搜索上。为此，智能体需要掌握工具使用策略中的通用原则或模式，这可能通过元工具学习实现。增强智能体对简单工具与复杂工具之间关系的理解，例如复杂工具如何建立在简单工具之上，有助于其推广工具使用能力，使其能够有效区分不同应用场景并将先前学到的知识迁移到新工具上。**课程学习** Curriculum Learning 方法与此需求相符，它允许智能体从简单工具开始，逐步学习复杂工具。此外，得益于对用户意图推理和规划能力的理解，智能体可以更好地设计工具使用和协作方法，从而提供更高质量的结果。

* **制造工具以实现自给自足** &#x20;

现有工具通常是为了方便人类设计的，未必最适合智能体使用。为了使智能体更好地使用工具，需要专门为智能体设计的工具。这些工具应更具模块化特性，并具有更适合智能体的输入输出格式。如果提供说明和演示，基于 LLM 的智能体还能够通过生成可执行程序或整合现有工具来创建更强大的工具。它们还可以学习进行自我调试。此外，如果作为工具制造者的智能体成功创建了一个工具，它不仅可以自己使用该工具，还可以在多智能体系统中为其他智能体生成包含工具代码和演示的包。推测未来，智能体可能会实现高度自主，并在工具使用方面表现出高度自治。

* **扩展智能体的动作空间**

借助工具，智能体可以在推理和规划阶段利用各种外部资源，例如网络应用和其他语言模型。这一过程可以为基于 LLM 的智能体提供高专业性、可靠性、多样性和质量的信息，从而促进其决策和行动。例如，基于搜索的工具可以通过外部数据库、知识图谱和网页扩大智能体可访问的知识范围和质量，而特定领域的工具可以增强智能体在相应领域的专业能力。一些研究人员已经开发出基于 LLM 的控制器，用于生成 SQL 语句查询数据库，或将用户查询转换为搜索请求并使用搜索引擎获取所需结果。此外，基于 LLM 的智能体可以使用科学工具执行化学中的有机合成任务，或与 Python 解释器交互以增强其在复杂数学计算任务中的性能。在多智能体系统中，通信工具，如电子邮件，可以在严格的安全约束下作为智能体之间互动的手段，促进协作并展现自主性和灵活性。

尽管上述工具增强了智能体的能力，但与环境的交互媒介仍以文本为基础。然而，工具旨在扩展语言模型的功能，其输出并不局限于文本。非文本输出的工具可以多样化智能体动作的模态，从而扩展基于 LLM 的智能体的应用场景。例如，智能体可以通过视觉模型完成图像处理和生成任务。在航空航天工程中，智能体被探索用于物理建模和求解复杂微分方程；在机器人领域，智能体需要规划物理操作并控制机器人执行任务。能够通过工具或多模态方式动态与环境或世界互动的智能体被称为数字化具身。

* **具身行动**

在追求通用人工智能 AGI 的过程中，具身智能体被认为是一个关键范式，因为它致力于将模型智能与物理世界相结合。具身假说从人类智能发展过程中汲取灵感，认为智能体的智能来源于与环境的持续交互和反馈，而非仅仅依赖精心编纂的教科书。同样，与传统深度学习模型通过互联网数据集学习明确能力以解决领域问题不同，人们期望基于 LLM 的智能体行为不再局限于纯文本输出或调用特定工具完成特定任务。相反，它们应能够主动感知、理解和与物理环境互动，做出决策，并基于LLM的广泛内部知识生成具体行为以改变环境。我们将这些统称为具身行动，这种能力使智能体能够以接近人类行为的方式与世界互动并理解世界。

**基于 LLM 的智能体在具身行动中的潜力**

在LLM广泛兴起之前，研究人员倾向于使用强化学习等方法探索智能体的具身行动。尽管基于强化学习的具身研究取得了广泛成功，但在某些方面仍存在局限性。简而言之，强化学习算法在数据效率、泛化能力和复杂问题推理方面面临挑战，这主要源于对动态且常模棱两可的真实环境建模的困难，或对精确奖励信号表示的高度依赖。最近的研究表明，利用LLM预训练过程中获得的丰富内部知识可以有效缓解这些问题。

1. **成本效率** &#x20;

一些策略算法在样本效率上表现不佳，因为它们需要新鲜数据进行策略更新，而收集足够的具身数据用于高性能训练既昂贵又充满噪声。这种限制也存在于一些端到端模型中。通过利用LLM的内在知识，像PaLM-E这样的智能体联合训练机器人数据与通用视觉-语言数据，从而在具身任务中实现显著的迁移能力，同时展示了几何输入表示可以提高训练数据效率。

* **具身行动泛化** &#x20;

智能体的能力应超越特定任务。面对复杂且未知的真实环境时，智能体必须展现出动态学习和泛化能力。然而，大多数强化学习算法旨在训练和评估特定任务的相关技能。相比之下，经过多样化形式和丰富任务类型的微调，LLM展现了卓越的跨任务泛化能力。例如，PaLM-E在处理新对象或现有对象的新组合时表现出令人惊讶的零样本或一样本泛化能力。此外，语言能力是基于LLM智能体的独特优势，它既是与环境互动的手段，也是将基础技能迁移到新任务的媒介。SayCan利用LLM将提示中的任务指令分解为相应的技能命令，但在部分可观测环境中，有限的先验技能往往无法达到满意的表现。为了解决这一问题，Voyager引入了技能库组件，持续收集新颖的自我验证技能，从而赋予智能体终身学习能力。

* **具身行动规划** &#x20;

规划是人类应对复杂问题以及基于LLM的智能体所采用的关键策略之一。在LLM展现出显著推理能力之前，研究人员引入了分层强化学习（HRL）方法，其中高层策略为目标设定子目标，低层策略生成适当的行为信号。类似于高层策略的作用，具有新兴推理能力的LLM可以无缝应用于复杂任务，以零样本或一样本的方式完成任务。此外，来自环境的外部反馈可以进一步提升基于LLM智能体的规划性能。根据当前环境反馈，一些工作动态生成、维护和调整高层行动计划，以减少对部分可观测环境中先验知识的依赖，从而将计划落地。反馈也可以来自模型或人类，通常被称为批评者，根据当前状态和任务提示评估任务完成情况。

**基于 LLM 的智能体的具身行动**

根据智能体在任务中的自主程度或行动的复杂性，基于LLM的具身行动主要包括观察、操作和导航。

1. **观察** &#x20;

观察是智能体获取环境信息和更新状态的主要方式，在提升后续具身行动效率方面起着关键作用。如§3.2所述，具身智能体的观察主要发生在具有多种输入的环境中，这些输入最终被整合为多模态信号。一种常见方法是使用预训练的视觉Transformer（ViT）作为文本和视觉信息的对齐模块，并标记特殊标记以表示多模态数据的位置。Soundspaces提出通过混响音频输入识别物理空间几何元素，从而增强智能体的观察视角。近期，更多研究将音频作为一种嵌入式观察模态。除了广泛使用的级联范式外，类似于ViT的音频信息编码进一步增强了音频与其他输入模态的无缝集成。智能体对环境的观察还可以来源于人类提供的实时语言指令，而人类反馈有助于智能体获取可能难以直接获得或解析的详细信息。

* **操作** &#x20;

一般来说，具身智能体的操作任务包括物体重新排列、桌面操作和移动操作。典型场景涉及智能体在厨房执行一系列任务，例如从抽屉中取出物品递给用户或清理桌面。除了精准的观察外，这还涉及利用LLM结合一系列子目标。因此，保持智能体状态与子目标之间的同步至关重要。DEPS利用基于LLM的交互规划方法维持这种一致性，并在整个多步骤、长时间推理过程中通过智能体反馈帮助纠错。与之相比，AlphaBlock专注于更具挑战性的操作任务（例如用积木拼出笑脸），这要求智能体对指令有更扎实的理解。不同于现有的开环范式，AlphaBlock构建了一个包含35个复杂高级任务的数据集，以及相应的多步规划和观察对，并通过微调多模态模型增强其对高级认知指令的理解。

* **导航** &#x20;

导航使智能体能够在环境中动态改变位置，通常涉及多角度、多目标观察以及基于当前探索的长跨度操作。在导航之前，具身智能体需要建立对外部环境的内部地图，通常以拓扑图、语义图或占用图的形式呈现。例如，LM-Nav利用VNM创建内部拓扑图，并进一步利用LLM和VLM分解输入命令和分析环境以找到最佳路径。此外，一些研究强调了空间表示的重要性，通过利用预训练的VLM模型将图像中的视觉特征与物理世界的3D重建结合，以实现空间目标的精确定位，而非传统的点或对象中心导航动作。导航通常是长跨度任务，智能体的未来状态受其过去行为影响。需要记忆缓冲区和摘要机制作为历史信息的参考，这在Smallville和Voyager中也有应用。此外，一些研究提出音频输入也具有重要意义，但将音频信息与视觉环境关联仍面临挑战。一个基本框架包括动态路径规划器，利用视觉和听觉观察以及空间记忆规划一系列导航动作。

通过整合这些能力，智能体可以完成更复杂的任务，例如具身问答。其主要目标是自主探索环境，并回答预定义的多模态问题，例如：“厨房里的西瓜比锅大吗？”“哪个更硬？”为了回答这些问题，智能体需要导航到厨房，观察两个物体的大小，然后通过比较得出答案。

在控制策略方面，如前所述，基于 LLM 并在特定具身数据集上训练的智能体通常会生成高层策略指令，以控制低层策略实现特定的子目标。低层策略可以是一个机器人Transformer，它将图像和指令作为输入，并为末端执行器和机械臂生成控制命令，以完成特定的具身任务。最近，在虚拟具身环境中，高层策略被用于控制游戏中的智能体或模拟世界中的角色。例如，Voyager通过调用Mineflayer API接口不断获取各种技能并探索世界。

**具身行动的未来前景**

基于LLM的具身行动被视为连接虚拟智能与物理世界的桥梁，使智能体能够像人类一样感知和改变环境。然而，仍存在一些限制，例如物理世界中机器人操作的成本高昂以及具身数据集的稀缺性，这促使越来越多的研究转向在 Minecraft 等模拟环境中探索智能体的具身行动。通过利用Mineflayer API，这些研究能够以低成本的方式广泛测试智能体的各种操作，包括探索、规划、自我改进甚至终身学习。尽管取得了显著进展，但由于模拟平台与物理世界之间存在显著差异，实现最佳的具身行动仍然面临挑战。为了在现实场景中有效部署具身智能体，对于更贴近真实条件的具身任务范式和评估标准的需求日益增加。另一方面，如何让智能体将语言与实际情境结合也是一个障碍。例如，“像猫一样跳下来”这样的表达主要传达了一种轻盈和安静的感觉，但这种语言隐喻需要足够的世界知识支持。有研究尝试将文本蒸馏与事后经验回放 **HER**（**H**indsight **E**xperience **R**eplay）相结合，构建一个数据集作为训练过程的监督信号。然而，随着具身行动在人类生活的各个领域中扮演越来越重要的角色，对具身数据集的进一步研究仍然是必要的。

## 7.3 Agent 系统优化

### 7.3.1 微调优化

### 7.3.2 无微调优化

## 7.4 Agent 框架

### 7.4.1 AutoGPT

### 7.4.2 BabyAGI

### 7.4.3 AutoGen

### 7.4.4 MetaGPT

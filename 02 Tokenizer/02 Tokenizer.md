# 02. Tokenizer

## 1. ç®€ä»‹

Tokenizeråˆ†è¯ç®—æ³•æ˜¯NLPå¤§æ¨¡å‹æœ€åŸºç¡€çš„ç»„ä»¶ï¼ŒåŸºäºTokenizerå¯ä»¥å°†æ–‡æœ¬è½¬æ¢æˆç‹¬ç«‹çš„**token**åˆ—è¡¨ï¼Œè¿›è€Œè½¬æ¢æˆè¾“å…¥çš„å‘é‡æˆä¸ºè®¡ç®—æœºå¯ä»¥ç†è§£çš„è¾“å…¥å½¢å¼ã€‚æœ¬æ–‡å°†å¯¹åˆ†è¯å™¨è¿›è¡Œç³»ç»Ÿæ¢³ç†ï¼ŒåŒ…æ‹¬åˆ†è¯æ¨¡å‹çš„æ¼”åŒ–è·¯å¾„ï¼Œå¯ç”¨çš„å·¥å…·ï¼Œå¹¶æ‰‹æ¨æ¯ä¸ªtokenizerçš„å…·ä½“å®ç°ã€‚

<aside>
ğŸ’¡

æ³¨ï¼šæœ¬æ–‡ä¸»è¦é’ˆå¯¹äºä¸­/è‹±æ–‡è¯­è¨€

</aside>

åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­ï¼Œ**â€œå­—â€**ï¼ˆcharacterï¼‰å’Œ**â€œè¯â€**ï¼ˆwordï¼‰æœ‰ä¸åŒçš„å®šä¹‰å’Œå¤„ç†æ–¹å¼ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†ä¸åŒè¯­è¨€æ—¶ã€‚

- **å­—ï¼ˆCharacterï¼‰**
    
    å­—æ˜¯æ„æˆæ–‡æœ¬çš„æœ€å°å•å…ƒã€‚ä¸åŒè¯­è¨€çš„â€œå­—â€æœ‰ä¸åŒçš„å®šä¹‰ï¼š
    
    â€¢ **è‹±è¯­**ï¼šå­—é€šå¸¸æŒ‡å•ä¸ªå­—æ¯ï¼ˆå¦‚a, b, cï¼‰ã€‚
    
    â€¢ **æ±‰å­—**ï¼šå­—æŒ‡å•ä¸ªæ±‰å­—ï¼ˆå¦‚â€œæˆ‘â€ã€â€œä½ â€ã€â€œä»–â€ï¼‰ã€‚
    
- **è¯ï¼ˆWordï¼‰**
    
    è¯æ˜¯è¯­è¨€ä¸­å…·æœ‰ç‹¬ç«‹æ„ä¹‰çš„åŸºæœ¬å•å…ƒï¼Œé€šå¸¸ç”±ä¸€ä¸ªæˆ–å¤šä¸ªå­—æ„æˆã€‚è¯çš„å®šä¹‰å’Œå¤„ç†æ–¹å¼å› è¯­è¨€è€Œå¼‚ï¼š
    
    â€¢ **è‹±è¯­**ï¼šè¯é€šå¸¸ç”±ç©ºæ ¼åˆ†éš”ï¼Œå¦‚â€œHello world!â€ä¸­çš„â€œHelloâ€å’Œâ€œworldâ€ã€‚
    
    â€¢ **æ±‰è¯­**ï¼šè¯é€šå¸¸ç”±ä¸€ä¸ªæˆ–å¤šä¸ªæ±‰å­—ç»„æˆï¼Œä½†æ±‰å­—ä¹‹é—´æ²¡æœ‰æ˜ç¡®çš„ç©ºæ ¼åˆ†éš”ã€‚å› æ­¤ï¼Œéœ€è¦åˆ†è¯æŠ€æœ¯æ¥è¯†åˆ«è¯è¾¹ç•Œã€‚ä¾‹å¦‚ï¼Œâ€œæˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†â€å¯ä»¥åˆ†è¯ä¸ºâ€œæˆ‘/çˆ±/è‡ªç„¶è¯­è¨€å¤„ç†â€ã€‚
    

## **2. åˆ‡åˆ†æ–¹æ³•é€Ÿè§ˆ**

1. æ ¹æ®ä¸åŒçš„åˆ‡åˆ†ç²’åº¦å¯ä»¥æŠŠtokenizeråˆ†ä¸º: åŸºäº**å­—(char)ã€è¯(word)**çš„åˆ‡åˆ†ï¼Œå’ŒåŸºäº**Subword**ä»¥åŠåŸºäº**å­—èŠ‚**çš„åˆ‡åˆ†ã€‚ **åŸºäºSubwordçš„åˆ‡åˆ†æ˜¯ç›®å‰çš„ä¸»æµåˆ‡åˆ†æ–¹å¼ã€‚**
2. Subwordçš„åˆ‡åˆ†åŒ…æ‹¬: BPEã€BBPEï¼ŒWordPiece ï¼ŒUnigram ****å››ç§åˆ†è¯æ¨¡å‹ã€‚å…¶ä¸­WordPieceå¯ä»¥è®¤ä¸ºæ˜¯ä¸€ç§ç‰¹æ®Šçš„BPEã€‚
3. åŸºäºå­—èŠ‚çš„åˆ‡åˆ†æœ‰ï¼š**BBPE**ï¼Œè·¨è¯­è¨€èƒ½åŠ›å¼ºï¼Œé€‚åˆå¤šè¯­è¨€ä»»åŠ¡ã€‚
4. å®Œæ•´çš„åˆ†è¯æµç¨‹åŒ…æ‹¬ï¼šæ–‡æœ¬å½’ä¸€åŒ– â¡ï¸ é¢„åˆ‡åˆ† â¡ï¸Â åŸºäºåˆ†è¯æ¨¡å‹çš„åˆ‡åˆ† â¡ï¸Â åå¤„ç†ã€‚
5. SentencePieceæ˜¯ä¸€ä¸ªåˆ†è¯å·¥å…·ï¼Œå†…ç½®BEPç­‰å¤šç§åˆ†è¯æ–¹æ³•ï¼ŒåŸºäºUnicodeç¼–ç å¹¶ä¸”å°†ç©ºæ ¼è§†ä¸ºç‰¹æ®Šçš„tokenã€‚è¿™æ˜¯å½“å‰å¤§æ¨¡å‹çš„ä¸»æµåˆ†è¯æ–¹æ¡ˆã€‚

| **åˆ†è¯æ–¹æ³•** | **å…¸å‹æ¨¡å‹** |
| --- | --- |
| BPE | GPT, GPT-2, GPT-J, GPT-Neo, RoBERTa, BART, LLaMA, ChatGLM-6B, Baichuan |
| BBPE |  |
| WordPiece | BERT, DistilBERTï¼ŒMobileBERT |
| Unigram | AlBERT, T5, mBART, XLNet |

## 3. **åˆ‡åˆ†æµç¨‹**

TokenizeråŒ…æ‹¬**è®­ç»ƒ**å’Œ**æ¨ç†**ä¸¤ä¸ªç¯èŠ‚ï¼š

è®­ç»ƒé˜¶æ®µï¼šæŒ‡å¾—æ˜¯ä»è¯­æ–™åº“è®­ç»ƒå¾—åˆ°ä¸€ä¸ªåˆ†è¯å™¨æ¨¡å‹

æ¨ç†é˜¶æ®µï¼šæŒ‡çš„æ˜¯ç»™å®šä¸€ä¸ªå¥å­ï¼ŒåŸºäºåˆ†è¯æ¨¡å‹åˆ‡åˆ†æˆä¸€è¿ä¸²çš„token

åŸºæœ¬çš„æµç¨‹å¦‚å›¾æ‰€ç¤ºï¼ŒåŒ…æ‹¬**å½’ä¸€åŒ–**ï¼Œ**é¢„åˆ†è¯**ï¼Œ**åŸºäºåˆ†è¯æ¨¡å‹çš„åˆ‡åˆ†**ï¼Œ**åå¤„ç†**4ä¸ªæ­¥éª¤ã€‚

![](https://pic2.zhimg.com/80/v2-651f237fb96410b1000c94fa85645c1d_1440w.webp)

### **5.1. å½’ä¸€åŒ–**

è¿™æ˜¯æœ€åŸºç¡€çš„æ–‡æœ¬æ¸…æ´—ï¼ŒåŒ…æ‹¬åˆ é™¤å¤šä½™çš„æ¢è¡Œå’Œç©ºæ ¼ï¼Œè½¬å°å†™ï¼Œç§»é™¤éŸ³è°ƒç­‰ã€‚ä¾‹å¦‚ï¼š

```markdown
input: HÃ©llÃ² hÃ´w are Ã¼?
normalization: hello how are u?
```

HuggingFace tokenizerçš„å®ç°ï¼šÂ [https://huggingface.co/docs/tokenizers/api/normalizers](https://link.zhihu.com/?target=https%3A//huggingface.co/docs/tokenizers/api/normalizers)

### **5.2. é¢„åˆ†è¯**

é¢„åˆ†è¯é˜¶æ®µä¼šæŠŠå¥å­åˆ‡åˆ†æˆæ›´å°çš„â€œè¯â€å•å…ƒã€‚å¯ä»¥åŸºäºç©ºæ ¼æˆ–è€…æ ‡ç‚¹è¿›è¡Œåˆ‡åˆ†ã€‚ ä¸åŒçš„tokenizerçš„å®ç°ç»†èŠ‚ä¸ä¸€æ ·ã€‚ä¾‹å¦‚:

```markdown
input: Hello, how are you?

pre-tokenize:
[BERT]: [('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]
[GPT2]: [('Hello', (0, 5)), (',', (5, 6)), ('Ä how', (6, 10)), ('Ä are', (10, 14)), ('Ä ', (14, 15)), ('Ä you', (15, 19)), ('?', (19, 20))]
[t5]: [('â–Hello,', (0, 6)), ('â–how', (7, 10)), ('â–are', (11, 14)), ('â–you?', (16, 20))]
```

å¯ä»¥çœ‹åˆ°BERTçš„tokenizerå°±æ˜¯ç›´æ¥åŸºäºç©ºæ ¼å’Œæ ‡ç‚¹è¿›è¡Œåˆ‡åˆ†ã€‚ GPT2ä¹Ÿæ˜¯åŸºäºç©ºæ ¼å’Œæ ‡ç­¾ï¼Œä½†æ˜¯ç©ºæ ¼ä¼šä¿ç•™æˆç‰¹æ®Šå­—ç¬¦â€œÄ â€ã€‚ T5åˆ™åªåŸºäºç©ºæ ¼è¿›è¡Œåˆ‡åˆ†ï¼Œæ ‡ç‚¹ä¸ä¼šåˆ‡åˆ†ã€‚å¹¶ä¸”ç©ºæ ¼ä¼šä¿ç•™æˆç‰¹æ®Šå­—ç¬¦"â–"ï¼Œå¹¶ä¸”å¥å­å¼€å¤´ä¹Ÿä¼šæ·»åŠ ç‰¹æ®Šå­—ç¬¦"â–"ã€‚

é¢„åˆ†è¯çš„å®ç°ï¼šÂ [https://huggingface.co/docs/tokenizers/api/pre-tokenizers](https://link.zhihu.com/?target=https%3A//huggingface.co/docs/tokenizers/api/pre-tokenizers)

### **5.3. åŸºäºåˆ†è¯æ¨¡å‹çš„åˆ‡åˆ†**

è¿™é‡ŒæŒ‡çš„å°±æ˜¯ä¸åŒåˆ†è¯æ¨¡å‹å…·ä½“çš„åˆ‡åˆ†æ–¹å¼ã€‚åˆ†è¯æ¨¡å‹åŒ…æ‹¬ï¼šBPEï¼ŒWordPiece å’Œ Unigram ä¸‰ç§åˆ†è¯æ¨¡å‹ã€‚

åˆ†è¯æ¨¡å‹çš„å®ç°ï¼šÂ [https://huggingface.co/docs/tokenizers/api/models](https://link.zhihu.com/?target=https%3A//huggingface.co/docs/tokenizers/api/models)

### **5.4. åå¤„ç†**

åå¤„ç†é˜¶æ®µä¼šåŒ…æ‹¬ä¸€äº›ç‰¹æ®Šçš„åˆ†è¯é€»è¾‘ï¼Œä¾‹å¦‚æ·»åŠ Sepcial tokenï¼š[CLS], [SEP]ç­‰ã€‚

åå¤„ç†çš„å®ç°ï¼šÂ [https://huggingface.co/docs/tokenizers/api/post-processors](https://link.zhihu.com/?target=https%3A//huggingface.co/docs/tokenizers/api/post-processors)

## 4. åŸºäºå­—ã€è¯çš„åˆ‡åˆ†ï¼ˆä¸é‡è¦ï¼‰

| **æŒ‡æ ‡** | **åŸºäºå­—çš„åˆ‡åˆ†ï¼ˆCharacter-levelï¼‰** | **åŸºäºè¯çš„åˆ‡åˆ†ï¼ˆWord-levelï¼‰** |
| --- | --- | --- |
| **é€‚ç”¨è¯­è¨€** | é€‚ç”¨äºæ— æ˜æ˜¾è¯ç•Œé™çš„è¯­è¨€ï¼Œå¦‚ä¸­æ–‡ã€æ—¥æ–‡ç­‰ | é€‚ç”¨äºæœ‰æ˜æ˜¾ç©ºæ ¼åˆ†éš”çš„è¯­è¨€ï¼Œå¦‚è‹±è¯­ã€æ³•è¯­ç­‰ |
| **OOVé—®é¢˜** | ä¸å­˜åœ¨OOVé—®é¢˜ | å­˜åœ¨OOVé—®é¢˜ |
| **è®¡ç®—æ•ˆç‡** | è®¡ç®—æ•ˆç‡è¾ƒä½ï¼Œtoken æ•°é‡å¤š | è®¡ç®—æ•ˆç‡è¾ƒé«˜ï¼Œtoken æ•°é‡å°‘ |
| **è¯­ä¹‰ä¿¡æ¯** | ä¿¡æ¯è¾ƒä¸ºç»†ç²’åº¦ï¼Œä½†è¾ƒä¸ºå±€é™ | ä¿¡æ¯è¾ƒä¸ºæŠ½è±¡ï¼Œèƒ½è¾ƒå¥½ä¿ç•™å•è¯è¯­ä¹‰ |
| **å¯¹å½¢æ€å˜åŒ–çš„é€‚åº”æ€§** | é€‚åº”æ€§å¼ºï¼Œèƒ½å¤Ÿå¤„ç†å¤åˆè¯æˆ–å˜åŒ–å½¢æ€ | éœ€è¦è¯å½¢é¢„å¤„ç†ï¼Œéš¾ä»¥å¤„ç†å½¢æ€å˜åŒ–ä¸°å¯Œçš„è¯­è¨€ |
| **åº”ç”¨åœºæ™¯** | é€‚ç”¨äºå½¢æ€ä¸°å¯Œã€å¤šè¯­è¨€æ··åˆçš„æƒ…å†µ | é€‚ç”¨äºè¯­è¨€ä¹‹é—´æœ‰æ˜æ˜¾è¯ç•Œé™çš„ä»»åŠ¡ |

<aside>
ğŸ’¡

**OOV:** åœ¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæ¨¡å‹åªä¼šå¤„ç†è¯æ±‡è¡¨ä¸­çš„å•è¯ï¼Œå¯¹äºè¯æ±‡è¡¨ä¹‹å¤–çš„å•è¯ï¼ˆOOVï¼‰

</aside>

## **5. åŸºäºSubwordçš„åˆ‡åˆ†**

ç›¸è¾ƒäºåŸºäº**è¯**å’ŒåŸºäº**å­—**çš„åˆ‡åˆ†ï¼ŒSubwordå°±æ˜¯ä¸€ç§ç›¸å¯¹å¹³è¡¡çš„æ–¹æ¡ˆï¼Œæ˜¯ç›®å‰ä¸»æµæœ€ä¸»æµçš„åˆ‡åˆ†æ–¹å¼ã€‚

Subwordçš„åŸºæœ¬åˆ‡åˆ†åŸåˆ™æ˜¯ï¼š

- é«˜é¢‘è¯ä¾æ—§åˆ‡åˆ†æˆå®Œæ•´çš„æ•´è¯
- ä½é¢‘è¯è¢«åˆ‡åˆ†æˆæœ‰æ„ä¹‰çš„å­è¯ï¼Œä¾‹å¦‚ dogs => [dog, ##s]

åŸºäºSubwordçš„åˆ‡åˆ†å¯ä»¥å®ç°ï¼š

- è¯è¡¨è§„æ¨¡é€‚ä¸­ï¼Œè§£ç æ•ˆç‡è¾ƒé«˜
- ä¸å­˜åœ¨UNKï¼Œä¿¡æ¯ä¸ä¸¢å¤±
- èƒ½å­¦ä¹ åˆ°è¯ç¼€ä¹‹é—´çš„å…³ç³»

åŸºäºSubwordçš„åˆ‡åˆ†åŒ…æ‹¬ï¼šBPEï¼ŒWordPiece å’Œ Unigram ä¸‰ç§åˆ†è¯æ¨¡å‹ã€‚

## **6. BPE**

Byte-Pair Encoding(BPE)æ˜¯æœ€å¹¿æ³›é‡‡ç”¨çš„subwordåˆ†è¯å™¨ã€‚

- è®­ç»ƒæ–¹æ³•ï¼šä»å­—ç¬¦çº§çš„å°è¯è¡¨å‡ºå‘ï¼Œ**è®­ç»ƒäº§ç”Ÿåˆå¹¶è§„åˆ™ä»¥åŠä¸€ä¸ªè¯è¡¨**
- ç¼–ç æ–¹æ³•ï¼šå°†æ–‡æœ¬åˆ‡åˆ†æˆå­—ç¬¦ï¼Œå†åº”ç”¨è®­ç»ƒé˜¶æ®µè·å¾—çš„åˆå¹¶è§„åˆ™
- ç»å…¸æ¨¡å‹ï¼šGPT, GPT-2, RoBERTa, BART, LLaMA, ChatGLMç­‰

### **6.1. è®­ç»ƒé˜¶æ®µ**

åœ¨è®­ç»ƒç¯èŠ‚ï¼Œç›®æ ‡æ˜¯ç»™å®šè¯­æ–™ï¼Œé€šè¿‡è®­ç»ƒç®—æ³•ï¼Œç”Ÿæˆ**åˆå¹¶è§„åˆ™**å’Œ**è¯è¡¨**ã€‚ BPEç®—æ³•æ˜¯ä»ä¸€ä¸ªå­—ç¬¦çº§åˆ«çš„è¯è¡¨ä¸ºåŸºç¡€ï¼Œåˆå¹¶pairå¹¶æ·»åŠ åˆ°è¯è¡¨ä¸­ï¼Œé€æ­¥å½¢æˆå¤§è¯è¡¨ã€‚åˆå¹¶è§„åˆ™ä¸ºé€‰æ‹©ç›¸é‚»pairè¯é¢‘æœ€å¤§çš„è¿›è¡Œåˆå¹¶ã€‚

ä¸‹é¢æˆ‘ä»¬è¿›è¡Œæ‰‹å·¥çš„å®ç°ã€‚

å‡å®šè®­ç»ƒçš„è¯­æ–™(å·²å½’ä¸€åŒ–å¤„ç†)ä¸º4ä¸ªå¥å­ã€‚

```python
corpus **=** [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

é¦–å…ˆè¿›è¡Œé¢„åˆ‡åˆ†å¤„ç†ã€‚è¿™é‡Œé‡‡ç”¨gpt2çš„é¢„åˆ‡åˆ†é€»è¾‘ã€‚ å…·ä½“ä¼šæŒ‰ç…§ç©ºæ ¼å’Œæ ‡ç‚¹è¿›è¡Œåˆ‡åˆ†ï¼Œå¹¶ä¸”ç©ºæ ¼ä¼šä¿ç•™æˆç‰¹æ®Šçš„å­—ç¬¦â€œÄ â€ã€‚

```python
**from** transformers **import** AutoTokenizer

*# init pre tokenize function*
gpt2_tokenizer **=** AutoTokenizer**.**from_pretrained("gpt2")
pre_tokenize_str **=** gpt2_tokenizer**.**backend_tokenizer**.**pre_tokenizer**.**pre_tokenize_str

*# pre tokenize*
pre_tokenized_corpus **=** [pre_tokenize_str(text) **for** text **in** corpus]
```

è·å¾—çš„pre_tokenized_corpuså¦‚ä¸‹ï¼Œæ¯ä¸ªå•å…ƒåˆ†åˆ«ä¸º[word, (start_index, end_index)]

```python
[
    [('This', (0, 4)), ('Ä is', (4, 7)), ('Ä the', (7, 11)), ('Ä Hugging', (11, 19)), ('Ä Face', (19, 24)), ('Ä Course', (24, 31)), ('.', (31, 32))], 
    [('This', (0, 4)), ('Ä chapter', (4, 12)), ('Ä is', (12, 15)), ('Ä about', (15, 21)), ('Ä tokenization', (21, 34)), ('.', (34, 35))], 
    [('This', (0, 4)), ('Ä section', (4, 12)), ('Ä shows', (12, 18)), ('Ä several', (18, 26)), ('Ä tokenizer', (26, 36)), ('Ä algorithms', (36, 47)), ('.', (47, 48))], 
    [('Hopefully', (0, 9)), (',', (9, 10)), ('Ä you', (10, 14)), ('Ä will', (14, 19)), ('Ä be', (19, 22)), ('Ä able', (22, 27)), ('Ä to', (27, 30)), ('Ä understand', (30, 41)), ('Ä how', (41, 45)), ('Ä they', (45, 50)), ('Ä are', (50, 54)), ('Ä trained', (54, 62)), ('Ä and', (62, 66)), ('Ä generate', (66, 75)), ('Ä tokens', (75, 82)), ('.', (82, 83))]
]
```

è¿›ä¸€æ­¥ç»Ÿè®¡æ¯ä¸ªæ•´è¯çš„è¯é¢‘

```python
word2count **=** defaultdict(int)
**for** split_text **in** pre_tokenized_corpus:
    **for** word, _ **in** split_text:
        word2count[word] **+=** 1
```

è·å¾—word2countå¦‚ä¸‹

```python
defaultdict(<class 'int'>, {'This': 3, 'Ä is': 2, 'Ä the': 1, 'Ä Hugging': 1, 'Ä Face': 1, 'Ä Course': 1, '.': 4, 'Ä chapter': 1, 'Ä about': 1, 'Ä tokenization': 1, 'Ä section': 1, 'Ä shows': 1, 'Ä several': 1, 'Ä tokenizer': 1, 'Ä algorithms': 1, 'Hopefully': 1, ',': 1, 'Ä you': 1, 'Ä will': 1, 'Ä be': 1, 'Ä able': 1, 'Ä to': 1, 'Ä understand': 1, 'Ä how': 1, 'Ä they': 1, 'Ä are': 1, 'Ä trained': 1, 'Ä and': 1, 'Ä generate': 1, 'Ä tokens': 1})
```

å› ä¸ºBPEæ˜¯ä»å­—ç¬¦çº§åˆ«çš„å°è¯è¡¨ï¼Œé€æ­¥åˆå¹¶æˆå¤§è¯è¡¨ï¼Œæ‰€ä»¥éœ€è¦å…ˆè·å¾—å­—ç¬¦çº§åˆ«çš„å°è¯è¡¨ã€‚

```python
vocab_set **=** set()
**for** word **in** word2count:
    vocab_set**.**update(list(word))
vocabs **=** list(vocab_set)
```

è·å¾—çš„åˆå§‹å°è¯è¡¨vocabså¦‚ä¸‹:

```python
['i', 't', 'p', 'o', 'r', 'm', 'e', ',', 'y', 'v', 'Ä ', 'F', 'a', 'C', 'H', '.', 'f', 'l', 'u', 'c', 'T', 'k', 'h', 'z', 'd', 'g', 'w', 'n', 's', 'b']
```

åŸºäºå°è¯è¡¨å°±å¯ä»¥å¯¹æ¯ä¸ªæ•´è¯è¿›è¡Œåˆ‡åˆ†

```python
word2splits **=** {word: [c **for** c **in** word] **for** word **in** word2count}

'This': ['T', 'h', 'i', 's'], 
'Ä is': ['Ä ', 'i', 's'], 
'Ä the': ['Ä ', 't', 'h', 'e'], 
**...**
'Ä and': ['Ä ', 'a', 'n', 'd'], 
'Ä generate': ['Ä ', 'g', 'e', 'n', 'e', 'r', 'a', 't', 'e'], 
'Ä tokens': ['Ä ', 't', 'o', 'k', 'e', 'n', 's']
```

åŸºäºword2splitsç»Ÿè®¡vocabsä¸­ç›¸é‚»ä¸¤ä¸ªpairçš„è¯é¢‘pair2count

```python
**def** **_compute_pair2score**(word2splits, word2count):
    pair2count **=** defaultdict(int)
    **for** word, word_count **in** word2count**.**items():
        split **=** word2splits[word]
        **if** len(split) **==** 1:
            **continuefor** i **in** range(len(split) **-** 1):
            pair **=** (split[i], split[i **+** 1])
            pair2count[pair] **+=** word_count
    **return** pair2count
```

è·å¾—pair2countå¦‚ä¸‹ï¼š

```python
defaultdict(**<class** '**int**'>, {('T', 'h'): 3, ('h', 'i'): 3, ('i', 's'): 5, ('Ä ', 'i'): 2, ('Ä ', 't'): 7, ('t', 'h'): 3, ..., ('n', 's'): 1})
```

ç»Ÿè®¡å½“å‰é¢‘ç‡æœ€é«˜çš„ç›¸é‚»pair

```python
**def** **_compute_most_score_pair**(pair2count):
    best_pair **=** **None**
		max_freq **=** **None
		for** pair, freq **in** pair2count**.**items():
        **if** max_freq **is** **None** **or** max_freq **<** freq:
            best_pair **=** pair
            max_freq **=** freq
    **return** best_pair
```

ç»è¿‡ç»Ÿè®¡ï¼Œå½“å‰é¢‘ç‡æœ€é«˜çš„pairä¸º: ('Ä ', 't')ï¼Œ é¢‘ç‡ä¸º7æ¬¡ã€‚ å°†('Ä ', 't')åˆå¹¶æˆä¸€ä¸ªè¯å¹¶æ·»åŠ åˆ°è¯è¡¨ä¸­ã€‚åŒæ—¶åœ¨åˆå¹¶è§„åˆ™ä¸­æ·»åŠ ('Ä ', 't')è¿™æ¡åˆå¹¶è§„åˆ™ã€‚

```python
merge_rules **=** []
best_pair **=** self**.**_compute_most_score_pair(pair2score)
vocabs**.**append(best_pair[0] **+** best_pair[1])
merge_rules**.**append(best_pair)
```

æ­¤æ—¶çš„vocabè¯è¡¨æ›´æ–°æˆ:

```python
['i', 't', 'p', 'o', 'r', 'm', 'e', ',', 'y', 'v', 'Ä ', 'F', 'a', 'C', 'H', '.', 'f', 'l', 'u', 'c', 'T', 'k', 'h', 'z', 'd', 'g', 'w', 'n', 's', 'b', 
'Ä t']
```

æ ¹æ®æ›´æ–°åçš„vocabé‡æ–°å¯¹word2countè¿›è¡Œåˆ‡åˆ†ã€‚å…·ä½“å®ç°ä¸Šï¼Œå¯ä»¥ç›´æ¥åœ¨æ—§çš„word2splitä¸Šåº”ç”¨æ–°çš„åˆå¹¶è§„åˆ™('Ä ', 't')

```python
**def** **_merge_pair**(a, b, word2splits):
    new_word2splits **=** dict()
    **for** word, split **in** word2splits**.**items():
        **if** len(split) **==** 1:
            new_word2splits[word] **=** split
            **continue**
				i **=** 0
        **while** i **<** len(split) **-** 1:
            **if** split[i] **==** a **and** split[i **+** 1] **==** b:
                split **=** split[:i] **+** [a **+** b] **+** split[i **+** 2:]
            **else**:
                i **+=** 1
        new_word2splits[word] **=** split
    **return** new_word2splits
```

ä»è€Œè·å¾—æ–°çš„word2split

```python
{'This': ['T', 'h', 'i', 's'], 
'Ä is': ['Ä ', 'i', 's'], 
'Ä the': ['Ä t', 'h', 'e'], 
'Ä Hugging': ['Ä ', 'H', 'u', 'g', 'g', 'i', 'n', 'g'],
**...**
'Ä tokens': ['Ä t', 'o', 'k', 'e', 'n', 's']}
```

å¯ä»¥çœ‹åˆ°æ–°çš„word2splitä¸­å·²ç»åŒ…å«äº†æ–°çš„è¯"Ä t"ã€‚

é‡å¤ä¸Šè¿°å¾ªç¯ç›´åˆ°æ•´ä¸ªè¯è¡¨çš„å¤§å°è¾¾åˆ°é¢„å…ˆè®¾å®šçš„è¯è¡¨å¤§å°ã€‚

```python
**while** len(vocabs) **<** vocab_size:
    pair2score **=** self**.**_compute_pair2score(word2splits, word2count)
    best_pair **=** self**.**_compute_most_score_pair(pair2score)
    vocabs**.**append(best_pair[0] **+** best_pair[1])
    merge_rules**.**append(best_pair)
    word2splits **=** self**.**_merge_pair(best_pair[0], best_pair[1], word2splits)
```

å‡å®šæœ€ç»ˆè¯è¡¨çš„å¤§å°ä¸º50ï¼Œç»è¿‡ä¸Šè¿°è¿­ä»£åæˆ‘ä»¬è·å¾—çš„è¯è¡¨å’Œåˆå¹¶è§„åˆ™å¦‚ä¸‹ï¼š

```python
vocabs **=** ['i', 't', 'p', 'o', 'r', 'm', 'e', ',', 'y', 'v', 'Ä ', 'F', 'a', 'C', 'H', '.', 'f', 'l', 'u', 'c', 'T', 'k', 'h', 'z', 'd', 'g', 'w', 'n', 's', 'b', 'Ä t', 'is', 'er', 'Ä a', 'Ä to', 'en', 'Th', 'This', 'ou', 'se', 'Ä tok', 'Ä token', 'nd', 'Ä is', 'Ä th', 'Ä the', 'in', 'Ä ab', 'Ä tokeni', 'Ä tokeniz']

merge_rules **=** [('Ä ', 't'), ('i', 's'), ('e', 'r'), ('Ä ', 'a'), ('Ä t', 'o'), ('e', 'n'), ('T', 'h'), ('Th', 'is'), ('o', 'u'), ('s', 'e'), ('Ä to', 'k'), ('Ä tok', 'en'), ('n', 'd'), ('Ä ', 'is'), ('Ä t', 'h'), ('Ä th', 'e'), ('i', 'n'), ('Ä a', 'b'), ('Ä token', 'i'), ('Ä tokeni', 'z')]
```

è‡³æ­¤æˆ‘ä»¬å°±æ ¹æ®ç»™å®šçš„è¯­æ–™å®Œæˆäº†BPEåˆ†è¯å™¨çš„è®­ç»ƒã€‚

### **6.2. æ¨ç†é˜¶æ®µ**

åœ¨æ¨ç†é˜¶æ®µï¼Œç»™å®šä¸€ä¸ªå¥å­ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶åˆ‡åˆ†æˆä¸€ä¸ªtokençš„åºåˆ—ã€‚ å…·ä½“å®ç°ä¸Šéœ€è¦å…ˆå¯¹å¥å­è¿›è¡Œé¢„åˆ†è¯å¹¶åˆ‡åˆ†æˆå­—ç¬¦çº§åˆ«çš„åºåˆ—ï¼Œç„¶åæ ¹æ®åˆå¹¶è§„åˆ™è¿›è¡Œåˆå¹¶ã€‚

```python
**def** **tokenize**(self, text: str) **->** List[str]:
    *# pre tokenize*
		words **=** [word **for** word, _ **in** self**.**pre_tokenize_str(text)]
    *# split into char level*
		splits **=** [[c **for** c **in** word] **for** word **in** words]
    *# apply merge rules*
		**for** merge_rule **in** self**.**merge_rules:
        **for** index, split **in** enumerate(splits):
            i **=** 0
            **while** i **<** len(split) **-** 1:
                **if** split[i] **==** merge_rule[0] **and** split[i **+** 1] **==** merge_rule[1]:
                    split **=** split[:i] **+** [""**.**join(merge_rule)] **+** split[i **+** 2:]
                **else**:
                    i **+=** 1
            splits[index] **=** split
    **return** sum(splits, [])
```

ä¾‹å¦‚

```python
**>>>** tokenize("This is not a token.")
**>>>** ['This', 'Ä is', 'Ä ', 'n', 'o', 't', 'Ä a', 'Ä token', '.']
```

## **7. â€¼ï¸BBPE**

2019å¹´æå‡ºçš„Byte-level BPE (BBPE)ç®—æ³•æ˜¯ä¸Šé¢BPEç®—æ³•çš„è¿›ä¸€æ­¥å‡çº§ã€‚å…·ä½“å‚è§ï¼š[Neural Machine Translation with Byte-Level Subwords](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1909.03341.pdf)ã€‚ æ ¸å¿ƒæ€æƒ³æ˜¯ç”¨byteæ¥æ„å»ºæœ€åŸºç¡€çš„è¯è¡¨è€Œä¸æ˜¯å­—ç¬¦ã€‚é¦–å…ˆå°†æ–‡æœ¬æŒ‰ç…§UTF-8è¿›è¡Œç¼–ç ï¼Œæ¯ä¸ªå­—ç¬¦åœ¨UTF-8çš„è¡¨ç¤ºä¸­å æ®1-4ä¸ªbyteã€‚ åœ¨byteåºåˆ—ä¸Šå†ä½¿ç”¨BPEç®—æ³•ï¼Œè¿›è¡Œbyte levelçš„ç›¸é‚»åˆå¹¶ã€‚ç¼–ç å½¢å¼å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![](https://pic3.zhimg.com/80/v2-6be86b9910c22e8ef6a6ef0f7d3c337e_1440w.webp)

é€šè¿‡è¿™ç§æ–¹å¼å¯ä»¥æ›´å¥½çš„å¤„ç†è·¨è¯­è¨€å’Œä¸å¸¸è§å­—ç¬¦çš„ç‰¹æ®Šé—®é¢˜(ä¾‹å¦‚ï¼Œé¢œæ–‡å­—)ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„BPEæ›´èŠ‚çœè¯è¡¨ç©ºé—´ï¼ˆåŒç­‰è¯è¡¨å¤§å°æ•ˆæœæ›´å¥½ï¼‰ï¼Œæ¯ä¸ªtokenä¹Ÿèƒ½è·å¾—æ›´å……åˆ†çš„è®­ç»ƒã€‚

ä½†æ˜¯åœ¨è§£ç é˜¶æ®µï¼Œä¸€ä¸ªbyteåºåˆ—å¯èƒ½è§£ç åä¸æ˜¯ä¸€ä¸ªåˆæ³•çš„å­—ç¬¦åºåˆ—ï¼Œè¿™é‡Œéœ€è¦é‡‡ç”¨**åŠ¨æ€è§„åˆ’**çš„ç®—æ³•è¿›è¡Œè§£ç ï¼Œä½¿å…¶èƒ½è§£ç å‡ºå°½å¯èƒ½å¤šçš„åˆæ³•å­—ç¬¦ã€‚å…·ä½“ç®—æ³•å¦‚ä¸‹ï¼š 

å‡å®š$f(k)$è¡¨ç¤ºå­—ç¬¦åºåˆ—$B_{1,k}$æœ€å¤§èƒ½è§£ç çš„åˆæ³•å­—ç¬¦æ•°é‡ï¼Œ$f(k)$æœ‰æœ€ä¼˜çš„å­ç»“æ„ï¼š

$f(k) = max_{t=1,2,3,4}f(k-t)+g(k-t+1, k)$

è¿™é‡Œå¦‚æœ$B_{i,j}$ä¸ºä¸€ä¸ªåˆæ³•å­—ç¬¦$g_{i,j}=1$ï¼Œå¦åˆ™$g_{i,j}=1$ã€‚

## **8. WordPiece**

WordPieceåˆ†è¯ä¸BPEéå¸¸ç±»ä¼¼ï¼Œåªæ˜¯åœ¨è®­ç»ƒé˜¶æ®µåˆå¹¶pairçš„ç­–ç•¥ä¸æ˜¯pairçš„é¢‘ç‡è€Œæ˜¯äº’ä¿¡æ¯ã€‚

$score=log(p(ab)) - (log(p(a)) + log(p(b)))=log(p(ab)/p(a)p(b))$

è¿™é‡Œçš„åŠ¨æœºæ˜¯ä¸€ä¸ªpairçš„é¢‘ç‡å¾ˆé«˜ï¼Œä½†æ˜¯å…¶ä¸­pairçš„ä¸€éƒ¨åˆ†çš„é¢‘ç‡æ›´é«˜ï¼Œè¿™æ—¶å€™ä¸ä¸€å®šéœ€è¦è¿›è¡Œè¯¥pairçš„åˆå¹¶ã€‚ è€Œå¦‚æœä¸€ä¸ªpairçš„é¢‘ç‡å¾ˆé«˜ï¼Œå¹¶ä¸”è¿™ä¸ªpairçš„ä¸¤ä¸ªéƒ¨åˆ†éƒ½æ˜¯åªå‡ºç°åœ¨è¿™ä¸ªpairä¸­ï¼Œå°±è¯´æ˜è¿™ä¸ªpairå¾ˆå€¼å¾—åˆå¹¶ã€‚

- è®­ç»ƒæ–¹æ³•ï¼šä»å­—ç¬¦çº§çš„å°è¯è¡¨å‡ºå‘ï¼Œè®­ç»ƒäº§ç”Ÿåˆå¹¶è§„åˆ™ä»¥åŠä¸€ä¸ªè¯è¡¨
- ç¼–ç æ–¹æ³•ï¼šå°†æ–‡æœ¬åˆ‡åˆ†æˆè¯ï¼Œå¯¹æ¯ä¸ªè¯åœ¨è¯è¡¨ä¸­è¿›è¡Œæœ€å¤§å‰å‘åŒ¹é…
- ç»å…¸æ¨¡å‹ï¼šBERTåŠå…¶ç³»åˆ—DistilBERTï¼ŒMobileBERTç­‰

## **4.1. è®­ç»ƒé˜¶æ®µ**

åœ¨è®­ç»ƒç¯èŠ‚ï¼Œç»™å®šè¯­æ–™ï¼Œé€šè¿‡è®­ç»ƒç®—æ³•ï¼Œç”Ÿæˆæœ€ç»ˆçš„è¯è¡¨ã€‚ WordPieceç®—æ³•ä¹Ÿæ˜¯ä»ä¸€ä¸ªå­—ç¬¦çº§åˆ«çš„è¯è¡¨ä¸ºåŸºç¡€ï¼Œé€æ­¥æ‰©å……æˆå¤§è¯è¡¨ã€‚åˆå¹¶è§„åˆ™ä¸ºé€‰æ‹©ç›¸é‚»pairäº’ä¿¡æ¯æœ€å¤§çš„è¿›è¡Œåˆå¹¶ã€‚

ä¸‹é¢è¿›è¡Œå…·ä½“æ‰‹å·¥å®ç°ã€‚

å‡å®šè®­ç»ƒçš„è¯­æ–™(å·²å½’ä¸€åŒ–å¤„ç†)ä¸º

```python
corpus **=** [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

é¦–å…ˆè¿›è¡Œé¢„åˆ‡åˆ†å¤„ç†ã€‚è¿™é‡Œé‡‡ç”¨BERTçš„é¢„åˆ‡åˆ†é€»è¾‘ã€‚å…·ä½“ä¼šæŒ‰ç…§ç©ºæ ¼å’Œæ ‡ç‚¹è¿›è¡Œåˆ‡åˆ†ã€‚

```python
**from** transformers **import** AutoTokenizer

*# init pre tokenize function*
bert_tokenizer **=** AutoTokenizer**.**from_pretrained("bert-base-cased")
pre_tokenize_function **=** bert_tokenizer**.**backend_tokenizer**.**pre_tokenizer**.**pre_tokenize_str

*# pre tokenize*
pre_tokenized_corpus **=** [pre_tokenize_str(text) **for** text **in** corpus]
```

è·å¾—çš„pre_tokenized_corpuså¦‚ä¸‹ï¼Œæ¯ä¸ªå•å…ƒåˆ†åˆ«ä¸º[word, (start_index, end_index)]

```python
[
    [('This', (0, 4)), ('is', (5, 7)), ('the', (8, 11)), ('Hugging', (12, 19)), ('Face', (20, 24)), ('Course', (25, 31)), ('.', (31, 32))], 
    [('This', (0, 4)), ('chapter', (5, 12)), ('is', (13, 15)), ('about', (16, 21)), ('tokenization', (22, 34)), ('.', (34, 35))], 
    [('This', (0, 4)), ('section', (5, 12)), ('shows', (13, 18)), ('several', (19, 26)), ('tokenizer', (27, 36)), ('algorithms', (37, 47)), ('.', (47, 48))], 
    [('Hopefully', (0, 9)), (',', (9, 10)), ('you', (11, 14)), ('will', (15, 19)), ('be', (20, 22)), ('able', (23, 27)), ('to', (28, 30)), ('understand', (31, 41)), ('how', (42, 45)), ('they', (46, 50)), ('are', (51, 54)), ('trained', (55, 62)), ('and', (63, 66)), ('generate', (67, 75)), ('tokens', (76, 82)), ('.', (82, 83))]
]
```

è¿›ä¸€æ­¥ç»Ÿè®¡è¯é¢‘

```python
word2count **=** defaultdict(int)
**for** split_text **in** pre_tokenized_corpus:
    **for** word, _ **in** split_text:
        word2count[word] **+=** 1
```

è·å¾—word2countå¦‚ä¸‹

```python
defaultdict(**<class** '**int**'>, {'This': 3, '**is**': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '**.**': 4, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1, ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1, 'trained': 1, '**and**': 1, 'generate': 1, 'tokens': 1})
```

å› ä¸ºWordPieceåŒæ ·æ˜¯ä»å­—ç¬¦çº§åˆ«çš„å°è¯è¡¨ï¼Œé€æ­¥åˆå¹¶æˆå¤§è¯è¡¨ï¼Œæ‰€ä»¥å…ˆè·å¾—å­—ç¬¦çº§åˆ«çš„å°è¯è¡¨ã€‚æ³¨æ„è¿™é‡Œå¦‚æœå­—ç¬¦ä¸æ˜¯åœ¨ä¸€ä¸ªè¯çš„å¼€å§‹ï¼Œéœ€è¦æ·»åŠ ä¸Šç‰¹æ®Šå­—ç¬¦"##"ã€‚

```python
vocab_set **=** set()
**for** word **in** word2count:
    vocab_set**.**add(word[0])
    vocab_set**.**update(['##' **+** c **for** c **in** word[1:]])
vocabs **=** list(vocab_set)
```

è·å¾—çš„åˆå§‹å°è¯è¡¨vocabså¦‚ä¸‹:

```python
['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y']
```

åŸºäºå°è¯è¡¨å¯¹æ¯ä¸ªè¯è¿›è¡Œåˆ‡åˆ†

```python
word2splits **=** {word: [word[0]] **+** ['##' **+** c **for** c **in** word[1:]] **for** word **in** word2count}

{'This': ['T', '##h', '##i', '##s'], 
'is': ['i', '##s'], 
'the': ['t', '##h', '##e'], 
'Hugging': ['H', '##u', '##g', '##g', '##i', '##n', '##g'], 
**...**
'generate': ['g', '##e', '##n', '##e', '##r', '##a', '##t', '##e'], 
'tokens': ['t', '##o', '##k', '##e', '##n', '##s']}
```

è¿›ä¸€æ­¥ç»Ÿè®¡vocabsä¸­ç›¸é‚»ä¸¤ä¸ªpairçš„äº’ä¿¡æ¯

```python
**def** **_compute_pair2score**(word2splits, word2count):
    """
    è®¡ç®—æ¯ä¸ªpairçš„åˆ†æ•°
    score=(freq_of_pair)/(freq_of_first_elementÃ—freq_of_second_element)
    :return:
    """
    vocab2count **=** defaultdict(int)
    pair2count **=** defaultdict(int)
    **for** word, word_count **in** word2count**.**items():
        splits **=** word2splits[word]
        **if** len(splits) **==** 1:
            vocab2count[splits[0]] **+=** word_count
            **continue
				for** i **in** range(len(splits) **-** 1):
            pair **=** (splits[i], splits[i **+** 1])
            vocab2count[splits[i]] **+=** word_count
            pair2count[pair] **+=** word_count
        vocab2count[splits[**-**1]] **+=** word_count
    scores **=** {
        pair: freq **/** (vocab2count[pair[0]] ***** vocab2count[pair[1]])
        **for** pair, freq **in** pair2count**.**items()
    }
    **return** scores
```

è·å¾—æ¯ä¸ªpairçš„äº’ä¿¡æ¯å¦‚ä¸‹ï¼š

```python
{('T', '##h'): 0.125, 
('##h', '##i'): 0.03409090909090909, 
('##i', '##s'): 0.02727272727272727, 
('a', '##b'): 0.2,
**...**
('##n', '##s'): 0.00909090909090909}
```

ç»Ÿè®¡å‡ºäº’ä¿¡æ¯æœ€é«˜çš„ç›¸é‚»pair

```python
**def** **_compute_most_score_pair**(pair2score):
    best_pair **=** **None**
		max_score **=** **None
		for** pair, score **in** pair2score**.**items():
        **if** max_score **is** **None** **or** max_score **<** score:
            best_pair **=** pair
            max_score **=** score
    **return** best_pair
```

æ­¤æ—¶äº’ä¿¡æ¯æœ€é«˜çš„pairä¸º: ('a', '##b') å°†('a', '##b')åˆå¹¶æˆä¸€ä¸ªè¯'ab'å¹¶æ·»åŠ åˆ°è¯è¡¨ä¸­

```python
best_pair **=** self**.**_compute_most_score_pair(pair2score)
vocabs**.**append(best_pair[0] **+** best_pair[1])
```

è¿™æ ·vocabè¯è¡¨æ›´æ–°æˆ:

```python
['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 
'ab']
```

æ ¹æ®æ›´æ–°çš„vocabé‡æ–°å¯¹word2countè¿›è¡Œåˆ‡åˆ†ã€‚

```python
**def** **_merge_pair**(a, b, word2splits):
    new_word2splits **=** dict()
    **for** word, split **in** word2splits**.**items():
        **if** len(split) **==** 1:
            new_word2splits[word] **=** split
            **continue**
				i **=** 0
        **while** i **<** len(split) **-** 1:
            **if** split[i] **==** a **and** split[i **+** 1] **==** b:
                merge **=** a **+** b[2:] **if** b**.**startswith("##") **else** a **+** b
                split **=** split[:i] **+** [merge] **+** split[i **+** 2:]
            **else**:
                i **+=** 1
        new_word2splits[word] **=** split
    **return** new_word2splits
```

è·å¾—æ–°çš„word2split

```python
{'This': ['T', '##h', '##i', '##s'], 
'is': ['i', '##s'], 'the': ['t', '##h', '##e'], 
'Hugging': ['H', '##u', '##g', '##g', '##i', '##n', '##g'], 
'about': ['ab', '##o', '##u', '##t'], 
'tokens': ['t', '##o', '##k', '##e', '##n', '##s']}
```

å¯ä»¥çœ‹åˆ°æ–°çš„word2splitä¸­å·²ç»åŒ…å«äº†æ–°çš„è¯"ab"ã€‚

é‡å¤ä¸Šè¿°æ­¥éª¤ï¼Œç›´åˆ°æ•´ä¸ªè¯è¡¨çš„å¤§å°è¾¾åˆ°é¢„å…ˆè®¾å®šçš„è¯è¡¨å¤§å°ã€‚

```python
**while** len(vocabs) **<** vocab_size:
    pair2score **=** self**.**_compute_pair2score(word2splits, word2count)
    best_pair **=** self**.**_compute_most_score_pair(pair2score)
    word2splits **=** self**.**_merge_pair(best_pair[0], best_pair[1], word2splits)
    new_token **=** best_pair[0] **+** best_pair[1][2:] **if** best_pair[1]**.**startswith('##') **else** best_pair[1]
    vocabs**.**append(new_token)
```

å‡å®šæœ€ç»ˆè¯è¡¨çš„å¤§å°ä¸º70ï¼Œç»è¿‡ä¸Šè¿°è¿­ä»£åæˆ‘ä»¬è·å¾—çš„è¯è¡¨å¦‚ä¸‹ï¼š

```python
vocabs **=** ['##a', '##b', '##c', '##ct', '##d', '##e', '##f', '##fu', '##ful', '##full', '##fully', '##g', '##h', '##hm', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##thm', '##thms', '##u', '##ut', '##v', '##w', '##y', '##z', '##za', '##zat', ',', '.', 'C', 'F', 'Fa', 'Fac', 'H', 'Hu', 'Hug', 'Hugg', 'T', 'Th', 'a', 'ab', 'b', 'c', 'ch', 'cha', 'chap', 'chapt', 'g', 'h', 'i', 'is', 's', 'sh', 't', 'th', 'u', 'w', 'y', '[CLS]', '[MASK]', '[PAD]', '[SEP]', '[UNK]']
```

æ³¨æ„è¯è¡¨ä¸­æ·»åŠ äº†ç‰¹æ®Šçš„tokenï¼š[CLS], [MASK], [PAD], [SEP], [UNK] è‡³æ­¤æˆ‘ä»¬å°±æ ¹æ®ç»™å®šçš„è¯­æ–™å®Œæˆäº†WordPieceåˆ†è¯å™¨çš„è®­ç»ƒã€‚

## **4.2. æ¨ç†é˜¶æ®µ**

åœ¨æ¨ç†é˜¶æ®µï¼Œç»™å®šä¸€ä¸ªå¥å­ï¼Œéœ€è¦å°†å…¶åˆ‡åˆ†æˆä¸€ä¸ªtokençš„åºåˆ—ã€‚ å…·ä½“å®ç°ä¸Šéœ€è¦å…ˆå¯¹å¥å­è¿›è¡Œé¢„åˆ†è¯ï¼Œç„¶åå¯¹æ¯ä¸ªè¯è¿›è¡Œåœ¨è¯è¡¨ä¸­è¿›è¡Œæœ€å¤§å‰å‘çš„åŒ¹é…ã€‚å¦‚æœè¯è¡¨ä¸­ä¸å­˜åœ¨åˆ™ä¸ºUNKã€‚

```python
**def** **_encode_word**(self, word):
    tokens **=** []
    **while** len(word) **>** 0:
        i **=** len(word)
        **while** i **>** 0 **and** word[:i] **not** **in** self**.**vocabs:
            i **-=** 1
        **if** i **==** 0:
            **return** ["[UNK]"]
        tokens**.**append(word[:i])
        word **=** word[i:]
        **if** len(word) **>** 0:
            word **=** f"##{word}"
    **return** tokens

**def** **tokenize**(self, text):
    words **=** [word **for** word, _ **in** self**.**pre_tokenize_str(text)]
    encoded_words **=** [self**.**_encode_word(word) **for** word **in** words]
    **return** sum(encoded_words, [])
```

ä¾‹å¦‚

```python
**>>>** tokenize("This is the Hugging Face course!")
**>>>** ['Th', '##i', '##s', 'is', 'th', '##e', 'Hugg', '##i', '##n', '##g', 'Fac', '##e', 'c', '##o', '##u', '##r', '##s', '##e', '[UNK]']
```

## **9. Unigram**

Unigramåˆ†è¯ä¸BPEå’ŒWordPieceä¸åŒï¼Œæ˜¯åŸºäºä¸€ä¸ªå¤§è¯è¡¨é€æ­¥è£å‰ªæˆä¸€ä¸ªå°è¯è¡¨ã€‚ é€šè¿‡Unigramè¯­è¨€æ¨¡å‹è®¡ç®—åˆ é™¤ä¸åŒsubwordé€ æˆçš„æŸå¤±æ¥è¡¡é‡subwordçš„é‡è¦æ€§ï¼Œä¿ç•™é‡è¦æ€§è¾ƒé«˜çš„å­è¯ã€‚

- è®­ç»ƒæ–¹æ³•ï¼šä»åŒ…å«å­—ç¬¦å’Œå…¨éƒ¨å­è¯çš„å¤§è¯è¡¨å‡ºå‘ï¼Œé€æ­¥è£å‰ªå‡ºä¸€ä¸ªå°è¯è¡¨ï¼Œå¹¶ä¸”æ¯ä¸ªè¯éƒ½æœ‰è‡ªå·±çš„åˆ†æ•°ã€‚
- ç¼–ç æ–¹æ³•ï¼šå°†æ–‡æœ¬åˆ‡åˆ†æˆè¯ï¼Œå¯¹æ¯ä¸ªè¯åŸºäºViterbiç®—æ³•æ±‚è§£å‡ºæœ€ä½³è§£ç è·¯å¾„ã€‚
- ç»å…¸æ¨¡å‹ï¼šAlBERT, T5, mBART, Big Bird, XLNet
    
    ## **5.1. è®­ç»ƒé˜¶æ®µ**
    
    åœ¨è®­ç»ƒç¯èŠ‚ï¼Œç›®æ ‡æ˜¯ç»™å®šè¯­æ–™ï¼Œé€šè¿‡è®­ç»ƒç®—æ³•ï¼Œç”Ÿæˆæœ€ç»ˆçš„è¯è¡¨ï¼Œå¹¶ä¸”æ¯ä¸ªè¯æœ‰è‡ªå·±çš„æ¦‚ç‡å€¼ã€‚ Unigramç®—æ³•æ˜¯ä»å¤§è¯è¡¨ä¸ºåŸºç¡€ï¼Œé€æ­¥è£å‰ªæˆå°è¯è¡¨ã€‚è£å‰ªè§„åˆ™æ˜¯æ ¹æ®**Unigramè¯­è¨€æ¨¡å‹**çš„æ‰“åˆ†ä¾æ¬¡è£å‰ªé‡è¦åº¦ç›¸å¯¹è¾ƒä½çš„è¯ã€‚
    
    ä¸‹é¢è¿›è¡Œå…·ä½“æ‰‹å·¥å®ç°ã€‚
    
    å‡å®šè®­ç»ƒçš„è¯­æ–™(å·²å½’ä¸€åŒ–å¤„ç†)ä¸º
    
    ```python
    corpus **=** [
        "This is the Hugging Face Course.",
        "This chapter is about tokenization.",
        "This section shows several tokenizer algorithms.",
        "Hopefully, you will be able to understand how they are trained and generate tokens.",
    ]
    ```
    
    é¦–å…ˆè¿›è¡Œé¢„åˆ‡åˆ†å¤„ç†ã€‚è¿™é‡Œé‡‡ç”¨xlnetçš„é¢„åˆ‡åˆ†é€»è¾‘ã€‚å…·ä½“ä¼šæŒ‰ç…§ç©ºæ ¼è¿›è¡Œåˆ‡åˆ†ï¼Œæ ‡ç‚¹ä¸ä¼šåˆ‡åˆ†ã€‚å¹¶ä¸”ç©ºæ ¼ä¼šä¿ç•™æˆç‰¹æ®Šå­—ç¬¦"â–"ï¼Œå¥å­å¼€å¤´ä¹Ÿä¼šæ·»åŠ ç‰¹æ®Šå­—ç¬¦"â–"ã€‚
    
    ```python
    **from** transformers **import** AutoTokenizer
    
    *# init pre tokenize function*
    xlnet_tokenizer **=** AutoTokenizer**.**from_pretrained("xlnet-base-cased")
    pre_tokenize_function **=** xlnet_tokenizer**.**backend_tokenizer**.**pre_tokenizer**.**pre_tokenize_str
    
    *# pre tokenize*
    pre_tokenized_corpus **=** [pre_tokenize_str(text) **for** text **in** corpus]
    ```
    
    è·å¾—çš„pre_tokenized_corpuså¦‚ä¸‹ï¼Œæ¯ä¸ªå•å…ƒåˆ†åˆ«ä¸º[word, (start_index, end_index)]
    
    ```python
    [
        [('â–This', (0, 4)), ('â–is', (5, 7)), ('â–the', (8, 11)), ('â–Hugging', (12, 19)), ('â–Face', (20, 24)), ('â–Course.', (25, 32))], 
        [('â–This', (0, 4)), ('â–chapter', (5, 12)), ('â–is', (13, 15)), ('â–about', (16, 21)), ('â–tokenization.', (22, 35))], 
        [('â–This', (0, 4)), ('â–section', (5, 12)), ('â–shows', (13, 18)), ('â–several', (19, 26)), ('â–tokenizer', (27, 36)), ('â–algorithms.', (37, 48))], 
        [('â–Hopefully,', (0, 10)), ('â–you', (11, 14)), ('â–will', (15, 19)), ('â–be', (20, 22)), ('â–able', (23, 27)), ('â–to', (28, 30)), ('â–understand', (31, 41)), ('â–how', (42, 45)), ('â–they', (46, 50)), ('â–are', (51, 54)), ('â–trained', (55, 62)), ('â–and', (63, 66)), ('â–generate', (67, 75)), ('â–tokens.', (76, 83))]
    ]
    ```
    
    è¿›ä¸€æ­¥ç»Ÿè®¡è¯é¢‘
    
    ```python
    word2count **=** defaultdict(int)
    **for** split_text **in** pre_tokenized_corpus:
        **for** word, _ **in** split_text:
            word2count[word] **+=** 1
    ```
    
    è·å¾—word2countå¦‚ä¸‹
    
    ```python
    defaultdict(**<class** '**int**'>, {'â–This': 3, 'â–**is**': 2, 'â–the': 1, 'â–Hugging': 1, 'â–Face': 1, 'â–Course**.**': 1, 'â–chapter': 1, 'â–about': 1, 'â–tokenization**.**': 1, 'â–section': 1, 'â–shows': 1, 'â–several': 1, 'â–tokenizer': 1, 'â–algorithms**.**': 1, 'â–Hopefully,': 1, 'â–you': 1, 'â–will': 1, 'â–be': 1, 'â–able': 1, 'â–to': 1, 'â–understand': 1, 'â–how': 1, 'â–they': 1, 'â–are': 1, 'â–trained': 1, 'â–**and**': 1, 'â–generate': 1, 'â–tokens**.**': 1})
    ```
    
    ç»Ÿè®¡è¯è¡¨çš„å…¨éƒ¨å­è¯å’Œè¯é¢‘ï¼Œå–å‰300ä¸ªè¯ï¼Œæ„æˆæœ€åˆçš„å¤§è¯è¡¨ã€‚ä¸ºäº†é¿å…OOVï¼ˆOut of Vocabularyï¼‰ï¼Œcharçº§åˆ«çš„è¯å‡éœ€è¦ä¿ç•™ã€‚
    
    ```python
    char2count **=** defaultdict(int)
    sub_word2count **=** defaultdict(int)
    **for** word, count **in** word2count**.**items():
        **for** i **in** range(len(word)):
            char2count[word[i]] **+=** count
            **for** j **in** range(i **+** 2, len(word) **+** 1):
                sub_word2count[word[i:j]] **+=** count
    sorted_sub_words **=** sorted(sub_word2count**.**items(), key**=lambda** x: x[1], reverse**=True**)
    *# init a large vocab with 300*
    tokens **=** list(char2count**.**items()) **+** sorted_sub_words[: 300 **-** len(char2count)]
    ```
    
    è·å¾—çš„åˆå§‹å°è¯è¡¨vocabså¦‚ä¸‹:
    
    ```python
    [('â–', 31), ('T', 3), ('h', 9), ('i', 13), ('s', 13), **...**,  ('several', 1)]
    ```
    
    è¿›ä¸€æ­¥ç»Ÿè®¡æ¯ä¸ªå­è¯çš„æ¦‚ç‡ï¼Œå¹¶è½¬æ¢æˆUnigramé‡Œçš„lossè´¡çŒ®
    
    ```python
    token2count **=** {token: count **for** token, count **in** tokens}
    total_count **=** sum([count **for** token, count **in** token2count**.**items()])
    model **=** {token: **-**log(count **/** total_count) **for** token, count **in** token2count**.**items()}
    
    model **=** {
        'â–': 2.952892114877499, 
        'T': 5.288267030694535, 
        'h': 4.189654742026425, 
        **...**, 
        'sever': 6.386879319362645, 
        'severa': 6.386879319362645, 
        'several': 6.386879319362645
    }
    ```
    
    åŸºäºæ¯ä¸ªå­è¯çš„lossä»¥åŠViterbiç®—æ³•å°±å¯ä»¥æ±‚è§£å‡ºï¼Œè¾“å…¥çš„ä¸€ä¸ªè¯çš„æœ€ä½³åˆ†è¯è·¯å¾„ã€‚å³æ•´ä½“è¯­è¨€æ¨¡å‹çš„lossæœ€å°ã€‚è¯çš„é•¿åº¦ä¸ºNï¼Œè§£ç çš„æ—¶é—´å¤æ‚åº¦ä¸º$O(N^2)$ã€‚
    
    ```python
    **def** **_encode_word**(word, model):
        best_segmentations **=** [{"start": 0, "score": 1}] **+** [{"start": **None**, "score": **None**} **for** _ **in** range(len(word))]
        **for** start_idx **in** range(len(word)):
            *# This should be properly filled by the previous steps of the loop*
    				best_score_at_start **=** best_segmentations[start_idx]["score"]
            **for** end_idx **in** range(start_idx **+** 1, len(word) **+** 1):
                token **=** word[start_idx:end_idx]
                **if** token **in** model **and** best_score_at_start **is** **not** **None**:
                    score **=** model[token] **+** best_score_at_start
                    *# If we have found a better segmentation (lower score) ending at end_idx*
    								**if** (
                            best_segmentations[end_idx]["score"] **is** **Noneor** best_segmentations[end_idx]["score"] **>** score
                    ):
                        best_segmentations[end_idx] **=** {"start": start_idx, "score": score}
        segmentation **=** best_segmentations[**-**1]
        **if** segmentation["score"] **is** **None**:
            *# We did not find a tokenization of the word -> unknown*
    				**return** ["<unk>"], **None**
    		score **=** segmentation["score"]
        start **=** segmentation["start"]
        end **=** len(word)
        tokens **=** []
        **while** start **!=** 0:
            tokens**.**insert(0, word[start:end])
            next_start **=** best_segmentations[start]["start"]
            end **=** start
            start **=** next_start
        tokens**.**insert(0, word[start:end])
        **return** tokens, score
    ```
    
    ä¾‹å¦‚ï¼š
    
    ```python
    **>>>** tokenize("This")
    **>>>** (['This'], 6.288267030694535)
    **>>>** tokenize("this")
    **>>>**(['t', 'his'], 10.03608902044192)
    ```
    
    åŸºäºä¸Šè¿°çš„å‡½æ•°ï¼Œå¯ä»¥è·å¾—ä»»ä¸€ä¸ªè¯çš„åˆ†è¯è·¯å¾„ï¼Œä»¥åŠlossã€‚è¿™æ ·å°±å¯ä»¥è®¡ç®—æ•´ä¸ªè¯­æ–™ä¸Šçš„lossã€‚
    
    ```python
    **def** **_compute_loss**(self, model, word2count):
        loss **=** 0
        **for** word, freq **in** word2count**.**items():
            _, word_loss **=** self**.**_encode_word(word, model)
            loss **+=** freq ***** word_loss
        **return** loss
    ```
    
    å°è¯•ç§»é™¤modelä¸­çš„ä¸€ä¸ªå­è¯ï¼Œå¹¶è®¡ç®—ç§»é™¤åæ–°çš„modelåœ¨å…¨éƒ¨è¯­æ–™ä¸Šçš„lossï¼Œä»è€Œè·å¾—è¿™ä¸ªå­è¯çš„scoreï¼Œå³åˆ é™¤è¿™ä¸ªå­è¯ä½¿å¾—lossæ–°å¢çš„é‡ã€‚
    
    ```python
    **def** **_compute_scores**(self, model, word2count):
        scores **=** {}
        model_loss **=** self**.**_compute_loss(model, word2count)
        **for** token, score **in** model**.**items():
            *# We always keep tokens of length 1*
    				**if** len(token) **==** 1:
                **continue**model_without_token **=** copy**.**deepcopy(model)
            _ **=** model_without_token**.**pop(token)
            scores[token] **=** self**.**_compute_loss(model_without_token, word2count) **-** model_loss
        **return** scores
    
    scores **=** self**.**_compute_scores(model, word2count)
    ```
    
    ä¸ºäº†æå‡è¿­ä»£æ•ˆç‡ï¼Œæ‰¹é‡åˆ é™¤å‰10%çš„ç»“æœï¼Œå³è®©æ•´ä½“losså¢é‡æœ€å°çš„å‰10%çš„è¯ã€‚(åˆ é™¤è¿™äº›è¯å¯¹æ•´ä½“lossçš„å½±å“ä¸å¤§ã€‚)
    
    ```python
    sorted_scores **=** sorted(scores**.**items(), key**=lambda** x: x[1])
    *# Remove percent_to_remove tokens with the lowest scores.*
    **for** i **in** range(int(len(model) ***** 0.1)):
        _ **=** token2count**.**pop(sorted_scores[i][0])
    ```
    
    è·å¾—æ–°çš„è¯è¡¨åï¼Œé‡æ–°è®¡ç®—æ¯ä¸ªè¯çš„æ¦‚ç‡ï¼Œè·å¾—æ–°çš„æ¨¡å‹ã€‚å¹¶é‡å¤ä»¥ä¸Šæ­¥éª¤ï¼Œç›´åˆ°è£å‰ªåˆ°è¯è¡¨å¤§å°ç¬¦åˆè¦æ±‚ã€‚
    
    ```python
    **while** len(model) **>** vocab_size:
        scores **=** self**.**_compute_scores(model, word2count)
        sorted_scores **=** sorted(scores**.**items(), key**=lambda** x: x[1])
        *# Remove percent_to_remove tokens with the lowest scores.*
    		**for** i **in** range(int(len(model) ***** percent_to_remove)):
            _ **=** token2count**.**pop(sorted_scores[i][0])
        total_count **=** sum([freq **for** token, freq **in** token2count**.**items()])
        model **=** {token: **-**log(count **/** total_count) **for** token, count **in** token2count**.**items()}
    ```
    
    å‡å®šé¢„è®¾çš„è¯è¡¨çš„å¤§å°ä¸º100ï¼Œç»è¿‡ä¸Šè¿°è¿­ä»£åæˆ‘ä»¬è·å¾—è¯è¡¨å¦‚ä¸‹:
    
    ```python
    model **=** {
        'â–': 2.318585434340487, 
        'T': 4.653960350157523, 
        'h': 3.5553480614894135, 
        'i': 3.1876232813640963, 
        **...**
    		'seve': 5.752572638825633, 
        'sever': 5.752572638825633, 
        'severa': 5.752572638825633, 
        'several': 5.752572638825633
    }
    ```
    
    ## **5.2. æ¨ç†é˜¶æ®µ**
    
    åœ¨æ¨ç†é˜¶æ®µï¼Œç»™å®šä¸€ä¸ªå¥å­ï¼Œéœ€è¦å°†å…¶åˆ‡åˆ†æˆä¸€ä¸ªtokençš„åºåˆ—ã€‚ å…·ä½“å®ç°ä¸Šå…ˆå¯¹å¥å­è¿›è¡Œé¢„åˆ†è¯ï¼Œç„¶åå¯¹æ¯ä¸ªè¯åŸºäºViterbiç®—æ³•è¿›è¡Œè§£ç ã€‚
    
    ```python
    **def** **tokenize**(self, text):
        words **=** [word **for** word, _ **in** self**.**pre_tokenize_str(text)]
        encoded_words **=** [self**.**_encode_word(word, self**.**model)[0] **for** word **in** words]
        **return** sum(encoded_words, [])
    ```
    
    ä¾‹å¦‚
    
    ```python
    **>>>** tokenize("This is the Hugging Face course!")
    **>>>** ['â–This', 'â–is', 'â–the', 'â–Hugging', 'â–Face', 'â–', 'c', 'ou', 'r', 's', 'e', '.']
    ```
    
    åŸºäºViterbiçš„åˆ‡åˆ†è·å¾—çš„æ˜¯æœ€ä½³åˆ‡åˆ†ï¼ŒåŸºäºunigramå¯ä»¥å®ç°ä¸€ä¸ªå¥å­çš„å¤šç§åˆ‡åˆ†æ–¹å¼ï¼Œå¹¶ä¸”å¯ä»¥è·å¾—æ¯ç§åˆ‡åˆ†è·¯å¾„çš„æ‰“åˆ†ã€‚
    

## **10. SentencePiece**

[SentencePiece](https://link.zhihu.com/?target=https%3A//github.com/google/sentencepiece)æ˜¯Googleå‡ºçš„ä¸€ä¸ªåˆ†è¯å·¥å…·:

- å†…ç½®BPEï¼ŒUnigramï¼Œcharå’Œwordçš„åˆ†è¯æ–¹æ³•
- æ— éœ€é¢„åˆ†è¯ï¼Œä»¥unicodeæ–¹å¼ç›´æ¥ç¼–ç æ•´ä¸ªå¥å­ï¼Œç©ºæ ¼ä¼šè¢«ç‰¹æ®Šç¼–ç ä¸ºâ–
- ç›¸æ¯”ä¼ ç»Ÿå®ç°è¿›è¡Œä¼˜åŒ–ï¼Œåˆ†è¯é€Ÿåº¦é€Ÿåº¦æ›´å¿«

å½“å‰ä¸»æµçš„å¤§æ¨¡å‹éƒ½æ˜¯åŸºäºsentencepieceå®ç°ï¼Œä¾‹å¦‚ChatGLMçš„tokenizerã€‚

```python
**...class** **TextTokenizer**:
    **def** **__init__**(self, model_path):
        self**.**sp **=** spm**.**SentencePieceProcessor()
        self**.**sp**.**Load(model_path)
        self**.**num_tokens **=** self**.**sp**.**vocab_size()

    **def** **encode**(self, text):
        **return** self**.**sp**.**EncodeAsIds(text)

    **def** **decode**(self, ids: List[int]):
        **return** self**.**sp**.**DecodeIds(ids)
**...**
```

[https://huggingface.co/THUDM/chatglm-6b/blob/main/tokenization_chatglm.py#L21](https://link.zhihu.com/?target=https%3A//huggingface.co/THUDM/chatglm-6b/blob/main/tokenization_chatglm.py%23L21)

## **6.1. byteå›é€€**

å½“SentencePieceåœ¨è®­ç»ƒBPEçš„æ—¶å¼€å¯`--byte_fallback`, åœ¨æ•ˆæœä¸Šç±»ä¼¼BBPEï¼Œé‡åˆ°UNKä¼šç»§ç»­æŒ‰ç…§byteè¿›è¡Œè¿›ä¸€æ­¥çš„åˆ‡åˆ†ã€‚å‚è§ï¼š[https://github.com/google/sentencepiece/issues/621](https://link.zhihu.com/?target=https%3A//github.com/google/sentencepiece/issues/621)Â å…·ä½“å®ç°ä¸Šæ˜¯å°†<0x00> ... <0xFF>è¿™256ä¸ªtokenæ·»åŠ åˆ°è¯è¡¨ä¸­ã€‚

åˆ†æChatGLMçš„æ¨¡å‹ï¼Œå¯ä»¥å‘ç°ChatGLMå°±æ˜¯å¼€å¯äº†`--byte_fallback`

```python
**from** sentencepiece **import** sentencepiece_model_pb2

m **=** sentencepiece_model_pb2**.**ModelProto()
**with** open('chatglm-6b/ice_text.model', 'rb') **as** f:
    m**.**ParseFromString(f**.**read())
print('ChatGLM tokenizer\n\n'**+**str(m**.**trainer_spec))
```

outputï¼š

```python
ChatGLM tokenizer

input: "/root/train_cn_en.json"
model_prefix: "new_ice_unigram"
vocab_size: 130000
character_coverage: 0.9998999834060669
split_digits: true
user_defined_symbols: "<n>"
byte_fallback: true
pad_id: 3
train_extremely_large_corpus: true
```

å¯ä»¥çœ‹åˆ°`byte_fallback: true`

åŒæ ·çš„æ–¹æ³•ï¼Œå¯ä»¥éªŒè¯LLaMA, ChatGLM-6B, Baichuanè¿™äº›å¤§æ¨¡å‹éƒ½æ˜¯åŸºäºsentencepieceå®ç°çš„BPEçš„åˆ†è¯ç®—æ³•ï¼Œå¹¶ä¸”é‡‡ç”¨byteå›é€€ã€‚

## 11. åˆ†è¯å™¨è¯„ä»·

è¯„ä»·ä¸€ä¸ª **åˆ†è¯å™¨**ï¼ˆTokenizerï¼‰å¥½åçš„æ ‡å‡†å¯ä»¥ä»å¤šä¸ªæ–¹é¢æ¥è¿›è¡Œåˆ†æï¼Œå…·ä½“åŒ…æ‹¬ **å‡†ç¡®æ€§**ã€**æ•ˆç‡**ã€**å¯æ‰©å±•æ€§**ã€**é€‚åº”æ€§** ç­‰æ–¹é¢ã€‚ä¸‹é¢æ˜¯ä¸€äº›å¸¸ç”¨çš„è¯„ä¼°æŒ‡æ ‡ï¼š

**1. å‡†ç¡®æ€§ï¼ˆAccuracyï¼‰**

åˆ†è¯å™¨çš„å‡†ç¡®æ€§æ˜¯è¯„ä»·å…¶å¥½åçš„æœ€é‡è¦æ ‡å‡†ä¹‹ä¸€ï¼Œé€šå¸¸é€šè¿‡ä»¥ä¸‹å‡ ä¸ªæ–¹é¢æ¥è¡¡é‡ï¼š

â€¢ **è¯æ±‡åˆ’åˆ†å‡†ç¡®åº¦**ï¼šåˆ†è¯å™¨åº”è¯¥èƒ½å¤Ÿæ­£ç¡®åœ°è¯†åˆ«å•è¯è¾¹ç•Œå¹¶å°†å•è¯æˆ–å­è¯æ­£ç¡®åˆ†å‰²ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸­æ–‡åˆ†è¯ä¸­ï¼Œèƒ½å¤Ÿå‡†ç¡®åœ°å¤„ç†è¯æ±‡çš„è¾¹ç•Œæ˜¯éå¸¸é‡è¦çš„ã€‚

â€¢ **OOVï¼ˆOut of Vocabularyï¼‰å¤„ç†**ï¼šä¸€ä¸ªå¥½çš„åˆ†è¯å™¨åº”å¯¹æœªè§è¿‡çš„è¯ï¼ˆOOVï¼‰èƒ½å¤Ÿåšå‡ºåˆç†çš„å¤„ç†ã€‚æ¯”å¦‚ï¼Œå­è¯åˆ‡åˆ†æ–¹æ³•ï¼ˆå¦‚ BPEã€WordPieceï¼‰èƒ½å¤Ÿå°†æœªè§è¿‡çš„è¯åˆ†è§£ä¸ºå·²çŸ¥çš„å­è¯å•å…ƒï¼Œè§£å†³ OOV é—®é¢˜ã€‚

â€¢ **ç»†ç²’åº¦åˆ†æèƒ½åŠ›**ï¼šèƒ½å¤Ÿç»†ç²’åº¦åœ°æ‹†è§£å¤æ‚è¯æ±‡ï¼Œæ¯”å¦‚è‹±æ–‡å¤åˆè¯ã€è¯ç¼€ã€æ‹¼å†™é”™è¯¯ç­‰ã€‚

â€¢ **è¯„ä¼°æ–¹æ³•**ï¼šå¯ä»¥é€šè¿‡ **ç²¾ç¡®åº¦**ï¼ˆPrecisionï¼‰ã€**å¬å›ç‡**ï¼ˆRecallï¼‰å’Œ **F1 å€¼** æ¥é‡åŒ–åˆ†è¯å‡†ç¡®æ€§ï¼Œå…·ä½“è¡¨ç°ä¸ºåˆ†è¯æ˜¯å¦æ­£ç¡®è¯†åˆ«äº†è¯æ±‡è¾¹ç•Œï¼Œå¹¶ä¸”åˆ†è¯ç»“æœä¸äººå·¥æ ‡æ³¨æˆ–æ ‡å‡†è¯å…¸çš„åŒ¹é…ç¨‹åº¦ã€‚

**2. æ•ˆç‡ï¼ˆEfficiencyï¼‰**

åˆ†è¯å™¨çš„å¤„ç†é€Ÿåº¦å’Œå†…å­˜æ¶ˆè€—ç›´æ¥å½±å“æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ã€‚é«˜æ•ˆçš„åˆ†è¯å™¨èƒ½å¤Ÿåœ¨å¤§è§„æ¨¡æ–‡æœ¬å¤„ç†æ—¶ä¿æŒè¾ƒä½çš„å»¶è¿Ÿã€‚

â€¢**å¤„ç†é€Ÿåº¦**ï¼šå¯¹äºå¤§è§„æ¨¡æ–‡æœ¬æˆ–å®æ—¶åº”ç”¨ï¼Œåˆ†è¯å™¨çš„å¤„ç†é€Ÿåº¦è‡³å…³é‡è¦ã€‚ä¸€ä¸ªé«˜æ•ˆçš„åˆ†è¯å™¨èƒ½å¤Ÿå¿«é€Ÿå¤„ç†æ•°ç™¾ä¸‡çº§åˆ«çš„æ–‡æœ¬ã€‚

â€¢**å†…å­˜å ç”¨**ï¼šå°¤å…¶æ˜¯åœ¨è®­ç»ƒå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æ—¶ï¼Œåˆ†è¯å™¨çš„å†…å­˜ä½¿ç”¨é‡åº”å°½å¯èƒ½ä½ï¼Œé¿å…è¿‡å¤šçš„å†…å­˜å ç”¨ã€‚

**è¯„ä¼°æ–¹æ³•**ï¼š

â€¢æ¯ç§’å¤„ç†çš„å•è¯æ•°é‡ï¼ˆthroughputï¼‰ã€‚

â€¢æ¯æ¬¡åˆ†è¯æ“ä½œæ¶ˆè€—çš„æ—¶é—´ã€‚

â€¢åˆ†è¯å™¨çš„å†…å­˜æ¶ˆè€—ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤§å‹æ–‡æœ¬æ—¶ã€‚

**3. é€‚åº”æ€§ï¼ˆAdaptabilityï¼‰**

å¥½çš„åˆ†è¯å™¨åº”èƒ½å¤Ÿé€‚åº”ä¸åŒç±»å‹çš„æ–‡æœ¬å’Œè¯­è¨€ï¼Œå…·æœ‰è¾ƒå¼ºçš„ **çµæ´»æ€§** å’Œ **å¯æ‰©å±•æ€§**ã€‚

â€¢**å¤šè¯­è¨€æ”¯æŒ**ï¼šä¸€ä¸ªä¼˜ç§€çš„åˆ†è¯å™¨åº”èƒ½æ”¯æŒå¤šè¯­è¨€å¤„ç†ï¼Œå°¤å…¶æ˜¯å¯¹äºå¤šè¯­è¨€ç¯å¢ƒä¸­çš„åˆ†è¯ä»»åŠ¡ã€‚

â€¢**é¢†åŸŸé€‚åº”æ€§**ï¼šé’ˆå¯¹ç‰¹å®šé¢†åŸŸï¼ˆå¦‚åŒ»å­¦ã€æ³•å¾‹ã€é‡‘èç­‰ï¼‰è®­ç»ƒçš„åˆ†è¯å™¨åº”èƒ½å¤Ÿå¤„ç†ä¸“ä¸šæœ¯è¯­ã€ç¼©å†™ã€é¢†åŸŸç‰¹å®šçš„è¡¨è¾¾ã€‚

â€¢**è‡ªé€‚åº”èƒ½åŠ›**ï¼šä¸€ä¸ªå¥½çš„åˆ†è¯å™¨èƒ½å¤Ÿæ ¹æ®è®­ç»ƒè¯­æ–™ä¸æ–­å­¦ä¹ å¹¶é€‚åº”ä¸åŒçš„æ–‡æœ¬ç‰¹å¾ï¼Œå‡å°‘äººå·¥å¹²é¢„ã€‚

**è¯„ä¼°æ–¹æ³•**ï¼š

â€¢åœ¨ä¸åŒè¯­è¨€ä¸Šçš„æ•ˆæœï¼šæ˜¯å¦èƒ½å¤Ÿæ”¯æŒå¤šè¯­è¨€å¹¶æä¾›è¾ƒå¥½çš„åˆ†è¯æ•ˆæœã€‚

â€¢åœ¨ä¸åŒé¢†åŸŸä¸Šçš„è¡¨ç°ï¼šä¾‹å¦‚ï¼Œæ³•å¾‹æ–‡æœ¬ä¸ç¤¾äº¤åª’ä½“æ–‡æœ¬çš„åˆ†è¯æ•ˆæœæ˜¯å¦æœ‰å·®å¼‚ã€‚

**4. å¯è§£é‡Šæ€§ï¼ˆInterpretabilityï¼‰**

åœ¨ä¸€äº›ä»»åŠ¡ä¸­ï¼Œåˆ†è¯å™¨çš„ **å¯è§£é‡Šæ€§** ä¹Ÿå˜å¾—éå¸¸é‡è¦ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬ä½¿ç”¨åˆ†è¯å™¨æ¥å¤„ç†æ–‡æœ¬æ—¶ï¼Œç†è§£ä¸ºä»€ä¹ˆæŸäº›è¯è¢«åˆ‡åˆ†ä¸ºå­è¯ã€è¯æ ¹ã€è¯ç¼€ç­‰ï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬è°ƒä¼˜åˆ†è¯ç­–ç•¥ã€‚

â€¢**æ¨¡å‹é€æ˜æ€§**ï¼šä¸€äº›åŸºäºç»Ÿè®¡çš„åˆ†è¯æ–¹æ³•ï¼ˆå¦‚ BPEã€WordPieceï¼‰æ¯”è¾ƒé»‘ç®±ï¼Œéš¾ä»¥ç›´æ¥è§£é‡Šï¼›ä½†ä¸€äº›åŸºäºè§„åˆ™çš„åˆ†è¯å™¨ï¼Œæˆ–è€…åœ¨æŸäº›ç‰¹æ®Šåº”ç”¨ä¸­ï¼Œåˆ†è¯çš„è¿‡ç¨‹éœ€è¦å¯è§£é‡Šæ€§ã€‚

**5. è¯æ±‡è¡¨å¤§å°ï¼ˆVocabulary Sizeï¼‰**

åˆ†è¯å™¨çš„è¯æ±‡è¡¨å¤§å°å¯¹æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†éƒ½æœ‰é‡è¦å½±å“ï¼š

â€¢**è¾ƒå°çš„è¯æ±‡è¡¨**ï¼šä½¿ç”¨å­è¯åˆ‡åˆ†ï¼ˆå¦‚ BPEã€WordPieceï¼‰æ—¶ï¼Œé€šå¸¸å¯ä»¥ä½¿ç”¨è¾ƒå°çš„è¯æ±‡è¡¨ï¼Œå› ä¸ºä¸éœ€è¦å­˜å‚¨æ¯ä¸ªè¯çš„å®Œæ•´å½¢å¼ï¼Œåªéœ€è¦å­˜å‚¨å­è¯å•å…ƒã€‚è¾ƒå°çš„è¯æ±‡è¡¨æœ‰åŠ©äºæé«˜æ•ˆç‡å¹¶å‡å°‘å†…å­˜å ç”¨ã€‚

â€¢**è¾ƒå¤§çš„è¯æ±‡è¡¨**ï¼šè™½ç„¶æ›´å¤§çš„è¯æ±‡è¡¨å¯èƒ½èƒ½å¤„ç†æ›´å¤šçš„ç‰¹å®šè¯æ±‡ï¼Œä½†ä¼šå¸¦æ¥æ›´é«˜çš„å†…å­˜æ¶ˆè€—å’Œå¤„ç†é€Ÿåº¦é—®é¢˜ã€‚å¯¹äºå¤§è§„æ¨¡æ–‡æœ¬ä»»åŠ¡æ¥è¯´ï¼Œè¯æ±‡è¡¨è¿‡å¤§ä¼šå½±å“åˆ†è¯å™¨çš„æ•ˆç‡ã€‚

**è¯„ä¼°æ–¹æ³•**ï¼š

â€¢è¯æ±‡è¡¨å¤§å°çš„å¹³è¡¡ï¼Œæ—¢èƒ½å¤Ÿè¦†ç›–å¤§å¤šæ•°å¸¸ç”¨è¯æ±‡ï¼Œåˆèƒ½å¤Ÿé¿å…è¯æ±‡è¡¨è¿‡å¤§å¯¼è‡´çš„è®¡ç®—è´Ÿæ‹…ã€‚

**6. å¤„ç†å¤šæ ·æ€§ï¼ˆDiversity Handlingï¼‰**

ç°ä»£æ–‡æœ¬ä¸­åŒ…å«è®¸å¤šå˜ç§è¯ã€æ‹¼å†™é”™è¯¯ã€ç¼©å†™å’Œè¡¨æƒ…ç¬¦å·ç­‰ï¼Œåˆ†è¯å™¨éœ€è¦èƒ½çµæ´»å¤„ç†è¿™äº›éæ ‡å‡†å½¢å¼ã€‚

â€¢**æ‹¼å†™çº é”™**ï¼šå¤„ç†æ‹¼å†™é”™è¯¯çš„èƒ½åŠ›ã€‚

â€¢**æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦**ï¼šèƒ½å¤Ÿåˆç†åœ°å¤„ç†æ ‡ç‚¹ç¬¦å·ã€è¡¨æƒ…ç¬¦å·ç­‰ã€‚

â€¢**æ‹¼éŸ³/ç¼©å†™è¯**ï¼šèƒ½æœ‰æ•ˆå¤„ç†ç¼©å†™ã€ä¿šè¯­ã€æ‹¼éŸ³ç­‰ç‰¹æ®Šæ–‡æœ¬ã€‚

**7. é²æ£’æ€§ï¼ˆRobustnessï¼‰**

åˆ†è¯å™¨çš„é²æ£’æ€§æ˜¯æŒ‡å…¶åœ¨é¢å¯¹ä¸åŒç±»å‹çš„æ–‡æœ¬å™ªéŸ³ï¼ˆå¦‚æ‹¼å†™é”™è¯¯ã€ä¹±ç ã€ç¬¦å·ç­‰ï¼‰æ—¶ï¼Œä»èƒ½ç¨³å®šå·¥ä½œã€‚

â€¢åœ¨å™ªå£°æ–‡æœ¬ä¸­ï¼ˆå¦‚ç¤¾äº¤åª’ä½“æ–‡æœ¬ã€å¸¦æœ‰æ‹¼å†™é”™è¯¯çš„æ–‡æœ¬ç­‰ï¼‰ï¼Œåˆ†è¯å™¨æ˜¯å¦èƒ½å¤Ÿä¿æŒè¾ƒå¥½çš„åˆ†è¯æ•ˆæœã€‚

## **å‚è€ƒ**

- **HuggingFace tokenizer tutorial**ï¼š[https://huggingface.co/learn/nlp-course/chapter6/1](https://huggingface.co/learn/nlp-course/chapter6/1)
- **google/sentencepiece**ï¼š[https://github.com/google/sentencepiece/](https://github.com/google/sentencepiece/)
- **BPE: Neural Machine Translation of Rare Words with Subword Units**ï¼š[https://arxiv.org/abs/1508.07909](https://arxiv.org/abs/1508.07909)
- **BBPE: Neural Machine Translation with Byte-Level Subwords**ï¼š[https://arxiv.org/pdf/1909.03341.pdf](https://arxiv.org/pdf/1909.03341.pdf)
- **Unigram: Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates**ï¼š[https://arxiv.org/abs/1804.10959](https://arxiv.org/abs/1804.10959)
- **SentencePiece**: A simple and language independent subword tokenizer and detokenizer for Neural Text Processingï¼š[https://arxiv.org/abs/1808.06226](https://arxiv.org/abs/1808.06226)